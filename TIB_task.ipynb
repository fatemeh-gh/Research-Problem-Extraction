{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TIB task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "for this task, the first and quick idea brings to my mind was a binary classification task. First, I extracted research problem sentences from research-problem.json, and using a similarity function, I labeled the dataset with 0 and 1 labels to represent \"non research problem\" and \"research problem\" respectively. in he following cells, I will explain more about the dataset creation.\n"
      ],
      "metadata": {
        "id": "ddltg6nvQFxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ncg-task/test-data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwSt22amfi6U",
        "outputId": "eef3f1a4-0e98-4f77-e27b-eb05640e8b69"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'test-data' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo2O4U4YfsIb",
        "outputId": "d3ddf0b8-46c8-4bd6-e8d0-1dc2dae294a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "from transformers import BertConfig, BertTokenizerFast, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "metadata": {
        "id": "HnSYegjielzs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Read_Data(data_dir):\n",
        "    articles = []\n",
        "    research_problem_sentences = []\n",
        "\n",
        "    for category in os.listdir(data_dir):\n",
        "        if category != 'README.md' and category != '.git':\n",
        "            article_category = os.path.join(data_dir, category)\n",
        "            if os.path.isfile(article_category):\n",
        "                continue\n",
        "\n",
        "            for folder_name in sorted(os.listdir(article_category)):\n",
        "                article_index = os.path.join(article_category, folder_name)\n",
        "\n",
        "                if len(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))) == 0:\n",
        "                    continue\n",
        "\n",
        "                with open(glob.glob(os.path.join(article_index, '*-Stanza-out.txt'))[0], encoding='utf-8') as f:\n",
        "                    article = f.read()\n",
        "                    articles.append(article.lower())\n",
        "\n",
        "                with open(os.path.join(article_index, 'info-units/research-problem.json'), encoding='utf-8') as f:\n",
        "                    research_problem_sentences.append([element[-1].get(\"from sentence\", None) if isinstance(element, list) else element.get(\"from sentence\", None) if isinstance(element, dict) else None for element in json.load(f)[\"has research problem\"]])\n",
        "    return articles, research_problem_sentences"
      ],
      "metadata": {
        "id": "a743epj4fCnU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdHmE-977QMn",
        "outputId": "84cf85cb-2047-4f83-a5a1-104d3663f1b1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.3.0/en_core_web_lg-3.3.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 400.7 MB 6.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (0.7.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-lg==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to label our dataset to \"non research problem\" and \"research problem\", we compare every sentences in research-problem.json with sentences of their related research papaer plain text in stanza format. if the similarity is more that 0.9, label will be 1 ( is research problem sentence) and otherwise, the label will be 0. \n",
        "In the bellow cell, we define a function to sompute similarity between two sentences. "
      ],
      "metadata": {
        "id": "l-ZLW2g_MXB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json , os , glob\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "def check_similarity(sentences , train_research_problem_sentences):\n",
        "    global df\n",
        "    ########################## Check Similarity\n",
        "\n",
        "    for i in range(len(train_research_problem_sentences)):\n",
        "      #print(train_research_problem_sentences[i])\n",
        "      #print(len(train_research_problem_sentences[i]))\n",
        "      for j in range(len(train_research_problem_sentences[i])):\n",
        "        print(train_research_problem_sentences[i][j])\n",
        "        print(\"-----------------\")\n",
        "        print(sentences)\n",
        "        nlp_doc1 = nlp(train_research_problem_sentences[i][j])\n",
        "\n",
        "\n",
        "      nlp_doc2 = nlp(sentences)\n",
        "\n",
        "      sim = nlp_doc1.similarity(nlp_doc2)\n",
        "      print(sim)\n",
        "      if sim > 0.9:\n",
        "          '''print(\"reserch problem : \", nlp_doc1)\n",
        "          print(\"\")\n",
        "          print(\"Sentence :\" ,  sentences[i])\n",
        "          #print(nlp_doc1)\n",
        "          print(\"similarity : \" , sim)\n",
        "\n",
        "          print(\"---------------------------\")\n",
        "\n",
        "          df2 = {'Sentence': sentences[i], 'RP': nlp_doc1, 'prop': sim}\n",
        "          df = df.append(df2, ignore_index = True)'''\n",
        "          return 1\n",
        "      else:\n",
        "          return 0"
      ],
      "metadata": {
        "id": "cOR16lSj2-9E"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''sentences = []\n",
        "labels = []\n",
        "for i, article in enumerate(train_articles):\n",
        "    for sentence in article.split('\\n')[0:-1]:\n",
        "        sentences.append(sentence)\n",
        "\n",
        "        labels.append(int(check_similarity(sentence, train_research_problem_sentences)))'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBClMPSJ6tQ7",
        "outputId": "e48b5222-8d03-41d6-ce2c-751c28110582"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a hierarchical model for data - to - text generation\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a hierarchical model for data - to - text generation\n",
            "0.8598829851373043\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "0.9999999782736635\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these structures generally regroup multiple elements , as well as their attributes .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these structures generally regroup multiple elements , as well as their attributes .\n",
            "0.8531945241192951\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "most attempts rely on translation encoder - decoder methods which linearize elements into a sequence .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "most attempts rely on translation encoder - decoder methods which linearize elements into a sequence .\n",
            "0.8350219177049956\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this however loses most of the structure contained in the data .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this however loses most of the structure contained in the data .\n",
            "0.8620388045392855\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this work , we propose to overpass this limitation with a hierarchical model that encodes the data - structure at the element - level and the structure level .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this work , we propose to overpass this limitation with a hierarchical model that encodes the data - structure at the element - level and the structure level .\n",
            "0.8927324544650109\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "evaluations on rotowire show the effectiveness of our model w.r.t. qualitative and quantitative metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "evaluations on rotowire show the effectiveness of our model w.r.t. qualitative and quantitative metrics .\n",
            "0.7625508864053911\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "knowledge and / or data is often modeled in a structure , such as indexes , tables , key - value pairs , or triplets .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "knowledge and / or data is often modeled in a structure , such as indexes , tables , key - value pairs , or triplets .\n",
            "0.8966559849552818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these data , by their nature ( e.g. , raw data or long time - series data ) , are not easily usable by humans ; outlining their crucial need to be synthesized .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these data , by their nature ( e.g. , raw data or long time - series data ) , are not easily usable by humans ; outlining their crucial need to be synthesized .\n",
            "0.9100297164158173\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "recently , numerous works have focused on leveraging structured data in various applications , such as question answering or table retrieval .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "recently , numerous works have focused on leveraging structured data in various applications , such as question answering or table retrieval .\n",
            "0.900890054179623\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "one emerging research field consists in transcribing data - structures into natural language in order to ease their understandablity and their usablity .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "one emerging research field consists in transcribing data - structures into natural language in order to ease their understandablity and their usablity .\n",
            "0.9185557833777703\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this field is referred to as \" data - to - text \" and has its place in several application domains ( such as journalism or medical diagnosis ) or wide - audience applications ( such as financial and weather reports , or sport broadcasting ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this field is referred to as \" data - to - text \" and has its place in several application domains ( such as journalism or medical diagnosis ) or wide - audience applications ( such as financial and weather reports , or sport broadcasting ) .\n",
            "0.9250775931674258\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as an example , shows a data - structure containing statistics on nba basketball games , paired with its corresponding journalistic description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as an example , shows a data - structure containing statistics on nba basketball games , paired with its corresponding journalistic description .\n",
            "0.8824244185749059\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "designing data - to - text models gives rise to two main challenges :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "designing data - to - text models gives rise to two main challenges :\n",
            "0.8897325529761707\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 ) understanding structured data and 2 ) generating associated descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 ) understanding structured data and 2 ) generating associated descriptions .\n",
            "0.8289630179108484\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "recent datato - text models mostly rely on an encoder - decoder architecture in which the data - structure is first encoded sequentially into a fixed - size vectorial representation by an encoder .\n",
            "0.869842283055873\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "then , a decoder generates words conditioned on this representation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "then , a decoder generates words conditioned on this representation .\n",
            "0.8362364314804818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with the introduction of the attention mechanism on one hand , which computes a context focused on important elements from the input at each decoding step and , on the other hand , the copy mechanism to deal with unknown or rare words , these systems produce fluent and domain comprehensive texts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with the introduction of the attention mechanism on one hand , which computes a context focused on important elements from the input at each decoding step and , on the other hand , the copy mechanism to deal with unknown or rare words , these systems produce fluent and domain comprehensive texts .\n",
            "0.9042379355329258\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for instance , roberti et al. train a characterwise encoder - decoder to generate descriptions of restaurants based on their attributes , while puduppully et al .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for instance , roberti et al. train a characterwise encoder - decoder to generate descriptions of restaurants based on their attributes , while puduppully et al .\n",
            "0.8153659618379184\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "design a more complex two - step decoder : they first generate a plan of elements to be mentioned , and then condition text generation on this plan .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "design a more complex two - step decoder : they first generate a plan of elements to be mentioned , and then condition text generation on this plan .\n",
            "0.9060794873317075\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "although previous work yield over all good results , we identify two important caveats , that hinder precision ( i.e. factual mentions ) in the descriptions :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "although previous work yield over all good results , we identify two important caveats , that hinder precision ( i.e. factual mentions ) in the descriptions :\n",
            "0.8755439864796325\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 .\n",
            "0.48926289646957233\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "linearization of the data - structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "linearization of the data - structure .\n",
            "0.7933571769348471\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in practice , most works focus on introducing innovating decoding modules , and still represent data as a unique sequence of elements to be encoded .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in practice , most works focus on introducing innovating decoding modules , and still represent data as a unique sequence of elements to be encoded .\n",
            "0.9076436836855873\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , the table from would be linearized to [ ( hawks , h/ v , h ) , ... ,\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , the table from would be linearized to [ ( hawks , h/ v , h ) , ... ,\n",
            "0.7757291994823714\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( magic , h/v , v ) , ... ] , effectively leading to losing distinction between rows , and therefore entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( magic , h/v , v ) , ... ] , effectively leading to losing distinction between rows , and therefore entities .\n",
            "0.7871048986629275\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to the best of our knowledge , only liu et al.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to the best of our knowledge , only liu et al.\n",
            "0.7801729271918382\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "propose encoders constrained by the structure but these approaches are designed for single - entity structures .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "propose encoders constrained by the structure but these approaches are designed for single - entity structures .\n",
            "0.8527126364221488\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "arbitrary ordering of unordered collections in recurrent networks ( rnn ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "arbitrary ordering of unordered collections in recurrent networks ( rnn ) .\n",
            "0.6966195501798353\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "most data - to - text systems use rnns as encoders ( such as grus or lstms ) , these architectures have however some limitations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "most data - to - text systems use rnns as encoders ( such as grus or lstms ) , these architectures have however some limitations .\n",
            "0.8835257134706364\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "indeed , they require in practice their input to be fed sequentially .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "indeed , they require in practice their input to be fed sequentially .\n",
            "0.8611612005812839\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this way of encoding unordered sequences ( i.e. collections of entities ) implicitly assumes an arbitrary order within the collection which , as demonstrated by vinyals et al. , significantly impacts the learning performance .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this way of encoding unordered sequences ( i.e. collections of entities ) implicitly assumes an arbitrary order within the collection which , as demonstrated by vinyals et al. , significantly impacts the learning performance .\n",
            "0.8841148880614508\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to address these shortcomings , we propose a new structured - data encoder assuming that structures should be hierarchically captured .\n",
            "0.8985601376954205\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our contribution focuses on the encoding of the data - structure , thus the decoder is chosen to be a classical module as used in .\n",
            "0.9045162223840025\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our contribution is threefold :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our contribution is threefold :\n",
            "0.6555691386797818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- we model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - we introduce the transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - we integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- we model the general structure of the data using a two - level architecture , first encoding all entities on the basis of their elements , then encoding the data structure on the basis of its entities ; - we introduce the transformer encoder in data - to - text models to ensure robust encoding of each element / entities in comparison to all others , no matter their initial positioning ; - we integrate a hierarchical attention mechanism to compute the hierarchical context fed into the decoder .\n",
            "0.9159724989342597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we report experiments on the rotowire benchmark which contains around 5 k statistical tables of nba basketball games paired with humanwritten descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we report experiments on the rotowire benchmark which contains around 5 k statistical tables of nba basketball games paired with humanwritten descriptions .\n",
            "0.8133098560482148\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our model is compared to several state - of - the - art models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our model is compared to several state - of - the - art models .\n",
            "0.834707492963496\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results show that the proposed architecture outperforms previous models on bleu score and is generally better on qualitative metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results show that the proposed architecture outperforms previous models on bleu score and is generally better on qualitative metrics .\n",
            "0.8179844140046225\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the following , we first present a state - of - the art of data - to - text literature ( section 2 ) , and then describe our proposed hierarchical data encoder ( section 3 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the following , we first present a state - of - the art of data - to - text literature ( section 2 ) , and then describe our proposed hierarchical data encoder ( section 3 ) .\n",
            "0.8993994592959818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the evaluation protocol is presented in section 4 , followed by the results ( section 5 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the evaluation protocol is presented in section 4 , followed by the results ( section 5 ) .\n",
            "0.8079447612843292\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "section 6 concludes the paper and presents perspectives .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "section 6 concludes the paper and presents perspectives .\n",
            "0.7718368681131558\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "until recently , efforts to bring out semantics from structured - data relied heavily on expert knowledge .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "until recently , efforts to bring out semantics from structured - data relied heavily on expert knowledge .\n",
            "0.9050069679848514\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , in order to better transcribe numerical time series of weather data to a textual forecast , reiter et al .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , in order to better transcribe numerical time series of weather data to a textual forecast , reiter et al .\n",
            "0.8660755891651002\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "devise complex template schemes in collaboration with weather experts to build a consistent set of data - to - word rules .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "devise complex template schemes in collaboration with weather experts to build a consistent set of data - to - word rules .\n",
            "0.911020290693592\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "modern approaches to the wide range of tasks based on structured - data ( e.g. table retrieval , table classification , question answering ) now propose to leverage progress in deep learning to represent these data into a semantic vector space ( also called embedding space ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "modern approaches to the wide range of tasks based on structured - data ( e.g. table retrieval , table classification , question answering ) now propose to leverage progress in deep learning to represent these data into a semantic vector space ( also called embedding space ) .\n",
            "0.9267168180418587\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in parallel , an emerging task , called \" data - to - text \" , aims at describing structured data into a natural language description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in parallel , an emerging task , called \" data - to - text \" , aims at describing structured data into a natural language description .\n",
            "0.9711248870891918\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this task stems from the neural machine translation ( nmt ) domain , and early work represent the data records as a single sequence of facts to be entirely translated into natural language .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this task stems from the neural machine translation ( nmt ) domain , and early work represent the data records as a single sequence of facts to be entirely translated into natural language .\n",
            "0.9213252432644725\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "wiseman et al .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "wiseman et al .\n",
            "0.2275206685373309\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "show the limits of traditional nmt systems on larger structured - data , where nmt systems fail to accurately extract salient elements .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "show the limits of traditional nmt systems on larger structured - data , where nmt systems fail to accurately extract salient elements .\n",
            "0.8553407898913429\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to improve these models , a number of work proposed innovating decoding modules based on planning and templates , to ensure factual and coherent mentions of records in generated descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to improve these models , a number of work proposed innovating decoding modules based on planning and templates , to ensure factual and coherent mentions of records in generated descriptions .\n",
            "0.8927496746597895\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , puduppully et al. propose a two - step decoder which first targets specific records and then use them as a plan for the actual text generation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , puduppully et al. propose a two - step decoder which first targets specific records and then use them as a plan for the actual text generation .\n",
            "0.9018956225590289\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "similarly ,\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "similarly ,\n",
            "0.6310327514643989\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "li et al .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "li et al .\n",
            "0.3300406337174864\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "proposed a delayed copy mechanism where their decoder also acts in two steps :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "proposed a delayed copy mechanism where their decoder also acts in two steps :\n",
            "0.8364006109532298\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 ) using a classical lstm decoder to generate delexicalized text and 2 ) using a pointer network to replace placeholders by records from the input data .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 ) using a classical lstm decoder to generate delexicalized text and 2 ) using a pointer network to replace placeholders by records from the input data .\n",
            "0.8515792014105453\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "closer to our work , very recent work have proposed to take into account the data structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "closer to our work , very recent work have proposed to take into account the data structure .\n",
            "0.8929958310561903\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "more particularly ,\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "more particularly ,\n",
            "0.7446343705319853\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "puduppully et al. follow entity - centric theories and propose a model based on dynamic entity representation at decoding time .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "puduppully et al. follow entity - centric theories and propose a model based on dynamic entity representation at decoding time .\n",
            "0.8523674481198235\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it consists in conditioning the decoder on entity representations thatare updated during inference at each decoding step .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it consists in conditioning the decoder on entity representations thatare updated during inference at each decoding step .\n",
            "0.8262709604097158\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "on the other hand , liu et al.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "on the other hand , liu et al.\n",
            "0.6651337289257082\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rather focus on introducing structure into the encoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rather focus on introducing structure into the encoder .\n",
            "0.8478360494376789\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for instance , they propose a dual encoder which encodes separately the sequence of element names and the sequence of element values .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for instance , they propose a dual encoder which encodes separately the sequence of element names and the sequence of element values .\n",
            "0.8442349729663399\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these approaches are however designed for single - entity data structures and do not account for delimitation between entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these approaches are however designed for single - entity data structures and do not account for delimitation between entities .\n",
            "0.8848068627194136\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our contribution differs from previous work in several aspects .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our contribution differs from previous work in several aspects .\n",
            "0.8240371698298057\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "first , instead of flatly concatenating elements from the data- structure and encoding them as a sequence , we constrain the encoding to the underlying structure of the input data , so that the delimitation between entities remains clear throughout the process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "first , instead of flatly concatenating elements from the data- structure and encoding them as a sequence , we constrain the encoding to the underlying structure of the input data , so that the delimitation between entities remains clear throughout the process .\n",
            "0.8909908068765271\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "second , unlike all works in the domain , we exploit the transformer architecture and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "second , unlike all works in the domain , we exploit the transformer architecture and leverage its particularity to directly compare elements with each others in order to avoid arbitrary assumptions on their ordering .\n",
            "0.8700976716355244\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "finally , in contrast to that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities , we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "finally , in contrast to that use a complex updating mechanism to obtain a dynamic representation of the input data and its entities , we argue that explicit hierarchical encoding naturally guides the decoding process via hierarchical attention .\n",
            "0.9041397936301218\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hierarchical encoder model for data - to - text\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hierarchical encoder model for data - to - text\n",
            "0.8011482681770715\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this section we introduce our proposed hierarchical model taking into account the data structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this section we introduce our proposed hierarchical model taking into account the data structure .\n",
            "0.8748305555465382\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we outline that the decoding component aiming to generate descriptions is considered as a black - box module so that our contribution is focused on the encoding module .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we outline that the decoding component aiming to generate descriptions is considered as a black - box module so that our contribution is focused on the encoding module .\n",
            "0.9025143308267227\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we first describe the model overview , before detailing the hierarchical encoder and the associated hierarchical attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we first describe the model overview , before detailing the hierarchical encoder and the associated hierarchical attention .\n",
            "0.8650329236813503\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "notation and general overview\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "notation and general overview\n",
            "0.7201300266962837\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let 's consider the following notations :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let 's consider the following notations :\n",
            "0.7804063027422385\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "an entity e i is a set of j i unordered records {r i , 1 , ... , r i , j , ... , r i , ji } ; where record r i , j is defined as a pair of key k i , j and value v i , j .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "an entity e i is a set of j i unordered records {r i , 1 , ... , r i , j , ... , r i , ji } ; where record r i , j is defined as a pair of key k i , j and value v i , j .\n",
            "0.7101485384816167\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we outline that j i might differ between entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we outline that j i might differ between entities .\n",
            "0.811030913290971\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a data - structure sis an unordered set of i entities e i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a data - structure sis an unordered set of i entities e i .\n",
            "0.8034547456361036\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we thus denote s := {e 1 , ... , e i , ... , e i }.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we thus denote s := {e 1 , ... , e i , ... , e i }.\n",
            "0.6529775276163878\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each data - structure , a textual description y is associated .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each data - structure , a textual description y is associated .\n",
            "0.8977948627816724\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we refer to the first t words of a description y as y 1:t .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we refer to the first t words of a description y as y 1:t .\n",
            "0.7957679838106332\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus , the full sequence of words can be noted as y = y 1:t .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus , the full sequence of words can be noted as y = y 1:t .\n",
            "0.8166323873462725\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dataset dis a collection of n aligned ( data- structure , description ) pairs ( s , y ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dataset dis a collection of n aligned ( data- structure , description ) pairs ( s , y ) .\n",
            "0.7795975886966255\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for instance , illustrates a data - structure associated with a description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for instance , illustrates a data - structure associated with a description .\n",
            "0.9006757065751072\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the data - structure includes a set of entities ( hawks , magic , al horford , jeff teague , ... ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the data - structure includes a set of entities ( hawks , magic , al horford , jeff teague , ... ) .\n",
            "0.7938943845936179\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the entity jeff teague is modeled as a set of records { ( pts , 17 ) , ( reb , 0 ) , ( ast , 7 ) ...} in which , e.g. , the record ( pts , 17 ) is characterized by a key ( pts ) and a value .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the entity jeff teague is modeled as a set of records { ( pts , 17 ) , ( reb , 0 ) , ( ast , 7 ) ...} in which , e.g. , the record ( pts , 17 ) is characterized by a key ( pts ) and a value .\n",
            "0.7556738106011284\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each data - structure sin d , the objective function aims to generate a description ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each data - structure sin d , the objective function aims to generate a description ?\n",
            "0.8937333911294953\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as close as possible to the ground truth y .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as close as possible to the ground truth y .\n",
            "0.8282814007489979\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this objective function optimizes the following log - likelihood over the whole dataset d:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this objective function optimizes the following log - likelihood over the whole dataset d:\n",
            "0.8322058858609068\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where ?\n",
            "0.651695237194094\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "stands for the model parameters and p (? = y | s ; ? ) the probability of the model to generate the adequate description y for table s.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "stands for the model parameters and p (? = y | s ; ? ) the probability of the model to generate the adequate description y for table s.\n",
            "0.8021621231720499\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "during inference , we generate the sequence ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "during inference , we generate the sequence ?\n",
            "0.8235297325864612\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "* with the maximum a posteriori probability conditioned on table s .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "* with the maximum a posteriori probability conditioned on table s .\n",
            "0.7602374307505978\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "using the chain rule , we get :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "using the chain rule , we get :\n",
            "0.8067736273405568\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this equation is intractable in practice , we approximate a solution using beam search , as in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this equation is intractable in practice , we approximate a solution using beam search , as in .\n",
            "0.8659454332907224\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our model follows the encoder - decoder architecture .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our model follows the encoder - decoder architecture .\n",
            "0.7380757697513999\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "because our contribution focuses on the encoding process , we chose the decoding module used in : a two - layers lstm network with a copy mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "because our contribution focuses on the encoding process , we chose the decoding module used in : a two - layers lstm network with a copy mechanism .\n",
            "0.8913766252614989\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to supervise this mechanism , we assume that each record value that also appears in the target is copied from the data- structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to supervise this mechanism , we assume that each record value that also appears in the target is copied from the data- structure and we train the model to switch between freely generating words from the vocabulary and copying words from the input .\n",
            "0.8954261317437455\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we now describe the hierarchical encoder and the hierarchical attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we now describe the hierarchical encoder and the hierarchical attention .\n",
            "0.8343708084107021\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hierarchical encoding model\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hierarchical encoding model\n",
            "0.4858051314835472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as outlined in section 2 , most previous work ] make use of flat encoders that do not exploit the data structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as outlined in section 2 , most previous work ] make use of flat encoders that do not exploit the data structure .\n",
            "0.8942609007126259\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to keep the semantics of each element from the data - structure , we propose a hierarchical encoder which relies on two modules .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to keep the semantics of each element from the data - structure , we propose a hierarchical encoder which relies on two modules .\n",
            "0.8930347621661552\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the first one ( module a in is called low - level encoder and encodes entities on the basis of their records ; the second one ( module b ) , called high - level encoder , encodes the data - structure on the basis of its underlying entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the first one ( module a in is called low - level encoder and encodes entities on the basis of their records ; the second one ( module b ) , called high - level encoder , encodes the data - structure on the basis of its underlying entities .\n",
            "0.8857587829104205\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the low - level encoder , the traditional embedding layer is replaced by a record embedding layer as in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the low - level encoder , the traditional embedding layer is replaced by a record embedding layer as in .\n",
            "0.8430814585048317\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we present in what follows the record embedding layer and introduce our two hierarchical modules .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we present in what follows the record embedding layer and introduce our two hierarchical modules .\n",
            "0.8594791543106927\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "record embedding layer .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "record embedding layer .\n",
            "0.6452383427596794\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the first layer of the network consists in learning two embedding matrices to embed the record keys and values .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the first layer of the network consists in learning two embedding matrices to embed the record keys and values .\n",
            "0.8405329147820053\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "keys k i , j are embedded to k i , j ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "keys k i , j are embedded to k i , j ?\n",
            "0.6313816203248774\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd and values v i , j to v i , j ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd and values v i , j to v i , j ?\n",
            "0.5552292798486834\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd , with d the size of the embedding .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd , with d the size of the embedding .\n",
            "0.7602493593900718\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as in previous work , each record embedding r i , j is computed by a linear projection on the concatenation [k i , j ; v i , j ] followed by anon linearity :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as in previous work , each record embedding r i , j is computed by a linear projection on the concatenation [k i , j ; v i , j ] followed by anon linearity :\n",
            "0.767710420086165\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where w r ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where w r ?\n",
            "0.48989002370156404\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "r 2dd and b r ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "r 2dd and b r ?\n",
            "0.4578523584876666\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd are learnt parameters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd are learnt parameters .\n",
            "0.6837650515498175\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the low - level encoder aims at encoding a collection of records belonging to the same entity while the high - level encoder encodes the whole set of entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the low - level encoder aims at encoding a collection of records belonging to the same entity while the high - level encoder encodes the whole set of entities .\n",
            "0.857017231259592\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "both the low - level and high - level encoders consider their input elements as unordered .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "both the low - level and high - level encoders consider their input elements as unordered .\n",
            "0.8333220190440693\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use the transformer architecture from .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use the transformer architecture from .\n",
            "0.786712567678037\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each encoder , we have the following peculiarities :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each encoder , we have the following peculiarities :\n",
            "0.8211793798022329\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the low - level encoder encodes each entity e ion the basis of its record embeddings r i , j .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the low - level encoder encodes each entity e ion the basis of its record embeddings r i , j .\n",
            "0.7771689260972758\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each record embedding r i , j is compared to other record embeddings to learn its final hidden representation h i , j .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each record embedding r i , j is compared to other record embeddings to learn its final hidden representation h i , j .\n",
            "0.8063941564656213\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "furthermore , we add a special record [ ent ] for each entity , illustrated in as the last record .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "furthermore , we add a special record [ ent ] for each entity , illustrated in as the last record .\n",
            "0.8750249817359096\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "since entities might have a variable number of records , this token allows to aggregate final hidden record representations {h i , j } ji j= 1 in a fixedsized representation vector hi .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "since entities might have a variable number of records , this token allows to aggregate final hidden record representations {h i , j } ji j= 1 in a fixedsized representation vector hi .\n",
            "0.8485760513859671\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the high - level encoder encodes the data - structure on the basis of its entity representation hi .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the high - level encoder encodes the data - structure on the basis of its entity representation hi .\n",
            "0.8573623190468168\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "similarly to the low - level encoder , the final hidden state e i of an entity is computed by comparing entity representation hi with each others .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "similarly to the low - level encoder , the final hidden state e i of an entity is computed by comparing entity representation hi with each others .\n",
            "0.8760001355345134\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the data - structure representation z is computed as the mean of these entity representations , and is used for the decoder initialization .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the data - structure representation z is computed as the mean of these entity representations , and is used for the decoder initialization .\n",
            "0.8765217417527231\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hierarchical attention\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hierarchical attention\n",
            "0.5994374306835257\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to fully leverage the hierarchical structure of our encoder , we propose two variants of hierarchical attention mechanism to compute the context fed to the decoder module .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to fully leverage the hierarchical structure of our encoder , we propose two variants of hierarchical attention mechanism to compute the context fed to the decoder module .\n",
            "0.8432262657048144\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "traditional hierarchical attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "traditional hierarchical attention .\n",
            "0.7505869250978349\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as in , we hypothesize that a dynamic context should be computed in two steps : first attending to entities , then to records corresponding to these entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as in , we hypothesize that a dynamic context should be computed in two steps : first attending to entities , then to records corresponding to these entities .\n",
            "0.9025896412509458\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to implement this hierarchical attention , at each decoding step t , the model learns a first set of attention scores ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to implement this hierarchical attention , at each decoding step t , the model learns a first set of attention scores ?\n",
            "0.8792361346124583\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , t over entities e i and a second set of attention scores ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , t over entities e i and a second set of attention scores ?\n",
            "0.8056149562676153\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , j,t over records r i , j belonging to entity e i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , j,t over records r i , j belonging to entity e i .\n",
            "0.6687592111569287\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the ?\n",
            "0.6825668072808502\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , t scores are normalized to form a distribution over all entities e i , and ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , t scores are normalized to form a distribution over all entities e i , and ?\n",
            "0.822093731143092\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , j, t scores are normalized to form a distribution over records r i , j of entity e i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , j, t scores are normalized to form a distribution over records r i , j of entity e i .\n",
            "0.7623647885891877\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each entity is then represented as a weighted sum of its record embeddings , and the entire data structure is represented as a weighted sum of the entity representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each entity is then represented as a weighted sum of its record embeddings , and the entire data structure is represented as a weighted sum of the entity representations .\n",
            "0.847960813248164\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dynamic context is computed as :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dynamic context is computed as :\n",
            "0.8255348553027343\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where d t is the decoder hidden state at time step t , w ? ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where d t is the decoder hidden state at time step t , w ? ?\n",
            "0.7650664096006629\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "r dd and w ? ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "r dd and w ? ?\n",
            "0.47379883162770736\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "r dd are learnt parameters , i ? i , t = 1 , and for all i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "r dd are learnt parameters , i ? i , t = 1 , and for all i ?\n",
            "0.6975314095912354\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "{ 1 , ... , i } j ? i , j, t = 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "{ 1 , ... , i } j ? i , j, t = 1 .\n",
            "0.5701649486069597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "key - guided hierarchical attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "key - guided hierarchical attention .\n",
            "0.8166715563303735\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this variant follows the intuition that once an entity is chosen for mention ( thanks to ? i , t ) , only the type of records is important to determine the content of the description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this variant follows the intuition that once an entity is chosen for mention ( thanks to ? i , t ) , only the type of records is important to determine the content of the description .\n",
            "0.8951082730462386\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , when deciding to mention a player , all experts automatically report his score without consideration of its specific value .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , when deciding to mention a player , all experts automatically report his score without consideration of its specific value .\n",
            "0.8820738070287929\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to test this intuition , we model the attention scores by computing the ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to test this intuition , we model the attention scores by computing the ?\n",
            "0.8383888839846395\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , j,t scores from equation solely on the embedding of the key rather than on the full record representation h i , j :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , j,t scores from equation solely on the embedding of the key rather than on the full record representation h i , j :\n",
            "0.8247148074021834\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "please note that the different embeddings and the model parameters presented in the model components are learnt using equation 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "please note that the different embeddings and the model parameters presented in the model components are learnt using equation 1 .\n",
            "0.8293032784457929\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experimental setup\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experimental setup\n",
            "0.48327630370559177\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the rotowire dataset\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the rotowire dataset\n",
            "0.5985732182590574\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to evaluate the effectiveness of our model , and demonstrate its flexibility at handling heavy data - structure made of several types of entities , we used the ro -towire dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to evaluate the effectiveness of our model , and demonstrate its flexibility at handling heavy data - structure made of several types of entities , we used the ro -towire dataset .\n",
            "0.8902732263352968\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it includes basketball games statistical tables paired with journalistic descriptions of the games , as can be seen in the example of .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it includes basketball games statistical tables paired with journalistic descriptions of the games , as can be seen in the example of .\n",
            "0.8544531371020602\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the descriptions are professionally written and average 337 words with a vocabulary size of 11.3k .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the descriptions are professionally written and average 337 words with a vocabulary size of 11.3k .\n",
            "0.8672939095872476\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "there are 39 different record keys , and the average number of records ( resp. entities ) in a single data - structure is 628 ( resp. 28 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "there are 39 different record keys , and the average number of records ( resp. entities ) in a single data - structure is 628 ( resp. 28 ) .\n",
            "0.8413705581939102\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "entities are of two types , either team or player , and player descriptions depend on their involvement in the game .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "entities are of two types , either team or player , and player descriptions depend on their involvement in the game .\n",
            "0.842104958133349\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we followed the data partitions introduced with the dataset and used a train / validation / test sets of respectively 3 , 398/727/728 ( data- structure , description ) pairs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we followed the data partitions introduced with the dataset and used a train / validation / test sets of respectively 3 , 398/727/728 ( data- structure , description ) pairs .\n",
            "0.8601415486229238\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "evaluation metrics\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "evaluation metrics\n",
            "0.49227623267003456\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we evaluate our model through two types of metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we evaluate our model through two types of metrics .\n",
            "0.8200153161761913\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the bleu score aims at measuring to what extent the generated descriptions are literally closed to the ground truth .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the bleu score aims at measuring to what extent the generated descriptions are literally closed to the ground truth .\n",
            "0.8545696546015012\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the second category designed by is more qualitative .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the second category designed by is more qualitative .\n",
            "0.8375072669063477\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bleu score .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bleu score .\n",
            "0.3942770860598376\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the bleu score is commonly used as an evaluation metric in text generation tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the bleu score is commonly used as an evaluation metric in text generation tasks .\n",
            "0.8833739399057139\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams ( n ? 1 , 2 , 3 , 4 ) between the generated candidate and the ground truth .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it estimates the correspondence between a machine output and that of a human by computing the number of co-occurrences for ngrams ( n ? 1 , 2 , 3 , 4 ) between the generated candidate and the ground truth .\n",
            "0.8654899085170928\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use the implementation code released by .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use the implementation code released by .\n",
            "0.8111187269944558\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "information extraction - oriented metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "information extraction - oriented metrics .\n",
            "0.7570616018385915\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these metrics estimate the ability of our model to integrate elements from the table in its descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these metrics estimate the ability of our model to integrate elements from the table in its descriptions .\n",
            "0.8653302884879599\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "particularly , they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "particularly , they compare the gold and generated descriptions and measure to what extent the extracted relations are aligned or differ .\n",
            "0.8764264431956578\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to do so , we follow the protocol presented in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to do so , we follow the protocol presented in .\n",
            "0.8587380461067018\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "first , we apply an information extraction ( ie ) system trained on labeled relations from the gold descriptions of the rotowire train dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "first , we apply an information extraction ( ie ) system trained on labeled relations from the gold descriptions of the rotowire train dataset .\n",
            "0.8903774247093474\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "entity - value pairs are extracted from the descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "entity - value pairs are extracted from the descriptions .\n",
            "0.8451019151155728\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , in the sentence isaiah thomas led the team in scoring , totaling 23 points [... ]. , an ie tool will extract the pair ( isaiah thomas , 23 , pts ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , in the sentence isaiah thomas led the team in scoring , totaling 23 points [... ]. , an ie tool will extract the pair ( isaiah thomas , 23 , pts ) .\n",
            "0.8085729500896472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "second , we compute three metrics on the extracted information :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "second , we compute three metrics on the extracted information :\n",
            "0.8266475889071931\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "relation generation ( rg ) estimates how well the system is able to generate text containing factual ( i.e. , correct ) records .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "relation generation ( rg ) estimates how well the system is able to generate text containing factual ( i.e. , correct ) records .\n",
            "0.8822702232564398\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we measure the precision and absolute number ( denoted respectively rg - p % and rg - # ) of unique relations r extracted from ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we measure the precision and absolute number ( denoted respectively rg - p % and rg - # ) of unique relations r extracted from ?\n",
            "0.7760620794297438\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1:t that also appear in s.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1:t that also appear in s.\n",
            "0.7642831608272623\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "content selection ( cs ) measures how well the generated document matches the gold document in terms of mentioned records .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "content selection ( cs ) measures how well the generated document matches the gold document in terms of mentioned records .\n",
            "0.8766460634161602\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we measure the precision and recall ( denoted respectively cs - p % and cs - r% ) of unique relations r extracted from ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we measure the precision and recall ( denoted respectively cs - p % and cs - r% ) of unique relations r extracted from ?\n",
            "0.7895950795621145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1:t thatare also extracted from y 1:t .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1:t thatare also extracted from y 1:t .\n",
            "0.632496089575885\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "content ordering ( co ) analyzes how well the system orders the records discussed in the description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "content ordering ( co ) analyzes how well the system orders the records discussed in the description .\n",
            "0.8689405861214794\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we measure the normalized damerau - levenshtein distance between the sequences of records extracted from ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we measure the normalized damerau - levenshtein distance between the sequences of records extracted from ?\n",
            "0.7823365687980781\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1:t thatare also extracted from y 1:t .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1:t thatare also extracted from y 1:t .\n",
            "0.632496089575885\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "cs primarily targets the \" what to say \" aspect of evaluation , co targets the \" how to say it \" aspect , and rg targets both .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "cs primarily targets the \" what to say \" aspect of evaluation , co targets the \" how to say it \" aspect , and rg targets both .\n",
            "0.8727030888266493\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "note that for cs , co , rg - % and bleu metrics , higher is better ; which is not true for rg -# .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "note that for cs , co , rg - % and bleu metrics , higher is better ; which is not true for rg -# .\n",
            "0.8049819347988892\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the ie system used in the experiments is able to extract an average of 17 factual records from gold descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the ie system used in the experiments is able to extract an average of 17 factual records from gold descriptions .\n",
            "0.8843388504844766\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to mimic a human expert , a generative system should approach this number and not overload generation with brute facts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to mimic a human expert , a generative system should approach this number and not overload generation with brute facts .\n",
            "0.8851004049198647\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "baselines\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "baselines\n",
            "0.08517900103327289\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we compare our hierarchical model against three systems .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we compare our hierarchical model against three systems .\n",
            "0.7815237091197029\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each of them , we report the results of the best performing models presented in each paper .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each of them , we report the results of the best performing models presented in each paper .\n",
            "0.8447959114272523\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "wiseman is a standard encoder - decoder system with copy mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "wiseman is a standard encoder - decoder system with copy mechanism .\n",
            "0.7236848864985778\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "li is a standard encoder - decoder with a delayed copy mechanism : text is first generated with placeholders , which are replaced by salient records extracted from the table by a pointer network .\n",
            "0.8745086607566901\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "puduppully - plan acts in two steps : a first standard encoder - decoder generates a plan , i.e. a list of salient records from the table ; a second standard encoder - decoder generates text from this plan .\n",
            "0.8798708824260653\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "puduppully - updt .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "puduppully - updt .\n",
            "0.5270708274079282\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it consists in a standard encoder - decoder , with an added module aimed at updating record representations during the generation process .\n",
            "0.8793115101058792\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "at each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "at each decoding step , a gated recurrent network computes which records should be updated and what should be their new representation .\n",
            "0.877916970159344\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "model scenarios\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "model scenarios\n",
            "0.5010919204421261\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we test the importance of the input structure by training different variants of the proposed architecture :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we test the importance of the input structure by training different variants of the proposed architecture :\n",
            "0.8291521101403486\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "flat , where we feed the input sequentially to the encoder , losing all notion of hierarchy .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "flat , where we feed the input sequentially to the encoder , losing all notion of hierarchy .\n",
            "0.8560394541811349\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as a consequence , the model uses standard attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as a consequence , the model uses standard attention .\n",
            "0.8646578961657141\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "is closest to wiseman , with the exception that we use a transformer to encode the input sequence instead of an rnn .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "is closest to wiseman , with the exception that we use a transformer to encode the input sequence instead of an rnn .\n",
            "0.8463432772829459\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hierarchical - kv is our full hierarchical model , with traditional hierarchical attention , i.e. where attention over records is computed on the full record encoding , as in equation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hierarchical - kv is our full hierarchical model , with traditional hierarchical attention , i.e. where attention over records is computed on the full record encoding , as in equation .\n",
            "0.893149767448856\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hierarchical -k is our full hierarchical model , with key - guided hierarchical attention , i.e. where attention over records is computed only on the record key representations , as in equation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hierarchical -k is our full hierarchical model , with key - guided hierarchical attention , i.e. where attention over records is computed only on the record key representations , as in equation .\n",
            "0.9056696672860213\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "implementation details\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "implementation details\n",
            "0.6027646744568035\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the decoder is the one used in with the same hyper - parameters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the decoder is the one used in with the same hyper - parameters .\n",
            "0.848821669509195\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for the encoder module , both the low - level and high - level encoders use a two - layers multi-head self - attention with two heads .\n",
            "0.8388703129595588\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to fit with the small number of record keys in our dataset , their embedding size is fixed to 20 .\n",
            "0.8669765365592821\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the size of the record value embeddings and hidden layers of the transformer encoders are both set to 300 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the size of the record value embeddings and hidden layers of the transformer encoders are both set to 300 .\n",
            "0.7974471252401865\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use dropout at rate 0.5 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use dropout at rate 0.5 .\n",
            "0.6673508628036634\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the models are trained with a batch size of 64 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the models are trained with a batch size of 64 .\n",
            "0.7972699550960152\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we follow the training procedure in and train the model for a fixed number of 25 k updates , and average the weights of the last 5 checkpoints ( at every 1 k updates ) to ensure more stability across runs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we follow the training procedure in and train the model for a fixed number of 25 k updates , and average the weights of the last 5 checkpoints ( at every 1 k updates ) to ensure more stability across runs .\n",
            "0.8355711755756248\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all models were trained with the adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 k steps .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all models were trained with the adam optimizer ; the initial learning rate is 0.001 , and is reduced by half every 10 k steps .\n",
            "0.8315551352943154\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we used beam search with beam size of 5 during inference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we used beam search with beam size of 5 during inference .\n",
            "0.7786304251016472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all the models are implemented in open nmt - py .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all the models are implemented in open nmt - py .\n",
            "0.7697468255377832\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all code is available at https://github.com/kaijuml/data-to-text-hierarchical\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all code is available at https://github.com/kaijuml/data-to-text-hierarchical\n",
            "0.7524417596928219\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results\n",
            "0.45462375196773575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our results on the rotowire testset are summarized in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our results on the rotowire testset are summarized in .\n",
            "0.8116934722205148\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each proposed variant of our architecture , we report the mean score over ten runs , as well as the standard deviation in subscript .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each proposed variant of our architecture , we report the mean score over ten runs , as well as the standard deviation in subscript .\n",
            "0.8642411327829385\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results are compared to baselines and variants of our models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results are compared to baselines and variants of our models .\n",
            "0.7753831394832672\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also report the result of the oracle ( metrics on the gold descriptions ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also report the result of the oracle ( metrics on the gold descriptions ) .\n",
            "0.8557927981272988\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "please note that gold descriptions trivially obtain 100 % on all metrics expect rg , as they are all based on comparison with themselves .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "please note that gold descriptions trivially obtain 100 % on all metrics expect rg , as they are all based on comparison with themselves .\n",
            "0.8455487153814906\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rg scores are different , as the ie system is imperfect and fails to extract accurate entities 4 % of the time .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rg scores are different , as the ie system is imperfect and fails to extract accurate entities 4 % of the time .\n",
            "0.8793859935683714\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rg -# is an absolute count .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rg -# is an absolute count .\n",
            "0.6831428713771923\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ablation studies\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ablation studies\n",
            "0.31934578434810756\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to evaluate the impact of our model components , we first compare scenarios flat , hierarchical - k , and hierarchical - kv .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to evaluate the impact of our model components , we first compare scenarios flat , hierarchical - k , and hierarchical - kv .\n",
            "0.8696702205110756\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as shown in , we can see the lower results obtained by the flat scenario compared to the other scenarios ( e.g. bleu 16.7 vs. 17.5 for resp .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as shown in , we can see the lower results obtained by the flat scenario compared to the other scenarios ( e.g. bleu 16.7 vs. 17.5 for resp .\n",
            "0.8371410153469547\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "flat and hierarchical -k ) , suggesting the effectiveness of encoding the data - structure using a hierarchy .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "flat and hierarchical -k ) , suggesting the effectiveness of encoding the data - structure using a hierarchy .\n",
            "0.8845680191815681\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this is expected , as losing explicit delimitation between entities makes it harder a ) for the encoder to encode semantics of the objects contained in the table and b ) for the attention mechanism to extract salient entities / records .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this is expected , as losing explicit delimitation between entities makes it harder a ) for the encoder to encode semantics of the objects contained in the table and b ) for the attention mechanism to extract salient entities / records .\n",
            "0.9045601568125202\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "second , the comparison between scenario hierarchical - kv and hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted cs - r% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "second , the comparison between scenario hierarchical - kv and hierarchical -k shows that omitting entirely the influence of the record values in the attention mechanism is more effective : this last variant performs slightly better in all metrics excepted cs - r% , reinforcing our intuition that focusing on the structure modeling is an important part of data encoding as well as confirming the intuition explained in section 3.3 : once an entity is selected , facts about this entity are relevant based on their key , not value which might add noise .\n",
            "0.9149989614122386\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to illustrate this intuition , we depict in attention scores ( recall ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to illustrate this intuition , we depict in attention scores ( recall ?\n",
            "0.8282845463523356\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , t and ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , t and ?\n",
            "0.6523375367609469\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i , j, t from equations and ) for both variants hierarchical - kv and hierarchical -k .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i , j, t from equations and ) for both variants hierarchical - kv and hierarchical -k .\n",
            "0.7922813496715492\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we particularly focus on the timestamp where the models should mention the number of points scored during the first quarter of the game .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we particularly focus on the timestamp where the models should mention the number of points scored during the first quarter of the game .\n",
            "0.8080192151070588\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "scores of hierarchical -k are sharp , with all of the weight on the correct record ( pts qtr1 , 26 ) whereas scores of hierarchical - kv are more distributed over all pts qtr records , ultimately failing to retrieve the correct one .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "scores of hierarchical -k are sharp , with all of the weight on the correct record ( pts qtr1 , 26 ) whereas scores of hierarchical - kv are more distributed over all pts qtr records , ultimately failing to retrieve the correct one .\n",
            "0.8564533302429428\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "over all models ; our best model hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "over all models ; our best model hierarchical -k reaching 17.5 vs. 16.5 against the best baseline .\n",
            "0.7566677846937684\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this means that our models learns to generate fluent sequences of words , close to the gold descriptions , adequately picking upon domain lingo .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this means that our models learns to generate fluent sequences of words , close to the gold descriptions , adequately picking upon domain lingo .\n",
            "0.902834028990408\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "qualitative metrics are either better or on par with baselines .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "qualitative metrics are either better or on par with baselines .\n",
            "0.7918525226906374\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we show in a text generated by our best model , which can be directly compared to the gold description in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we show in a text generated by our best model , which can be directly compared to the gold description in .\n",
            "0.88491757375733\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "generation is fluent and contains domain - specific expressions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "generation is fluent and contains domain - specific expressions .\n",
            "0.8524026321740745\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as reflected in , the number of correct mentions ( in green ) outweights the number of incorrect mentions ( in red ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as reflected in , the number of correct mentions ( in green ) outweights the number of incorrect mentions ( in red ) .\n",
            "0.8132503817329203\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "please note that , as in previous work , generated texts still contain a number of incorrect facts , as well hallucinations ( in blue ) : sentences that have no basis in the input data ( e.g. \" [... ] he 's now averaging 22 points [... ]. \" ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "please note that , as in previous work , generated texts still contain a number of incorrect facts , as well hallucinations ( in blue ) : sentences that have no basis in the input data ( e.g. \" [... ] he 's now averaging 22 points [... ]. \" ) .\n",
            "0.8996587530295971\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while not the direct focus of our work , this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while not the direct focus of our work , this highlights that any operation meant to enrich the semantics of structured data can also enrich the data with incorrect facts .\n",
            "0.9050356395411628\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "specifically , regarding all baselines , we can outline the following statements .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "specifically , regarding all baselines , we can outline the following statements .\n",
            "0.8549726625468056\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture wiseman , reinforcing the crucial role of structure in data semantics and saliency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our hierarchical models achieve significantly better scores on all metrics when compared to the flat architecture wiseman , reinforcing the crucial role of structure in data semantics and saliency .\n",
            "0.8429109245947374\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the analysis of rg metrics shows that wiseman seems to be the more naturalistic in terms of number of factual mentions ( rg# ) since it is the closest scenario to the gold value ( 16.83 vs. 17.31 for resp .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the analysis of rg metrics shows that wiseman seems to be the more naturalistic in terms of number of factual mentions ( rg# ) since it is the closest scenario to the gold value ( 16.83 vs. 17.31 for resp .\n",
            "0.8457653289636236\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "wiseman and hierarchical -k ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "wiseman and hierarchical -k ) .\n",
            "0.5758489579191608\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above wiseman .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , wiseman achieves only 75 . 62 % of precision , effectively mentioning on average a total of 22.25 records ( wrong or accurate ) , where our model hierarchical -k scores a precision of 89 . 46 % , leading to 23.66 total mentions , just slightly above wiseman .\n",
            "0.8237982250783532\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the comparison between the flat scenario and wiseman is particularly interesting .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the comparison between the flat scenario and wiseman is particularly interesting .\n",
            "0.807998200230621\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "indeed , these two models share the same intuition to flatten the data -structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "indeed , these two models share the same intuition to flatten the data -structure .\n",
            "0.8581923292078372\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the only difference stands on the encoder mechanism : bi - lstm vs. transformer , for wiseman and flat respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the only difference stands on the encoder mechanism : bi - lstm vs. transformer , for wiseman and flat respectively .\n",
            "0.7904721352326559\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results shows that our flat scenario obtains a significant higher bleu score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( rg - p % ) thatare also included in the gold descriptions ( cs - r% ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results shows that our flat scenario obtains a significant higher bleu score ( 16.7 vs. 14.5 ) and generates fluent descriptions with accurate mentions ( rg - p % ) thatare also included in the gold descriptions ( cs - r% ) .\n",
            "0.8235869481701589\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this suggests that introducing the transformer architecture is promising way to implicitly account for data structure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this suggests that introducing the transformer architecture is promising way to implicitly account for data structure .\n",
            "0.8837279902282819\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our hierarchical models outperform the two - step decoders of li and puduppully - plan on both bleu and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our hierarchical models outperform the two - step decoders of li and puduppully - plan on both bleu and all qualitative metrics , showing that capturing structure in the encoding process is more effective that predicting a structure in the decoder ( i.e. , planning or templating ) .\n",
            "0.9014111273601383\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while our models sensibly outperform in precision at factual mentions , the baseline puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while our models sensibly outperform in precision at factual mentions , the baseline puduppully - plan reaches 34.28 mentions on average , showing that incorporating modules dedicated to entity extraction leads to over- focusing on entities ; contrasting with our models that learn to generate more balanced descriptions .\n",
            "0.898872474394806\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the comparison with puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better content ordering ( co ) and rg - p% .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the comparison with puduppully - updt shows that dynamically updating the encoding across the generation process can lead to better content ordering ( co ) and rg - p% .\n",
            "0.8874236873069749\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , this does not help with content selection ( cs ) since our best model hierarchical -k obtains slightly better scores .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , this does not help with content selection ( cs ) since our best model hierarchical -k obtains slightly better scores .\n",
            "0.8575801709602026\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "indeed , puduppullyupdt updates representations after each mention allowing to keep track of the mention history .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "indeed , puduppullyupdt updates representations after each mention allowing to keep track of the mention history .\n",
            "0.8703343803781532\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this guides the ordering of mentions ( co metric ) , each step limiting more the number of candidate mentions ( increasing rg - p% ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this guides the ordering of mentions ( co metric ) , each step limiting more the number of candidate mentions ( increasing rg - p% ) .\n",
            "0.8367226606187803\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in contrast , our model encodes saliency among records / entities more effectively ( cs metric ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in contrast , our model encodes saliency among records / entities more effectively ( cs metric ) .\n",
            "0.8203587297371067\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we note that while our model encodes the data - structure once and for all , puduppully - updt recomputes , via the updates , the encoding at each step and therefore significantly increases computation complexity .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we note that while our model encodes the data - structure once and for all , puduppully - updt recomputes , via the updates , the encoding at each step and therefore significantly increases computation complexity .\n",
            "0.8946104014517043\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "combined with their rg -# score of 30.11 , we argue that our model is simpler , and obtains fluent description with accurate mentions in a more human - like fashion .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "combined with their rg -# score of 30.11 , we argue that our model is simpler , and obtains fluent description with accurate mentions in a more human - like fashion .\n",
            "0.8967204854594796\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we would also like to draw attention to the number of parameters used by those architectures .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we would also like to draw attention to the number of parameters used by those architectures .\n",
            "0.864510972379444\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we note that our scenarios relies on a lower number of parameters ( 14 millions ) compared to all baselines ( ranging from 23 to 45 millions ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we note that our scenarios relies on a lower number of parameters ( 14 millions ) compared to all baselines ( ranging from 23 to 45 millions ) .\n",
            "0.8375510516599789\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this outlines the effectiveness in the design of our model relying on a structure encoding , in contrast to other approach that try to learn the structure of data / descriptions from a linearized encoding .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this outlines the effectiveness in the design of our model relying on a structure encoding , in contrast to other approach that try to learn the structure of data / descriptions from a linearized encoding .\n",
            "0.8944834228109879\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "conclusion and future work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "conclusion and future work\n",
            "0.777043617808491\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this work we have proposed a hierarchical encoder for structured data , which 1 ) leverages the structure to form efficient representation of its input ; 2 ) has strong synergy with the hierarchical attention of its associated decoder .\n",
            "0.8922282788267726\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this results in an effective and more light - weight model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this results in an effective and more light - weight model .\n",
            "0.831473199514271\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experimental evaluation on the rotowire benchmark shows that our model outperforms competitive baselines in terms of bleu score and is generally better on qualitative metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experimental evaluation on the rotowire benchmark shows that our model outperforms competitive baselines in terms of bleu score and is generally better on qualitative metrics .\n",
            "0.8109841697642546\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this way of representing structured data bases may lead to automatic inference and enrichment , e.g. , by comparing entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this way of representing structured data bases may lead to automatic inference and enrichment , e.g. , by comparing entities .\n",
            "0.8899688859399353\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this direction could be driven by very recent operation - guided networks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this direction could be driven by very recent operation - guided networks .\n",
            "0.852204869315838\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in addition , we note that our approach can still lead to erroneous facts or even hallucinations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in addition , we note that our approach can still lead to erroneous facts or even hallucinations .\n",
            "0.8728499782248967\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "an interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "an interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions .\n",
            "0.8910508441964837\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "acknowledgements\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "acknowledgements\n",
            "0.06339469142519472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we would like to thank the h2020 project ai4eu ( 825619 ) which partially supports laure soulier and patrick gallinari .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we would like to thank the h2020 project ai4eu ( 825619 ) which partially supports laure soulier and patrick gallinari .\n",
            "0.7864988808793014\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a deep ensemble model with slot alignment for sequence - to - sequence natural language generation\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a deep ensemble model with slot alignment for sequence - to - sequence natural language generation\n",
            "0.8509003869462345\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "natural language generation lies at the core of generative dialogue systems and conversational agents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "natural language generation lies at the core of generative dialogue systems and conversational agents .\n",
            "0.824304479103227\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we describe an ensemble neural language generator , and present several novel methods for data representation and augmentation that yield improved results in our model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we describe an ensemble neural language generator , and present several novel methods for data representation and augmentation that yield improved results in our model .\n",
            "0.8811710498559356\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we test the model on three datasets in the restaurant , tv and laptop domains , and report both objective and subjective evaluations of our best model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we test the model on three datasets in the restaurant , tv and laptop domains , and report both objective and subjective evaluations of our best model .\n",
            "0.8427555893471044\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "using a range of automatic metrics , as well as human evaluators , we show that our approach achieves better results than state - of - the - art models on the same datasets .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "using a range of automatic metrics , as well as human evaluators , we show that our approach achieves better results than state - of - the - art models on the same datasets .\n",
            "0.9002007729560698\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "there has recently been a substantial amount of research in natural language processing ( nlp ) in the context of personal assistants , such as cortana or alexa .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "there has recently been a substantial amount of research in natural language processing ( nlp ) in the context of personal assistants , such as cortana or alexa .\n",
            "0.8882640449102907\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the capabilities of these conversational agents are still fairly limited and lacking in various aspects , one of the most challenging of which is the ability to produce utterances with humanlike coherence and naturalness for many different kinds of content .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the capabilities of these conversational agents are still fairly limited and lacking in various aspects , one of the most challenging of which is the ability to produce utterances with humanlike coherence and naturalness for many different kinds of content .\n",
            "0.8601916169239826\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this is the responsibility of the natural language generation ( nlg ) component .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this is the responsibility of the natural language generation ( nlg ) component .\n",
            "0.8522561380991795\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our work focuses on language generators whose inputs are structured meaning representations ( mrs ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our work focuses on language generators whose inputs are structured meaning representations ( mrs ) .\n",
            "0.8636457630086883\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "an mr describes a single dialogue act with a list of key concepts which need to be conveyed to the human user during the dialogue .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "an mr describes a single dialogue act with a list of key concepts which need to be conveyed to the human user during the dialogue .\n",
            "0.8932592851841874\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each piece of information is represented by a slotvalue pair , where the slot identifies the type of information and the value is the corresponding content .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each piece of information is represented by a slotvalue pair , where the slot identifies the type of information and the value is the corresponding content .\n",
            "0.8629914172131835\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "dialogue act ( da ) types vary depending on the dialogue manager , ranging from simple ones , such as a goodbye da with no slots at all , to complex ones , such as an inform da containing multiple slots with various types of values ( see example in utt .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "dialogue act ( da ) types vary depending on the dialogue manager , ranging from simple ones , such as a goodbye da with no slots at all , to complex ones , such as an inform da containing multiple slots with various types of values ( see example in utt .\n",
            "0.8802080133204194\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "located near\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "located near\n",
            "0.3344310837474822\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the bakers , kid - friendly restaurant , the golden curry , offers japanese cuisine with a moderate price range .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the bakers , kid - friendly restaurant , the golden curry , offers japanese cuisine with a moderate price range .\n",
            "0.7156680331760356\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a natural language generator must produce a syntactically and semantically correct utterance from a given mr .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a natural language generator must produce a syntactically and semantically correct utterance from a given mr .\n",
            "0.8309643718112998\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the utterance should express all the information contained in the mr , in a natural and conversational way .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the utterance should express all the information contained in the mr , in a natural and conversational way .\n",
            "0.8734426352129757\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in traditional language generator architectures , the assembling of an utterance from an mr is performed in two stages : sentence planning , which enforces semantic correctness and determines the structure of the utterance , and surface realization , which enforces syntactic correctness and produces the final utterance form .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in traditional language generator architectures , the assembling of an utterance from an mr is performed in two stages : sentence planning , which enforces semantic correctness and determines the structure of the utterance , and surface realization , which enforces syntactic correctness and produces the final utterance form .\n",
            "0.8722906323140375\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "earlier work on statistical nlg approaches were typically hybrids of a handcrafted component and a statistical training method .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "earlier work on statistical nlg approaches were typically hybrids of a handcrafted component and a statistical training method .\n",
            "0.852167948805757\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the handcrafted aspects , however , lead to decreased portability and potentially limit the variability of the outputs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the handcrafted aspects , however , lead to decreased portability and potentially limit the variability of the outputs .\n",
            "0.8287043706445012\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "new corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their mrs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "new corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their mrs .\n",
            "0.8778497276816907\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the alignment provides valuable information during training , but the semantic annotation is costly .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the alignment provides valuable information during training , but the semantic annotation is costly .\n",
            "0.8723678448988991\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the most recent methods do not require aligned data and use an end - to - end approach to training , performing sentence planning and surface realization simultaneously .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the most recent methods do not require aligned data and use an end - to - end approach to training , performing sentence planning and surface realization simultaneously .\n",
            "0.9160321842228969\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the most successful systems trained on unaligned data use recurrent neural networks ( rnns ) paired with an encoder - decoder system design , but also other concepts , such as imitation learning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the most successful systems trained on unaligned data use recurrent neural networks ( rnns ) paired with an encoder - decoder system design , but also other concepts , such as imitation learning .\n",
            "0.8731088366450939\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these nlg models , however , typically require greater amount of data for training due to the lack of semantic alignment , and they still have problems producing syntactically and semantically correct output , as well as being limited in naturalness .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these nlg models , however , typically require greater amount of data for training due to the lack of semantic alignment , and they still have problems producing syntactically and semantically correct output , as well as being limited in naturalness .\n",
            "0.8837923694920127\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we present a neural ensemble natural language generator , which we train and test on three large unaligned datasets in the restaurant , television , and laptop domains .\n",
            "0.8796099361369368\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we explore novel ways to represent the mr inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we explore novel ways to represent the mr inputs , including novel methods for delexicalizing slots and their values , automatic slot alignment , as well as the use of a semantic reranker .\n",
            "0.8882825592313002\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .\n",
            "0.8304278983775941\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "on the largest of the datasets , the e2e dataset with nearly 50 k samples , we also demonstrate that our model significantly outperforms the baseline e2e nlg challenge 1 system in human evaluation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "on the largest of the datasets , the e2e dataset with nearly 50 k samples , we also demonstrate that our model significantly outperforms the baseline e2e nlg challenge 1 system in human evaluation .\n",
            "0.8296308854836653\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "finally , after augmenting our model with stylistic data selection , subjective evaluations reveal that it can still produce over all better results despite a significantly reduced training set .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "finally , after augmenting our model with stylistic data selection , subjective evaluations reveal that it can still produce over all better results despite a significantly reduced training set .\n",
            "0.874796544551608\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "nlg is closely related to machine translation and has similarly benefited from recent rapid development of deep learning methods .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "nlg is closely related to machine translation and has similarly benefited from recent rapid development of deep learning methods .\n",
            "0.8714512416671306\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "state - of - the - art nlg systems build thus on deep neural sequenceto - sequence models with an encoder - decoder architecture equipped with an attention mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "state - of - the - art nlg systems build thus on deep neural sequenceto - sequence models with an encoder - decoder architecture equipped with an attention mechanism .\n",
            "0.8514027682102664\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "they typically also rely on slot delexicalization , which allows the model to better generalize to unseen inputs , as exemplified by tgen .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "they typically also rely on slot delexicalization , which allows the model to better generalize to unseen inputs , as exemplified by tgen .\n",
            "0.8712217910604674\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , point out that there are frequent scenarios where delexicalization behaves inadequately ( see section 5.1 for more details ) , and show that a character - level approach to nlg may avoid the need for delexicalization , at the potential cost of making more semantic omission errors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , point out that there are frequent scenarios where delexicalization behaves inadequately ( see section 5.1 for more details ) , and show that a character - level approach to nlg may avoid the need for delexicalization , at the potential cost of making more semantic omission errors .\n",
            "0.8959862704815348\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the end - to - end approach to nlg typically requires a mechanism for aligning slots on the output utterances : this allows the model to generate our work builds upon the successful attentional encoder - decoder framework for sequenceto - sequence learning and expands it through ensembling .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the end - to - end approach to nlg typically requires a mechanism for aligning slots on the output utterances : this allows the model to generate our work builds upon the successful attentional encoder - decoder framework for sequenceto - sequence learning and expands it through ensembling .\n",
            "0.8866225071198555\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we explore the feasibility of a domainindependent slot aligner that could be applied to any dataset , regardless of its size , and beyond the reranking task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we explore the feasibility of a domainindependent slot aligner that could be applied to any dataset , regardless of its size , and beyond the reranking task .\n",
            "0.8833999521101339\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also tackle some challenges caused by delexicalization in order to improve the quality of surface realizations , while retaining the ability of the neural model to generalize .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also tackle some challenges caused by delexicalization in order to improve the quality of surface realizations , while retaining the ability of the neural model to generalize .\n",
            "0.8532645161608231\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "datasets\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "datasets\n",
            "0.29911750978549795\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we evaluated the models on three datasets from different domains .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we evaluated the models on three datasets from different domains .\n",
            "0.7868939869824645\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the primary one is the recently released e2e restaurant dataset with 48 k samples .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the primary one is the recently released e2e restaurant dataset with 48 k samples .\n",
            "0.7898802413981278\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for benchmarking we use the tv dataset and the laptop dataset with 7 k and 13k samples , respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for benchmarking we use the tv dataset and the laptop dataset with 7 k and 13k samples , respectively .\n",
            "0.8041543942398309\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "summarizes the proportions of the training , validation , and test sets for each dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "summarizes the proportions of the training , validation , and test sets for each dataset .\n",
            "0.8503266820812597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "e2e\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "e2e\n",
            "-0.2644830897176112\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "dataset\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "dataset\n",
            "0.3049420311306086\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the e2e dataset is by far the largest one available for task - oriented language generation in the restaurant domain .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the e2e dataset is by far the largest one available for task - oriented language generation in the restaurant domain .\n",
            "0.8708076353691676\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the human references were note that the number of mrs in the e2e dataset was cutoff at 10 k for the sake of visibility of the small differences between other column pairs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the human references were note that the number of mrs in the e2e dataset was cutoff at 10 k for the sake of visibility of the small differences between other column pairs .\n",
            "0.8249748799631632\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "collected using pictures as the source of information , which was shown to inspire more informative and natural utterances .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "collected using pictures as the source of information , which was shown to inspire more informative and natural utterances .\n",
            "0.8861092584534463\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with nearly 50 k samples , it offers almost 10 times more data than the san francisco restaurant dataset introduced in wen et al. ( 2015 b ) , which has frequently been used for benchmarks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with nearly 50 k samples , it offers almost 10 times more data than the san francisco restaurant dataset introduced in wen et al. ( 2015 b ) , which has frequently been used for benchmarks .\n",
            "0.8403599131843094\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the reference utterances in the e2e dataset exhibit superior lexical richness and syntactic variation , including more complex discourse phenomena .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the reference utterances in the e2e dataset exhibit superior lexical richness and syntactic variation , including more complex discourse phenomena .\n",
            "0.796764000673891\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it aims to provide higher - quality training data for end - to - end nlg systems to learn to produce more naturally sounding utterances .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it aims to provide higher - quality training data for end - to - end nlg systems to learn to produce more naturally sounding utterances .\n",
            "0.8892175845054743\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dataset was released as apart of the e2e nlg challenge .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dataset was released as apart of the e2e nlg challenge .\n",
            "0.7977054741308225\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "although the e2e dataset contains a large number of samples , each mr is associated on average with 8.65 different reference utterances , effectively offering less than 5 k unique mrs in the training set ( .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "although the e2e dataset contains a large number of samples , each mr is associated on average with 8.65 different reference utterances , effectively offering less than 5 k unique mrs in the training set ( .\n",
            "0.872083371090154\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "explicitly providing the model with multiple ground truths , it offers multiple alternative utterance structures the model can learn to apply for the same type of mr .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "explicitly providing the model with multiple ground truths , it offers multiple alternative utterance structures the model can learn to apply for the same type of mr .\n",
            "0.8849040077074314\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the delexicalization , as detailed later in section 5.1 , improves the ability of the model to share the concepts across different mrs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the delexicalization , as detailed later in section 5.1 , improves the ability of the model to share the concepts across different mrs .\n",
            "0.8751712699785527\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dataset contains only 8 different slot types , which are fairly equally distributed .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dataset contains only 8 different slot types , which are fairly equally distributed .\n",
            "0.8452185846809179\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the number of slots in each mr ranges between 3 and 8 , but the majority of mrs consist of 5 or 6 slots .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the number of slots in each mr ranges between 3 and 8 , but the majority of mrs consist of 5 or 6 slots .\n",
            "0.7609176158597473\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "even though most of the mrs contain many slots , the majority of the corresponding human utterances , however , consist of one or two sentences only ( table 3 ) , suggesting a reasonably high level of sentence complexity in the references .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "even though most of the mrs contain many slots , the majority of the corresponding human utterances , however , consist of one or two sentences only ( table 3 ) , suggesting a reasonably high level of sentence complexity in the references .\n",
            "0.8787254879266961\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "tv and laptop datasets\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "tv and laptop datasets\n",
            "0.5449809721134523\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the reference utterances in the tv and the laptop datasets were collected using amazon mechani - :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the reference utterances in the tv and the laptop datasets were collected using amazon mechani - :\n",
            "0.8307736005537465\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "average number of sentences in the reference utterance for a given number of slots in the corresponding mr , along with the proportion of mrs with specific slot counts . cal turk ( amt ) , one utterance per mr .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "average number of sentences in the reference utterance for a given number of slots in the corresponding mr , along with the proportion of mrs with specific slot counts . cal turk ( amt ) , one utterance per mr .\n",
            "0.8201012850927497\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these two datasets are similar in structure , both using the same 14 da types .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these two datasets are similar in structure , both using the same 14 da types .\n",
            "0.8443381649253866\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the laptop dataset , however , is almost twice as large and contains 25 % more slot types .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the laptop dataset , however , is almost twice as large and contains 25 % more slot types .\n",
            "0.8527125020461226\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "although both of these datasets contain more than a dozen different da types , the vast majority ( 68 % and 80 % respectively ) of the mrs describe a da of either type inform or recommend , which in most cases have very similarly structured realizations , comparable to those in the e2e dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "although both of these datasets contain more than a dozen different da types , the vast majority ( 68 % and 80 % respectively ) of the mrs describe a da of either type inform or recommend , which in most cases have very similarly structured realizations , comparable to those in the e2e dataset .\n",
            "0.8755625589102765\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "da s such as suggest , ? request , or goodbye are represented by less than a dozen samples , but are significantly easier to learn to generate an utterance from because the corresponding mrs contain three slots at the most .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "da s such as suggest , ? request , or goodbye are represented by less than a dozen samples , but are significantly easier to learn to generate an utterance from because the corresponding mrs contain three slots at the most .\n",
            "0.8812121784194125\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "input sequence to pay attention to , given the output generated so far .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "input sequence to pay attention to , given the output generated so far .\n",
            "0.8610610172067239\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this attentional encoderdecoder architecture , the probability of the output at each time step t of the decoder depends on a distinct context vector qt in the following way :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this attentional encoderdecoder architecture , the probability of the output at each time step t of the decoder depends on a distinct context vector qt in the following way :\n",
            "0.8432926427188592\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "wherein the place of function g we use the softmax function over the size of the vocabulary , and st is a hidden state of the decoder rnn at time step t , calculated as :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "wherein the place of function g we use the softmax function over the size of the vocabulary , and st is a hidden state of the decoder rnn at time step t , calculated as :\n",
            "0.8512971823157807\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the context vector qt is obtained as a weighted sum of all the hidden states h 1 , . . . , h l of the encoder :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the context vector qt is obtained as a weighted sum of all the hidden states h 1 , . . . , h l of the encoder :\n",
            "0.817085749208712\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where ?\n",
            "0.651695237194094\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "t , i corresponds to the attention score the t- th word in the target sentence assigns to the i - th item in the input mr .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "t , i corresponds to the attention score the t- th word in the target sentence assigns to the i - th item in the input mr .\n",
            "0.8245019597608504\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we compute the attention score ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we compute the attention score ?\n",
            "0.7490009937988946\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "t , i using a multi - layer perceptron ( mlp ) jointly trained with the entire system .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "t , i using a multi - layer perceptron ( mlp ) jointly trained with the entire system .\n",
            "0.7882048188470511\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the encoder 's and decoder 's hidden states at time i and t , respectively , are concatenated and used as the input to the mlp , namely :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the encoder 's and decoder 's hidden states at time i and t , respectively , are concatenated and used as the input to the mlp , namely :\n",
            "0.840964812007575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where w and ware the weight matrix and the vector of the first and the second layer of the mlp , respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where w and ware the weight matrix and the vector of the first and the second layer of the mlp , respectively .\n",
            "0.7732208668429243\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the learned weights indicate the level of influence of the individual words in the input sequence on the prediction of the word at time step t of the decoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the learned weights indicate the level of influence of the individual words in the input sequence on the prediction of the word at time step t of the decoder .\n",
            "0.834682787210556\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model thus learns a soft alignment between the source and the target sequence .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model thus learns a soft alignment between the source and the target sequence .\n",
            "0.8408937496189827\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ensembling\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ensembling\n",
            "0.0\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to enhance the quality of the predicted utterances , we create three neural models with different encoders .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to enhance the quality of the predicted utterances , we create three neural models with different encoders .\n",
            "0.8483747876190475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "two of the models use a bidirectional lstm encoder , whereas the third model has a cnn ( le - cun et al. , 1998 ) encoder .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "two of the models use a bidirectional lstm encoder , whereas the third model has a cnn ( le - cun et al. , 1998 ) encoder .\n",
            "0.7819195981366437\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we train these models individually for a different number of epochs and then combine their predictions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we train these models individually for a different number of epochs and then combine their predictions .\n",
            "0.8240748373120059\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "initially , we attempted to combine the predictions of the models by averaging the logprobability at each time step and then selecting the word with the maximum log-probability .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "initially , we attempted to combine the predictions of the models by averaging the logprobability at each time step and then selecting the word with the maximum log-probability .\n",
            "0.8641671599444175\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we noticed that the quality , as well as the bleu score of our utterances , decreased significantly .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we noticed that the quality , as well as the bleu score of our utterances , decreased significantly .\n",
            "0.837797212707981\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we believe that this is due to the fact that different models learn different sentence structures and , hence , combining predictions at the probability level results in incoherent utterances .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we believe that this is due to the fact that different models learn different sentence structures and , hence , combining predictions at the probability level results in incoherent utterances .\n",
            "0.8798135706409389\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "therefore , instead of combining the models at the log - probability level , we accumulate the top 10 predicted utterances from each model type using beam search and allow the reranker ( see section 4.4 ) to rank all candidate utterances taking the proportion of slots they successfully realized into consideration .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "therefore , instead of combining the models at the log - probability level , we accumulate the top 10 predicted utterances from each model type using beam search and allow the reranker ( see section 4.4 ) to rank all candidate utterances taking the proportion of slots they successfully realized into consideration .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "on the other hand , training on oracle clusters leads to a mismatch between training and test , which can hurt performance .\n",
            "0.851674628559633\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "search\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "search\n",
            "0.5058796930922321\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "when moving from a strictly local objective to one with global features , the test - time search problem becomes intractable .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "when moving from a strictly local objective to one with global features , the test - time search problem becomes intractable .\n",
            "0.8988668345980498\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the local objective requires o ( n 2 ) time , whereas the full clustering problem is np - hard .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the local objective requires o ( n 2 ) time , whereas the full clustering problem is np - hard .\n",
            "0.8336981402611915\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "past work with global features has used integer linear programming solvers for exact search , or beam search with ( delayed ) early update training for an approximate solution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "past work with global features has used integer linear programming solvers for exact search , or beam search with ( delayed ) early update training for an approximate solution .\n",
            "0.8674413839960043\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in contrast , we simply use greedy search at test time , which also requires o ( n 2 ) time .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in contrast , we simply use greedy search at test time , which also requires o ( n 2 ) time .\n",
            "0.8497567805165684\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the full algorithm algorithm 1 greedy search with global rnns 1 : procedure greedycluster ( x1 , . . . , xn ) 2 :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the full algorithm algorithm 1 greedy search with global rnns 1 : procedure greedycluster ( x1 , . . . , xn ) 2 :\n",
            "0.7319555937465266\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "initialize clusters x ( 1 ) . . . as empty lists , hidden states h ( 0 ) , . . . as 0 vectors in rd , z as map from mention to cluster , and cluster counter m ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "initialize clusters x ( 1 ) . . . as empty lists , hidden states h ( 0 ) , . . . as 0 vectors in rd , z as map from mention to cluster , and cluster counter m ?\n",
            "0.7720395899805077\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "0 3 :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "0 3 :\n",
            "0.2495813105401968\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for n = 2 . . .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for n = 2 . . .\n",
            "0.589194220346008\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "n do 4 : y * ? arg max f ( xn , y ) + g ( xn , y , z 1:n? 1 )\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "n do 4 : y * ? arg max f ( xn , y ) + g ( xn , y , z 1:n? 1 )\n",
            "0.445389386918127\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "5 :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "5 :\n",
            "0.2941954733382229\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "m ? zy * 6:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "m ? zy * 6:\n",
            "0.34108435453100566\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "if y * = then 7:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "if y * = then 7:\n",
            "0.5569544664323188\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "append xn to x ( m ) 10 :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "append xn to x ( m ) 10 :\n",
            "0.4268119465120325\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "zn ? m 11:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "zn ? m 11:\n",
            "0.28193150205756673\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "is shown in algorithm\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "is shown in algorithm\n",
            "0.718490646261297\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 . the greedy search algorithm is identical to a simple mention - ranking system , with the exception of line 11 , which updates the current rnn representation based on the previous decision that was made , and line 4 , which then uses this cluster representation as part of scoring .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 . the greedy search algorithm is identical to a simple mention - ranking system , with the exception of line 11 , which updates the current rnn representation based on the previous decision that was made , and line 4 , which then uses this cluster representation as part of scoring .\n",
            "0.8953292537804961\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments\n",
            "0.3699332036909505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "methods\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "methods\n",
            "0.5299871435116957\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we run experiments on the conll 2012 english shared task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we run experiments on the conll 2012 english shared task .\n",
            "0.8199789304811982\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the task uses the ontonotes corpus , consisting of 3,493 documents in various domains and formats .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the task uses the ontonotes corpus , consisting of 3,493 documents in various domains and formats .\n",
            "0.8743142162317623\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use the experimental split provided in the shared task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use the experimental split provided in the shared task .\n",
            "0.8683458011828366\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for all experiments , we use the berkeley coreference system for mention extraction and to compute features ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for all experiments , we use the berkeley coreference system for mention extraction and to compute features ?\n",
            "0.8435383789703871\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a and ? p .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a and ? p .\n",
            "0.7130473636772716\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "features\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "features\n",
            "0.4537983150515719\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use the raw basic + feature sets described by , with the following modifications :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use the raw basic + feature sets described by , with the following modifications :\n",
            "0.8550735493439684\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we remove all features from ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we remove all features from ?\n",
            "0.7769132203333192\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "p that concatenate a feature of the antecedent with a feature of the current mention , such as bi-head features .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "p that concatenate a feature of the antecedent with a feature of the current mention , such as bi-head features .\n",
            "0.8592982296662915\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we add true - cased head features , a current speaker indicator feature , and a 2 - character they underperformed .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we add true - cased head features , a current speaker indicator feature , and a 2 - character they underperformed .\n",
            "0.8507630769861946\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also experimented with training approaches and model variants that expose the model to its own predictions , but found that these yielded a negligible performance improvement. , , , and .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also experimented with training approaches and model variants that expose the model to its own predictions , but found that these yielded a negligible performance improvement. , , , and .\n",
            "0.8639436712433314\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "f 1 gains are significant ( p < 0.05 under the bootstrap resample test ) compared with for all metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "f 1 gains are significant ( p < 0.05 under the bootstrap resample test ) compared with for all metrics .\n",
            "0.7288584433137542\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "genre ( out of {bc , bn , mz , nw , pt , tc , wb} ) indicator to ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "genre ( out of {bc , bn , mz , nw , pt , tc , wb} ) indicator to ?\n",
            "0.6087207422455228\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "p and ? a .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "p and ? a .\n",
            "0.7130473647085485\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we add features indicating if a mention has a substring overlap with the current speaker (? p and ? a ) , and if an antecedent has a substring overlap with a speaker distinct from the current mention 's speaker (? p ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we add features indicating if a mention has a substring overlap with the current speaker (? p and ? a ) , and if an antecedent has a substring overlap with a speaker distinct from the current mention 's speaker (? p ) .\n",
            "0.8583040068675011\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we add a single centered , rescaled document position feature to each mention when learning h c .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we add a single centered , rescaled document position feature to each mention when learning h c .\n",
            "0.8705587034065437\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we calculate a mention x n 's rescaled document position as 2 n ? n ?1 n ?1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we calculate a mention x n 's rescaled document position as 2 n ? n ?1 n ?1 .\n",
            "0.6947499688511276\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these modifications result in there being approximately 14 k distinct features in ? a and approximately 28 k distinct features in ? p , which is far fewer features than has been typical in past work .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these modifications result in there being approximately 14 k distinct features in ? a and approximately 28 k distinct features in ? p , which is far fewer features than has been typical in past work .\n",
            "0.8425864517355293\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for training , we use document - size minibatches , which allows for efficient pre-computation of rnn states , and we minimize the loss described in section 5 with adagrad ( after clipping lstm gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for training , we use document - size minibatches , which allows for efficient pre-computation of rnn states , and we minimize the loss described in section 5 with adagrad ( after clipping lstm gradients to lie ( elementwise ) in ( ?10 , 10 ) ) .\n",
            "0.8617965439667915\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we find that the initial learning rate chosen for adagrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we find that the initial learning rate chosen for adagrad has a significant impact on results , and we choose learning rates for each layer out of { 0.1 , 0.02 , 0.01 , 0.002 , 0.001 } .\n",
            "0.825234466307147\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? r 200 , and hp ( x n , y) ? r 700 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? r 200 , and hp ( x n , y) ? r 700 .\n",
            "0.6138782222006476\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use a single - layer lstm ( without \" peep - hole \" connections ) , as implemented in the element - rnn library .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use a single - layer lstm ( without \" peep - hole \" connections ) , as implemented in the element - rnn library .\n",
            "0.8750827224507788\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for regularization , we apply dropout with a rate of 0.4 before applying the linear weights u , and we also apply dropout with a rate of 0.3 to the lstm states before forming the dot -product scores .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for regularization , we apply dropout with a rate of 0.4 before applying the linear weights u , and we also apply dropout with a rate of 0.3 to the lstm states before forming the dot -product scores .\n",
            "0.813149770542276\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "following we use the costweights ? = 0.5 , 1.2 , 1 in defining ? , and we use their pre-training scheme as well .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "following we use the costweights ? = 0.5 , 1.2 , 1 in defining ? , and we use their pre-training scheme as well .\n",
            "0.8635647005224486\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for final results , we train on both training and development portions of the conll data .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for final results , we train on both training and development portions of the conll data .\n",
            "0.8579316293898118\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "scoring uses the official conll 2012 script .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "scoring uses the official conll 2012 script .\n",
            "0.7383465191560393\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "code for our system is available at https : //github.com/swiseman/nn_coref .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "code for our system is available at https : //github.com/swiseman/nn_coref .\n",
            "0.7215731315671315\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the system makes use of a gpu for training , and trains in about two hours .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the system makes use of a gpu for training , and trains in about two hours .\n",
            "0.819150916241461\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results\n",
            "0.45462375196773575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in we present our main results on the conll english test set , and compare with other recent stateof - the - art systems .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in we present our main results on the conll english test set , and compare with other recent stateof - the - art systems .\n",
            "0.8651532183914382\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we see a statistically significant improvement of over 0.8 co nll points over the previous state of the art , and the highest f 1 scores to date on all three conll metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we see a statistically significant improvement of over 0.8 co nll points over the previous state of the art , and the highest f 1 scores to date on all three conll metrics .\n",
            "0.8221907960785743\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we now consider in more detail the impact of global features and rnns on performance .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we now consider in more detail the impact of global features and rnns on performance .\n",
            "0.849329323702836\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for these experiments , we report muc , b 3 , and ceaf e f 1 scores in as well as errors broken down by mention type and by whether the mention is anaphoric or not in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for these experiments , we report muc , b 3 , and ceaf e f 1 scores in as well as errors broken down by mention type and by whether the mention is anaphoric or not in .\n",
            "0.8640595791218273\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "are defined in section 5.1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "are defined in section 5.1 .\n",
            "0.752845456149168\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we typically think of fl and wl as representing precision errors , and fn as representing recall errors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we typically think of fl and wl as representing precision errors , and fn as representing recall errors .\n",
            "0.8317379368269836\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our experiments consider several different settings .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our experiments consider several different settings .\n",
            "0.7861099399408269\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "first , we consider an oracle setting ( \" rnn , oh \" in tables ) , in which the model receives z ( o ) 1:n?1 , the oracle partial clustering of all mentions preceding x n in the document , and is therefore not forced to rely on its own past predictions when predicting x n .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "first , we consider an oracle setting ( \" rnn , oh \" in tables ) , in which the model receives z ( o ) 1:n?1 , the oracle partial clustering of all mentions preceding x n in the document , and is therefore not forced to rely on its own past predictions when predicting x n .\n",
            "0.8752327058465995\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this provides us with an upper bound on the performance achievable with our model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this provides us with an upper bound on the performance achievable with our model .\n",
            "0.8134977781624054\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "next , we consider the performance of the model under a greedy inference strategy ( rnn , gh ) , as in algorithm\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "next , we consider the performance of the model under a greedy inference strategy ( rnn , gh ) , as in algorithm\n",
            "0.8439412168014683\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 . finally , for baselines we consider the mention - ranking system ( mr ) of using our updated feature - set , as well as a non-local baseline with oracle history ( avg , oh ) , which averages the representations h c ( x j ) for all x j ? x ( m ) , rather than feed them through an rnn ; errors are still backpropagated through the h c representations during learning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 . finally , for baselines we consider the mention - ranking system ( mr ) of using our updated feature - set , as well as a non-local baseline with oracle history ( avg , oh ) , which averages the representations h c ( x j ) for all x j ? x ( m ) , rather than feed them through an rnn ; errors are still backpropagated through the h c representations during learning .\n",
            "0.8546705192110399\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in we see that the rnn improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in we see that the rnn improves performance over all , with the most dramatic improve - ments on non-anaphoric pronouns , though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head .\n",
            "0.8656167782383172\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while wl errors also decrease for both these mention - categories under the rnn model , fn errors increase .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while wl errors also decrease for both these mention - categories under the rnn model , fn errors increase .\n",
            "0.8186458083252528\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "importantly , the rnn performance is significantly better than that of the avg baseline , which barely improves over mention - ranking , even with oracle history .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "importantly , the rnn performance is significantly better than that of the avg baseline , which barely improves over mention - ranking , even with oracle history .\n",
            "0.8383980583383951\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this suggests that modeling the sequence of mentions in a cluster is advantageous .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this suggests that modeling the sequence of mentions in a cluster is advantageous .\n",
            "0.8523813896675986\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also note that while rnn performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also note that while rnn performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting , we are still able to recover a significant portion of the possible performance improvement .\n",
            "0.8681443097911368\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "qualitative analysis\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "qualitative analysis\n",
            "0.4833952063873106\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this section we consider in detail the impact of the g term in the rnn scoring function on the two error categories that improve most under the rnn model ( as shown in ) , namely , pronominal wl errors and pronominal fl errors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this section we consider in detail the impact of the g term in the rnn scoring function on the two error categories that improve most under the rnn model ( as shown in ) , namely , pronominal wl errors and pronominal fl errors .\n",
            "0.8551306933577307\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we consider an example from the conll development set in each category on which the baseline mr model makes an error but the greedy rnn model does not .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we consider an example from the conll development set in each category on which the baseline mr model makes an error but the greedy rnn model does not .\n",
            "0.8568947513592489\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the example in involves the resolution of the ambiguous pronoun \" his , \" which is bracketed and in bold in the figure .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the example in involves the resolution of the ambiguous pronoun \" his , \" which is bracketed and in bold in the figure .\n",
            "0.8703798311851948\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "whereas the baseline mr model incorrectly predicts \" his \" to corefer with the closest gender - consistent antecedent \" justin \" thus making a wl error - the greedy rnn model : magnitudes of gradients of na score applied to bold \" it 's \" with respect to final mention in three preceding clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "whereas the baseline mr model incorrectly predicts \" his \" to corefer with the closest gender - consistent antecedent \" justin \" thus making a wl error - the greedy rnn model : magnitudes of gradients of na score applied to bold \" it 's \" with respect to final mention in three preceding clusters .\n",
            "0.877625759049037\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "see text for full description .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "see text for full description .\n",
            "0.8131228788417141\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "correctly predicts \" his \" to corefer with \" mr. kaye \" in the previous sentence .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "correctly predicts \" his \" to corefer with \" mr. kaye \" in the previous sentence .\n",
            "0.8230829656388302\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( note that \" the official \" also refers to mr. kaye ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( note that \" the official \" also refers to mr. kaye ) .\n",
            "0.805339607596933\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to get a sense of the greedy rnn model 's decision - making on this example , we color the mentions the greedy rnn model has predicted to corefer with \" mr. kaye \" in green , and the mentions it has predicted to corefer with \" justin \" in blue .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to get a sense of the greedy rnn model 's decision - making on this example , we color the mentions the greedy rnn model has predicted to corefer with \" mr. kaye \" in green , and the mentions it has predicted to corefer with \" justin \" in blue .\n",
            "0.8634258916219283\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( note that the model incorrectly predicts the initial \" i \" mentions to corefer with \" justin . \" )\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( note that the model incorrectly predicts the initial \" i \" mentions to corefer with \" justin . \" )\n",
            "0.8373393085553452\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "letting x ( 1 ) refer to the blue cluster , x ( 2 ) refer to the green cluster , and x n refer to the ambiguous mention \" his , \" we further shade each mention x j in x ( 1 ) so that it s intensity corresponds to h c ( x n ) th ( 1 ) <k , where k = j + 1 ; mentions in x ( 2 ) are shaded analogously .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "letting x ( 1 ) refer to the blue cluster , x ( 2 ) refer to the green cluster , and x n refer to the ambiguous mention \" his , \" we further shade each mention x j in x ( 1 ) so that it s intensity corresponds to h c ( x n ) th ( 1 ) <k , where k = j + 1 ; mentions in x ( 2 ) are shaded analogously .\n",
            "0.7271684717247043\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus , the shading shows how highly g scores the compatibility between \" his \" and a cluster x ( i ) as each of x ( i ) 's mentions is added .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus , the shading shows how highly g scores the compatibility between \" his \" and a cluster x ( i ) as each of x ( i ) 's mentions is added .\n",
            "0.8377586962741292\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we see that when the initial \" justin \" mentions are added to x ( 1 ) the g-score is relatively high .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we see that when the initial \" justin \" mentions are added to x ( 1 ) the g-score is relatively high .\n",
            "0.8343873306350463\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , after \" the company \" is correctly predicted to corefer with \" justin , \" the score of x ( 1 ) drops , since companies are generally not coreferent with pronouns like \" his. \" shows an example ( consisting of a telephone conversation between \" a \" and \" b \" ) in which the bracketed pronoun \" it 's \" is being used pleonastically .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , after \" the company \" is correctly predicted to corefer with \" justin , \" the score of x ( 1 ) drops , since companies are generally not coreferent with pronouns like \" his. \" shows an example ( consisting of a telephone conversation between \" a \" and \" b \" ) in which the bracketed pronoun \" it 's \" is being used pleonastically .\n",
            "0.8799374625682741\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "whereas the baseline mr model predicts \" it 's \" to corefer with a previous \" it \" - thus making a fl error - the greedy rnn model does not .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "whereas the baseline mr model predicts \" it 's \" to corefer with a previous \" it \" - thus making a fl error - the greedy rnn model does not .\n",
            "0.8671178757033812\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the na term in g with respect to that mention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the na term in g with respect to that mention .\n",
            "0.829307227708695\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this visualization resembles the \" saliency \" technique of , and it attempts to gives a sense of the contribution of a ( preceding ) cluster in the calculation of the na score .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this visualization resembles the \" saliency \" technique of , and it attempts to gives a sense of the contribution of a ( preceding ) cluster in the calculation of the na score .\n",
            "0.8677678116440252\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we see that the potential antecedent \" s - bahn \" has a large gradient , but also that the initial , obviously pleonastic use of \" it 's \" has a large gradient , which may suggest that earlier , easier predictions of pleonasm can inform subsequent predictions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we see that the potential antecedent \" s - bahn \" has a large gradient , but also that the initial , obviously pleonastic use of \" it 's \" has a large gradient , which may suggest that earlier , easier predictions of pleonasm can inform subsequent predictions .\n",
            "0.8853282066624024\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in addition to the related work noted throughout , we add supplementary references here .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in addition to the related work noted throughout , we add supplementary references here .\n",
            "0.8671813217360412\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "unstructured approaches to coreference typically divide into mention - pair models , which classify ( nearly ) every pair of mentions in a document as coreferent or not , and mention - ranking models , which select a single antecedent for each anaphoric mention ( denis and baldridge , 2008 ; .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "unstructured approaches to coreference typically divide into mention - pair models , which classify ( nearly ) every pair of mentions in a document as coreferent or not , and mention - ranking models , which select a single antecedent for each anaphoric mention ( denis and baldridge , 2008 ; .\n",
            "0.9042168176636425\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "structured approaches typically divide between those that induce a clustering of mentions , and , more recently , those that learn a latent tree of mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "structured approaches typically divide between those that induce a clustering of mentions , and , more recently , those that learn a latent tree of mentions .\n",
            "0.8737055092604106\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "there have also been structured approaches that merge the mention - ranking and mention - pair ideas in someway .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "there have also been structured approaches that merge the mention - ranking and mention - pair ideas in someway .\n",
            "0.8830202495132887\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for instance , rank clusters rather than mentions ; use the output of both mention - ranking and mention pair systems to learn a clustering .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for instance , rank clusters rather than mentions ; use the output of both mention - ranking and mention pair systems to learn a clustering .\n",
            "0.881573250930676\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the application of rnns to modeling ( the trajectory of ) the state of a cluster is apparently novel , though it bears some similarity to the recent work of , who use lstms to embed the state of a transition based parser 's stack .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the application of rnns to modeling ( the trajectory of ) the state of a cluster is apparently novel , though it bears some similarity to the recent work of , who use lstms to embed the state of a transition based parser 's stack .\n",
            "0.8895080535436682\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "conclusion\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "conclusion\n",
            "0.5120527534823697\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we have presented a simple , state of the art approach to incorporating global information in an end - to - end coreference system , which obviates the need to define global features , and moreover allows for simple ( greedy ) inference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we have presented a simple , state of the art approach to incorporating global information in an end - to - end coreference system , which obviates the need to define global features , and moreover allows for simple ( greedy ) inference .\n",
            "0.9198199367056497\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "future work will examine improving recall , and more sophisticated approaches to global training .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "future work will examine improving recall , and more sophisticated approaches to global training .\n",
            "0.8523599716070565\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "learning word representations with cross - sentence dependency for end - to - end co -reference resolution\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "learning word representations with cross - sentence dependency for end - to - end co -reference resolution\n",
            "0.8614104195159535\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( e2e - cr ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this work , we present a word embedding model that learns cross - sentence dependency for improving end - to - end co-reference resolution ( e2e - cr ) .\n",
            "0.8869333039078413\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while the traditional e2e - cr model generates word representations by running long short - term memory ( lstm ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while the traditional e2e - cr model generates word representations by running long short - term memory ( lstm ) recurrent neural networks on each sentence of an input article or conversation separately , we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency .\n",
            "0.8947943574315661\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "both sentence linking strategies enable the lstms to make use of valuable information from context sentences while calculating the representation of the current input word .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "both sentence linking strategies enable the lstms to make use of valuable information from context sentences while calculating the representation of the current input word .\n",
            "0.8932699789162025\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with this approach , the lstms learn word embeddings considering knowledge not only from the current sentence but also from the entire input document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with this approach , the lstms learn word embeddings considering knowledge not only from the current sentence but also from the entire input document .\n",
            "0.8951933553692344\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments show that learning cross - sentence dependency enriches information contained by the word representations , and improves the performance of the co-reference resolution model compared with our baseline .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments show that learning cross - sentence dependency enriches information contained by the word representations , and improves the performance of the co-reference resolution model compared with our baseline .\n",
            "0.8962970541976071\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "co-reference resolution requires models to cluster mentions that refer to the same physical entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "co-reference resolution requires models to cluster mentions that refer to the same physical entities .\n",
            "0.8835218556941385\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the models based on neural networks typically require different levels of semantic representations of input sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the models based on neural networks typically require different levels of semantic representations of input sentences .\n",
            "0.8205360365131329\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the models usually need to calculate the representations of word spans , or mentions , given pre-trained character and wordlevel embeddings before predicting antecedents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the models usually need to calculate the representations of word spans , or mentions , given pre-trained character and wordlevel embeddings before predicting antecedents .\n",
            "0.8789793521428834\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the mention - level embeddings are used to make coreference decisions , typically by scoring mention pairs and making links .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the mention - level embeddings are used to make coreference decisions , typically by scoring mention pairs and making links .\n",
            "0.8628751114946172\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "long short - term memories ( lstms ) are often used to encode the syntactic and semantic information of input sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "long short - term memories ( lstms ) are often used to encode the syntactic and semantic information of input sentences .\n",
            "0.8899420143575832\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "articles and conversations include more than one sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "articles and conversations include more than one sentences .\n",
            "0.8299671048768814\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "considering the accuracy and efficiency of co-reference resolution models , the encoder lstm usually processes input sentences separately as a batch .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "considering the accuracy and efficiency of co-reference resolution models , the encoder lstm usually processes input sentences separately as a batch .\n",
            "0.8760561347200472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dis advantage of this method is that the models do not consider the dependency among words from different sentences , which plays a significant role in word representation learning and co-reference predicting .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dis advantage of this method is that the models do not consider the dependency among words from different sentences , which plays a significant role in word representation learning and co-reference predicting .\n",
            "0.9075274067411705\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , pronouns are often linked to entities mentioned in other sentences , while their initial word vectors lack dependency information .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , pronouns are often linked to entities mentioned in other sentences , while their initial word vectors lack dependency information .\n",
            "0.8875914853356885\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as a result , a word representation model can not learn an informative embedding of a pronoun without considering cross - sentence dependency in this case .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as a result , a word representation model can not learn an informative embedding of a pronoun without considering cross - sentence dependency in this case .\n",
            "0.9006277939351877\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is also problematic if we encode the input document considering cross - sentence dependency and treat the entire document as one sentence .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is also problematic if we encode the input document considering cross - sentence dependency and treat the entire document as one sentence .\n",
            "0.901776402941104\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "an input article or conversation can be too long for a single lstm cell to memorize .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "an input article or conversation can be too long for a single lstm cell to memorize .\n",
            "0.8689457662962027\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "if the lstm updates itself for too many steps , gradients will vanish or explode , and the coreference resolution model will be very difficult to optimize .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "if the lstm updates itself for too many steps , gradients will vanish or explode , and the coreference resolution model will be very difficult to optimize .\n",
            "0.860673556377583\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .\n",
            "0.863883374656321\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to solve the problem that traditional lstm encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( e2e - cr ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to solve the problem that traditional lstm encoders , which treat the input sentences as a batch , lack an ability to capture cross - sentence dependency , and to avoid the time complexity and difficulties of training the model concatenating all input sentences , we propose a cross - sentence encoder for end - to - end co-reference ( e2e - cr ) .\n",
            "0.9196174079288667\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard lstm model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "borrowing the idea of an external memory module from , an external memory block containing syntactic and semantic information from context sentences is added to the standard lstm model .\n",
            "0.876179217434515\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with this context memory block , the proposed model is able to encode input sentences as a batch , and also calculate the representations of input words by taking both target sentences and context sentences into consideration .\n",
            "0.90897421677284\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments showed that this approach improved the performance of co-reference resolution models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments showed that this approach improved the performance of co-reference resolution models .\n",
            "0.8382042623451392\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "2 related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "2 related work\n",
            "0.6502634200590565\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "co- reference resolution\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "co- reference resolution\n",
            "0.5784344890152415\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a popular method of co-reference resolution is mention ranking .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a popular method of co-reference resolution is mention ranking .\n",
            "0.8462724722888696\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "reading each mention , the model calculates coreference scores for all antecedent mentions , and picks the mention with the highest positive score to be its co-reference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "reading each mention , the model calculates coreference scores for all antecedent mentions , and picks the mention with the highest positive score to be its co-reference .\n",
            "0.85897461423184\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "many recent works are based on this approach .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "many recent works are based on this approach .\n",
            "0.8372381045658663\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "designed a set of feature templates to improve the mention - ranking model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "designed a set of feature templates to improve the mention - ranking model .\n",
            "0.8577427553394464\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "proposed a mention - ranking model by jointly learning mention heads and co-references .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "proposed a mention - ranking model by jointly learning mention heads and co-references .\n",
            "0.8616604043094303\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "proposed a reinforcement learning framework for the mention ranking approach .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "proposed a reinforcement learning framework for the mention ranking approach .\n",
            "0.8235270285091633\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "based on similar ideas but without using parsing features , the authors of proposed the current state - of - the - art model which uses neural networks to embed mentions and calculate mention and antecedent scores .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "based on similar ideas but without using parsing features , the authors of proposed the current state - of - the - art model which uses neural networks to embed mentions and calculate mention and antecedent scores .\n",
            "0.9002158890983482\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "applied elmo embeddings to improve within - sentence dependency modeling and word representation learning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "applied elmo embeddings to improve within - sentence dependency modeling and word representation learning .\n",
            "0.8380752980698878\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "and proposed models using global entity - level features .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "and proposed models using global entity - level features .\n",
            "0.8397025327959725\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "language representation learning\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "language representation learning\n",
            "0.6873006822566622\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "distributed word embeddings has been used as the basic unit of language representation for over a decade .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "distributed word embeddings has been used as the basic unit of language representation for over a decade .\n",
            "0.8772886945568126\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "pre-trained word embeddings , for example glove and skip - gram are widely used as the input of natural language processing models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "pre-trained word embeddings , for example glove and skip - gram are widely used as the input of natural language processing models .\n",
            "0.8930063383149518\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "long short - term memory ( lstm ) networks are widely used for sentence modeling .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "long short - term memory ( lstm ) networks are widely used for sentence modeling .\n",
            "0.8636648237657423\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a single - layer lstm network was applied in the previous state - of - theart co-reference model to generate word and mention representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a single - layer lstm network was applied in the previous state - of - theart co-reference model to generate word and mention representations .\n",
            "0.902669270553857\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to capture dependency of longer distances , proposed a recurrent model that outputs hidden states by skipping input tokens .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to capture dependency of longer distances , proposed a recurrent model that outputs hidden states by skipping input tokens .\n",
            "0.8464411975904569\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "recently , memory networks have been applied in language modeling .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "recently , memory networks have been applied in language modeling .\n",
            "0.8470923580040771\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "applying an attention mechanism on memory cells , memory networks allow the model to focus on significant words or segments for classification and generation tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "applying an attention mechanism on memory cells , memory networks allow the model to focus on significant words or segments for classification and generation tasks .\n",
            "0.87075736156699\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "previous works have shown that applying memory blocks in lstms also improves longdistance dependency extraction .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "previous works have shown that applying memory blocks in lstms also improves longdistance dependency extraction .\n",
            "0.8032413633049262\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "learning cross - sentence dependency\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "learning cross - sentence dependency\n",
            "0.7216744960690649\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to improve the word representation learning model for better co-reference resolution performance , we propose two word representation models that learn cross - sentence dependency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to improve the word representation learning model for better co-reference resolution performance , we propose two word representation models that learn cross - sentence dependency .\n",
            "0.8967751316879795\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "linear sentence linking\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "linear sentence linking\n",
            "0.5905044876239447\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "instead of treating the entire input document as separate sentences and encode the sentences as a batch with an lstm , the most direct way to consider cross - sentence dependency is to initialize lstm states with the encodings of adjacent sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "instead of treating the entire input document as separate sentences and encode the sentences as a batch with an lstm , the most direct way to consider cross - sentence dependency is to initialize lstm states with the encodings of adjacent sentences .\n",
            "0.9000158698858025\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we name this method linear sentence linking ( lsl ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we name this method linear sentence linking ( lsl ) .\n",
            "0.7878086609538121\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in lsl , we encode input sentences with a 2 layer bidirectional lstm .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in lsl , we encode input sentences with a 2 layer bidirectional lstm .\n",
            "0.7655488721619272\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "give input sentences [ s 1 , s 2 . . . s n ] , the outputs of the first layer are\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "give input sentences [ s 1 , s 2 . . . s n ] , the outputs of the first layer are\n",
            "0.8050416149142754\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the second lstm layer , the initial state of the forward lstm of s i is initialized as\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the second lstm layer , the initial state of the forward lstm of s i is initialized as\n",
            "0.8206755053045681\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while the backward state is initialized as\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while the backward state is initialized as\n",
            "0.7788684296932018\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where c i 0 stands for the initial cell of the ith layer , and x stands for the final output of the lstms in first layer .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where c i 0 stands for the initial cell of the ith layer , and x stands for the final output of the lstms in first layer .\n",
            "0.7992646741214066\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we then concatenate the outputs of the forward and backward lstms in the second layer as the word representations for coreference prediction .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we then concatenate the outputs of the forward and backward lstms in the second layer as the word representations for coreference prediction .\n",
            "0.8462062996232941\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "attentional sentence linking\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "attentional sentence linking\n",
            "0.44090085489535347\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is difficult for lstms to embed enough information about along sentence into a lowdimensional distributed vector .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is difficult for lstms to embed enough information about along sentence into a lowdimensional distributed vector .\n",
            "0.8901314120795943\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to collect richer knowledge from neighbor sentences , we propose along short - term recurrent memory module and an attention mechanism to improve sentence linking .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to collect richer knowledge from neighbor sentences , we propose along short - term recurrent memory module and an attention mechanism to improve sentence linking .\n",
            "0.8985776607854744\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to describe the architecture of the proposed model , we focus on adjacent input sentences s i ?1 and s i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to describe the architecture of the proposed model , we focus on adjacent input sentences s i ?1 and s i .\n",
            "0.8593680111032352\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we present the input embeddings of the j - th word in the i - th sentence with x i , j .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we present the input embeddings of the j - th word in the i - th sentence with x i , j .\n",
            "0.7754488111993616\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "long short - term memory rnns\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "long short - term memory rnns\n",
            "0.7555960010128662\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to solve the traditional recurrent neural networks , proposed the lstm architecture .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to solve the traditional recurrent neural networks , proposed the lstm architecture .\n",
            "0.8057424140415886\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the detail of recurrent state updating in lstms ht = f lstm ( x t , h t?1 , c t?1 ) is shown in following equations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the detail of recurrent state updating in lstms ht = f lstm ( x t , h t?1 , c t?1 ) is shown in following equations .\n",
            "0.7704246056643607\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where x t is the input embedding and ht is the output representation of the t- th word .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where x t is the input embedding and ht is the output representation of the t- th word .\n",
            "0.8106382752693521\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "lstms with cross - sentence attention\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "lstms with cross - sentence attention\n",
            "0.7798570504342568\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we design an lstm module with cross - sentence attention for capturing cross - sentence dependency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we design an lstm module with cross - sentence attention for capturing cross - sentence dependency .\n",
            "0.8572405275809937\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we name this method attentional sentence linking ( asl ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we name this method attentional sentence linking ( asl ) .\n",
            "0.7810325110845253\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "considering input word x i ,t in the ith sentence and all words from the previous sentence x i?1 = [ x i ?1 , 1 , x i?1 , 2 , . . . , x i?1 , m ] , we regard the matrix x i?1 as an external memory module and calculate an attention on its cells , where each cell contains a word embedding .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "considering input word x i ,t in the ith sentence and all words from the previous sentence x i?1 = [ x i ?1 , 1 , x i?1 , 2 , . . . , x i?1 , m ] , we regard the matrix x i?1 as an external memory module and calculate an attention on its cells , where each cell contains a word embedding .\n",
            "0.8344177828581044\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with the attention distribution ? , we can get a vector summarizing related information from\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with the attention distribution ? , we can get a vector summarizing related information from\n",
            "0.8719741594596634\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model decides if it needs to pay more attention on the current input or cross - sentence information with a context gate .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model decides if it needs to pay more attention on the current input or cross - sentence information with a context gate .\n",
            "0.8835222958991603\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "? ( ) stands for the sigmoid function .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "? ( ) stands for the sigmoid function .\n",
            "0.7124703343299685\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the word representation of the target word is calculated as\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the word representation of the target word is calculated as\n",
            "0.8396853787398505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where f lstm stands for standard lstm update described in section 3.2.1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where f lstm stands for standard lstm update described in section 3.2.1 .\n",
            "0.7785969027775346\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "co- reference prediction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "co- reference prediction\n",
            "0.5434450378336285\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this work , we apply the mention - ranking endto - end co-reference resolution ( e2e - cr ) model proposed by for co-reference prediction .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this work , we apply the mention - ranking endto - end co-reference resolution ( e2e - cr ) model proposed by for co-reference prediction .\n",
            "0.8400784511469946\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the word representations applied in e2e - cr model is formed by concatenating pre-trained word embeddings and the outputs of lstms .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the word representations applied in e2e - cr model is formed by concatenating pre-trained word embeddings and the outputs of lstms .\n",
            "0.8420310397118823\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in our work , we represent words by concatenating pre-trained word embeddings and the outputs of lsl - and asl - lstms .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in our work , we represent words by concatenating pre-trained word embeddings and the outputs of lsl - and asl - lstms .\n",
            "0.8517738745702624\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments\n",
            "0.3699332036909505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we train and evaluate our model on the english corpus of the conll - 2012 shared task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we train and evaluate our model on the english corpus of the conll - 2012 shared task .\n",
            "0.8508028605454324\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we implement our model based on the published implementation of the baseline e2e - cr model 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we implement our model based on the published implementation of the baseline e2e - cr model 1 .\n",
            "0.7900443338773083\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our implementation is also available online for reproducing the results reported in this paper 2 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our implementation is also available online for reproducing the results reported in this paper 2 .\n",
            "0.8409943579442302\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this section , we first describe our hyperparameter setup , and then show the experimental results of previous work and our proposed models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this section , we first describe our hyperparameter setup , and then show the experimental results of previous work and our proposed models .\n",
            "0.8593694623571575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "model and hyperparameter setup\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "model and hyperparameter setup\n",
            "0.6292761596406428\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in practice , the lstm modules applied in our model have 200 output units .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in practice , the lstm modules applied in our model have 200 output units .\n",
            "0.8247235643391385\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in asl , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in asl , we calculate cross - sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units .\n",
            "0.8088856749393407\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the initial learning rate is set as 0.001 and decays 0.001 % every 100 steps .\n",
            "0.7406550488986229\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model is optimized with the adam algorithm ( kingma and ba , 2014 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model is optimized with the adam algorithm ( kingma and ba , 2014 ) .\n",
            "0.786729649906969\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we randomly select up to 40 continuous sentences for training if the input is too long .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we randomly select up to 40 continuous sentences for training if the input is too long .\n",
            "0.8665148501329314\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in co-reference prediction , we select 250 candidate antecedents as our baseline model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in co-reference prediction , we select 250 candidate antecedents as our baseline model .\n",
            "0.8484225405228454\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiment results and discussion\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiment results and discussion\n",
            "0.7075497078469535\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we evaluate our model on the test set of the conll - 2012 shared task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we evaluate our model on the test set of the conll - 2012 shared task .\n",
            "0.8398478150919013\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the performance of previous work and our model are shown in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the performance of previous work and our model are shown in .\n",
            "0.8231022688694013\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we mainly focus on the average f 1 score of muc , b 3 , and ceaf metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we mainly focus on the average f 1 score of muc , b 3 , and ceaf metrics .\n",
            "0.7854232188655484\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "comparing with the baseline model that achieved 67.2 % f1 score , the asl model improved the performance by 0.6 % and achieved 67.8 % average f1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "comparing with the baseline model that achieved 67.2 % f1 score , the asl model improved the performance by 0.6 % and achieved 67.8 % average f1 .\n",
            "0.6591972673356087\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments : experimental results of previous models and cross - sentence dependency learning models on the conll - 2012 shared task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments : experimental results of previous models and cross - sentence dependency learning models on the conll - 2012 shared task .\n",
            "0.8548173857039765\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- i remember receiving an sms like this one last year before it snowed since snowfall would affect road conditions in beijing to a large extent .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- i remember receiving an sms like this one last year before it snowed since snowfall would affect road conditions in beijing to a large extent .\n",
            "0.8030114052767975\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- uh- huh .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- uh- huh .\n",
            "0.483519433580998\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , it did not give people such a special feeling as it did this time .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , it did not give people such a special feeling as it did this time .\n",
            "0.8274710165120552\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- reporters are tired of the usual stand ups .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- reporters are tired of the usual stand ups .\n",
            "0.7664542470454088\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- they want to be riding on a train or walking in the rain or something to get attention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- they want to be riding on a train or walking in the rain or something to get attention .\n",
            "0.8124091740419167\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- planned terrorist bombing that ripped a 20 x 40 - foot hole in the navy destroyer uss cole in the yemeni port of aden .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- planned terrorist bombing that ripped a 20 x 40 - foot hole in the navy destroyer uss cole in the yemeni port of aden .\n",
            "0.65506821215513\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- the ship was therefor refueling .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- the ship was therefor refueling .\n",
            "0.6716972649628586\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- yemeni authorities claimed they have detained over 70 people for questioning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- yemeni authorities claimed they have detained over 70 people for questioning .\n",
            "0.7162181767848931\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "- these include some afghan - arab volunteers .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "- these include some afghan - arab volunteers .\n",
            "0.6921191348396272\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "show that the models that consider cross - sentence dependency significantly outperform the baseline model , which encodes each sentence from the input document separately .\n",
            "0.8789168892092059\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments also indicated that the asl model has better performance than the lsl model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments also indicated that the asl model has better performance than the lsl model , since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence - level embeddings .\n",
            "0.8925290274339407\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this gives the model a better ability to model cross - sentence dependency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this gives the model a better ability to model cross - sentence dependency .\n",
            "0.8509523966519678\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "examples for comparing the performance of the asl model and the baseline are shown in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "examples for comparing the performance of the asl model and the baseline are shown in .\n",
            "0.812766668392865\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each example contains two continuous sentences with co-references distritubed in different sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each example contains two continuous sentences with co-references distritubed in different sentences .\n",
            "0.8489762459279822\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "underlined spans in bold are target mentions and annotated co-references .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "underlined spans in bold are target mentions and annotated co-references .\n",
            "0.8269655625474134\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "spans in green are asl predictions , and spans in red are baseline predictions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "spans in green are asl predictions , and spans in red are baseline predictions .\n",
            "0.7469198102577312\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a prediction on \" - \" means that no mention is predicted as a co-reference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a prediction on \" - \" means that no mention is predicted as a co-reference .\n",
            "0.8728488255921806\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "shows that the baseline model , which does not consider cross - sentence dependency , has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "shows that the baseline model , which does not consider cross - sentence dependency , has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence .\n",
            "0.904241307351472\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the pretrained embeddings of pronouns are not informative enough .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the pretrained embeddings of pronouns are not informative enough .\n",
            "0.7675561012400066\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the first example , \" it \" is not semantically similar with \" sms \" in glo ve without any context , and in this case , \" it \" and \" sms \" are in different sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the first example , \" it \" is not semantically similar with \" sms \" in glo ve without any context , and in this case , \" it \" and \" sms \" are in different sentences .\n",
            "0.8901781261854259\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as a result , if reading this two sentences separately , it is hard for the encoder to represent \" it \" with the semantics of \" sms \" .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as a result , if reading this two sentences separately , it is hard for the encoder to represent \" it \" with the semantics of \" sms \" .\n",
            "0.9108493891013695\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this difficulty makes the co-reference resolution model either prediction a wrong antecedent mention , or can not find any co-reference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this difficulty makes the co-reference resolution model either prediction a wrong antecedent mention , or can not find any co-reference .\n",
            "0.8709776319186026\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , with asl , the model learns the semantics of pronouns with an attention to words in other sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , with asl , the model learns the semantics of pronouns with an attention to words in other sentences .\n",
            "0.8738939149363392\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with the proposed context gate , asl takes knowledge from context sentences if local inputs are not informative enough .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with the proposed context gate , asl takes knowledge from context sentences if local inputs are not informative enough .\n",
            "0.8950007926481215\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "based on word represents enhanced with cross - sentence dependency , the co-reference scoring model can make better predictions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "based on word represents enhanced with cross - sentence dependency , the co-reference scoring model can make better predictions .\n",
            "0.8938377245629386\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "conclusion and future work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "conclusion and future work\n",
            "0.777043617808491\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we proposed linear and attentional sentence linking models for learning word representations that captures cross - sentence dependency .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we proposed linear and attentional sentence linking models for learning word representations that captures cross - sentence dependency .\n",
            "0.8565435945126397\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments showed that the embeddings learned by proposed models successfully improved the performance of the state - of - the - art co-reference resolution model , indicating that cross - sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments showed that the embeddings learned by proposed models successfully improved the performance of the state - of - the - art co-reference resolution model , indicating that cross - sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences .\n",
            "0.9138240490156181\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it worth exploring if our model can improve the performance of other natural language processing applications whose inputs contain multiple sentences , for example , reading comprehension , dialog generation , and sentiment analysis .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it worth exploring if our model can improve the performance of other natural language processing applications whose inputs contain multiple sentences , for example , reading comprehension , dialog generation , and sentiment analysis .\n",
            "0.9071732734011719\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "coreference resolution with entity equalization\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "coreference resolution with entity equalization\n",
            "0.5989951334084789\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a key challenge in coreference resolution is to capture properties of entity clusters , and use those in the resolution process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a key challenge in coreference resolution is to capture properties of entity clusters , and use those in the resolution process .\n",
            "0.8785277274513433\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we provide a simple and effective approach for achieving this , via an \" entity equalization \" mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we provide a simple and effective approach for achieving this , via an \" entity equalization \" mechanism .\n",
            "0.8890385618512142\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster .\n",
            "0.8191225758474745\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we show how this can be done in a fully differentiable end - to - end manner , thus enabling high - order inferences in the resolution process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we show how this can be done in a fully differentiable end - to - end manner , thus enabling high - order inferences in the resolution process .\n",
            "0.8914119638172008\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our approach , which also employs bert embeddings , results in new stateof - the - art results on the conll - 2012 coreference resolution task , improving average f1 by 3.6 % .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our approach , which also employs bert embeddings , results in new stateof - the - art results on the conll - 2012 coreference resolution task , improving average f1 by 3.6 % .\n",
            "0.8496912888596015\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1\n",
            "0.22804272532698444\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "coreference resolution is the task of grouping mentions into entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "coreference resolution is the task of grouping mentions into entities .\n",
            "0.8668153081818762\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a key challenge in this task is that information about an entity is spread across multiple mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a key challenge in this task is that information about an entity is spread across multiple mentions .\n",
            "0.8936818317870221\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus , deciding whether to assign a given mention to a candidate entity could require entity - level information that needs to be aggregated from all mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus , deciding whether to assign a given mention to a candidate entity could require entity - level information that needs to be aggregated from all mentions .\n",
            "0.8936704568322947\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "most coreference resolution systems rely on pairwise scoring of entity mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "most coreference resolution systems rely on pairwise scoring of entity mentions .\n",
            "0.7894124192159704\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as such they are prone to missing global entity information .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as such they are prone to missing global entity information .\n",
            "0.8776175663826177\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the problem of entity - level representation ( also referred to as high - order coreference models ) has attracted considerable interest recently , with methods ranging from imitation learning to iterative refinement .\n",
            "0.9039880555983993\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "specifically , tackled this problem by iteratively averaging the antecedents of each mention to create mention representations thatare more \" global \" ( i.e. , reflect information about the entity to which the mention refers ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "specifically , tackled this problem by iteratively averaging the antecedents of each mention to create mention representations thatare more \" global \" ( i.e. , reflect information about the entity to which the mention refers ) .\n",
            "0.908641665332224\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we propose an approach that provides an entity - level representation in a simple and intuitive manner , and also facilitates end - to - end optimization .\n",
            "0.9176725351915783\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our \" entity equalization \" approach posits that each entity should be represented via the sum of its corresponding mention representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our \" entity equalization \" approach posits that each entity should be represented via the sum of its corresponding mention representations .\n",
            "0.8651142458281715\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is not immediately obvious how to perform this equalization , which relies on the entity - to- mention mapping , but we provide a natural smoothed representation of this mapping , and demonstrate how to use it for equalization .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is not immediately obvious how to perform this equalization , which relies on the entity - to- mention mapping , but we provide a natural smoothed representation of this mapping , and demonstrate how to use it for equalization .\n",
            "0.9003620631258461\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "now that each mention contains information about all its corresponding entities , we can use a standard pairwise scoring model , and this model will be able to use global entity - level information .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "now that each mention contains information about all its corresponding entities , we can use a standard pairwise scoring model , and this model will be able to use global entity - level information .\n",
            "0.8970709079160614\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "similar to recent coreference models , our approach uses contextual embeddings as input mention representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "similar to recent coreference models , our approach uses contextual embeddings as input mention representations .\n",
            "0.8492591110795147\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while previous approaches employed the elmo model , we propose to use bert embeddings , motivated by the impressive empirical performance of bert on other tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while previous approaches employed the elmo model , we propose to use bert embeddings , motivated by the impressive empirical performance of bert on other tasks .\n",
            "0.8486332339856989\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is challenging to apply bert to the coreference resolution setting because bert is limited to a fixed sequence length which is shorter than most coreference resolution documents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is challenging to apply bert to the coreference resolution setting because bert is limited to a fixed sequence length which is shorter than most coreference resolution documents .\n",
            "0.8650165820145646\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we show that this can be done by using bert in a fully convolutional manner .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we show that this can be done by using bert in a fully convolutional manner .\n",
            "0.824167922956864\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our work is the first to use bert for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our work is the first to use bert for the task of coreference resolution , and we demonstrate that this results in significant improvement over current state - of - the - art .\n",
            "0.8801478298294829\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in summary , our contributions are : a.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in summary , our contributions are : a.\n",
            "0.800038422161126\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a simple and intuitive approach for entity - level representation via the notion of entity - equalization .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a simple and intuitive approach for entity - level representation via the notion of entity - equalization .\n",
            "0.8792740470088332\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "b.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "b.\n",
            "0.17223815173916193\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the first use of bert embeddings in coreferenceresolution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the first use of bert embeddings in coreferenceresolution .\n",
            "0.7194752320687435\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "c. new state - of - the - art performance on the conll - 2012 coreference resolution task , improving over previous f1 performance by 3.6 % .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "c. new state - of - the - art performance on the conll - 2012 coreference resolution task , improving over previous f1 performance by 3.6 % .\n",
            "0.8094196025887079\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "background\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "background\n",
            "0.5260580083344957\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "following , we cast the coreference resolution task as finding a set of antecedent assignments y i for each span i in the document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "following , we cast the coreference resolution task as finding a set of antecedent assignments y i for each span i in the document .\n",
            "0.8720603550058256\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the set of possible values for each y i is y ( i ) = { , 1 , . . . , i ? 1 } , a dummy antecedent and all preceding spans .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the set of possible values for each y i is y ( i ) = { , 1 , . . . , i ? 1 } , a dummy antecedent and all preceding spans .\n",
            "0.7869456759729728\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "non-dummy antecedents represent coreference links between i and y i , whereas indicates that the span is either not a mention , or is a first mention in a newly formed cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "non-dummy antecedents represent coreference links between i and y i , whereas indicates that the span is either not a mention , or is a first mention in a newly formed cluster .\n",
            "0.8671547885068662\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "whenever a new cluster is formed it receives a new index , and every mention with y i = receives the index of its antecedents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "whenever a new cluster is formed it receives a new index , and every mention with y i = receives the index of its antecedents .\n",
            "0.8440127305943471\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus the process results in clusters of coreferent entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus the process results in clusters of coreferent entities .\n",
            "0.8322732332217175\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "baseline model\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "baseline model\n",
            "0.43820396485732244\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we briefly describe the baseline model ) which we will later augment with entity - equalization and bert features .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we briefly describe the baseline model ) which we will later augment with entity - equalization and bert features .\n",
            "0.8708077479142516\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let s ( i , j ) denote a pairwise score between two spans i and j.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let s ( i , j ) denote a pairwise score between two spans i and j.\n",
            "0.7207216597043677\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "next , for each span i define the distribution p ( y i ) over antecedents :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "next , for each span i define the distribution p ( y i ) over antecedents :\n",
            "0.7763637022148131\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the score is a function of the span representations defined as follows .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the score is a function of the span representations defined as follows .\n",
            "0.8597285508985626\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each span i let g i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each span i let g i ?\n",
            "0.6613638449133242\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd denote its corresponding representation vector ( see for more details about the model architecture ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd denote its corresponding representation vector ( see for more details about the model architecture ) .\n",
            "0.8223022583079387\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "computes the antecedent score s ( i , j ) = f s ( g i , g j ) as a pairwise function of the span representations , i.e. not directly incorporating any information about the entities to which they might belong .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "computes the antecedent score s ( i , j ) = f s ( g i , g j ) as a pairwise function of the span representations , i.e. not directly incorporating any information about the entities to which they might belong .\n",
            "0.8321184334655622\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "improved upon this model by \" refining \" the span representations as follows .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "improved upon this model by \" refining \" the span representations as follows .\n",
            "0.8798911903638964\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the expected antecedent representation a i of each span i is computed by using the current antecedent distribution p ( y i ) as an attention mechanism :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the expected antecedent representation a i of each span i is computed by using the current antecedent distribution p ( y i ) as an attention mechanism :\n",
            "0.8319636618741609\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the current span representation g i is then updated via interpolation with its expected antecedent representation a i :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the current span representation g i is then updated via interpolation with its expected antecedent representation a i :\n",
            "0.8266273885592254\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where f i = ff ( g i , a i ) is a learned gate vector .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where f i = ff ( g i , a i ) is a learned gate vector .\n",
            "0.6880843365918126\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus , the refined representation g i is an elementwise weighted average of the current span representation and its direct antecedents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus , the refined representation g i is an elementwise weighted average of the current span representation and its direct antecedents .\n",
            "0.8385929378797592\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "using this representation the refined antecedent distribution can be calculated as follows :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "using this representation the refined antecedent distribution can be calculated as follows :\n",
            "0.8399562861887371\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "entity equalization\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "entity equalization\n",
            "0.37426113301556385\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the idea behind the refinement procedure in was to create features thatare closer to cluster - level representations and hence more \" global \" .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the idea behind the refinement procedure in was to create features thatare closer to cluster - level representations and hence more \" global \" .\n",
            "0.9112894171895366\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this was partially achieved by considering not only the current span but also its antecedents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this was partially achieved by considering not only the current span but also its antecedents .\n",
            "0.8240478153193731\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we would like take this idea one step further and create refined span representations that contain information about the entire cluster to which it belongs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we would like take this idea one step further and create refined span representations that contain information about the entire cluster to which it belongs .\n",
            "0.8797438518650322\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "one way to achieve this is by simply representing each mention via the sum of the mentions currently in its coreference cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "one way to achieve this is by simply representing each mention via the sum of the mentions currently in its coreference cluster .\n",
            "0.8567102371786979\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "formally , let c ( i ) be a coreference cluster ( as defined by the antecedent distribution p ( y i ) ) such that i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "formally , let c ( i ) be a coreference cluster ( as defined by the antecedent distribution p ( y i ) ) such that i ?\n",
            "0.7760820691025192\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "c ( i ) , and replace equation 1 with :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "c ( i ) , and replace equation 1 with :\n",
            "0.6790237355371178\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as a result each span will now contain information about all of its current coreference cluster , effectively equalizing the representations of different spans belonging to the same cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as a result each span will now contain information about all of its current coreference cluster , effectively equalizing the representations of different spans belonging to the same cluster .\n",
            "0.87325899624987\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , note that it is not clear how to train such a procedure end - to - end because the clustering process is not differentiable .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , note that it is not clear how to train such a procedure end - to - end because the clustering process is not differentiable .\n",
            "0.8894709721428603\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to overcome this problem , we use a differentiable relaxation of the clustering process and use the resulting soft clustering matrix to create a fully differentiable cluster representation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to overcome this problem , we use a differentiable relaxation of the clustering process and use the resulting soft clustering matrix to create a fully differentiable cluster representation .\n",
            "0.8352887231851496\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we call this refinement procedure entity equalization and provide a detailed description in the next section .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we call this refinement procedure entity equalization and provide a detailed description in the next section .\n",
            "0.8663017525974419\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to illustrate the difference between entity equalization and antecedent averaging , consider the following example : \" [ john ] went to the park and [ he ] got tired .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to illustrate the difference between entity equalization and antecedent averaging , consider the following example : \" [ john ] went to the park and [ he ] got tired .\n",
            "0.8560374105562709\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "[ john ] decided to go back home . \"\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "[ john ] decided to go back home . \"\n",
            "0.727271794222168\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "now assume that the model outputs the following antecedent distribution p ( y i ) :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "now assume that the model outputs the following antecedent distribution p ( y i ) :\n",
            "0.773856897234674\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "john 1 he john 2 john 1 1 0 0 he 1 0 0 john 2 1 0 0 there is only one coreference cluster induced by this antecedent matrix , c = { john 1 , he , john 2 }. a cluster representation for john 2 would be the sum of the representations of all three mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "john 1 he john 2 john 1 1 0 0 he 1 0 0 john 2 1 0 0 there is only one coreference cluster induced by this antecedent matrix , c = { john 1 , he , john 2 }. a cluster representation for john 2 would be the sum of the representations of all three mentions .\n",
            "0.697213647458118\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , using antecedent averaging , the representation of john 2 will be a weighted average of the representations of john 2 and john 1 , because only john 1 is an antecedent of john 2 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , using antecedent averaging , the representation of john 2 will be a weighted average of the representations of john 2 and john 1 , because only john 1 is an antecedent of john 2 .\n",
            "0.7916632592405873\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "implementing equalization\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "implementing equalization\n",
            "0.34847492593332074\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to achieve differentiable cluster representations , we need a differentiable soft - clustering process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to achieve differentiable cluster representations , we need a differentiable soft - clustering process .\n",
            "0.8009106066737391\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduced such a relaxation given an antecedent distribution , based on the following observation : in a document containing m mentions there are m potential entities e 1 , ... , e m where e i has mention i as the first mention .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduced such a relaxation given an antecedent distribution , based on the following observation : in a document containing m mentions there are m potential entities e 1 , ... , e m where e i has mention i as the first mention .\n",
            "0.8611281443620508\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let q (i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let q (i ?\n",
            "0.5648904333274626\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "e j ) be the probability that mention i corresponds to entity e j ( that is , to the entity that has j as its first mention ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "e j ) be the probability that mention i corresponds to entity e j ( that is , to the entity that has j as its first mention ) .\n",
            "0.8285360600399598\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "showed that this probability can be computed recursively based on the antecedent distribution p ( y i ) as follows :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "showed that this probability can be computed recursively based on the antecedent distribution p ( y i ) as follows :\n",
            "0.7979436305259104\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "note that this is a fully differentiable procedure that calculates the clustering distribution for each entity i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "note that this is a fully differentiable procedure that calculates the clustering distribution for each entity i .\n",
            "0.8294391667709691\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the distribution q ( i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the distribution q ( i ?\n",
            "0.6554573928015401\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "e j ) above leads to a simple differentiable implementation of the equalization operation in ( 3 ) , as described next .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "e j ) above leads to a simple differentiable implementation of the equalization operation in ( 3 ) , as described next .\n",
            "0.8436421884829329\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to use entity representations for equalizing mention representations , we need a representation for each entity e i at each time step t , so we wo n't represent a mention using mentions not yet encountered .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to use entity representations for equalizing mention representations , we need a representation for each entity e i at each time step t , so we wo n't represent a mention using mentions not yet encountered .\n",
            "0.8821852845677579\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we denote it by :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we denote it by :\n",
            "0.7405212032584854\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "finally , an entity representation for each mention i is calculated using the entity distribution of mention i and the global entity representations :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "finally , an entity representation for each mention i is calculated using the entity distribution of mention i and the global entity representations :\n",
            "0.8604722544243294\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it can be seen that the above a i will indeed lead to ( 3 ) when the distributions p ( y ) are deterministic .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it can be seen that the above a i will indeed lead to ( 3 ) when the distributions p ( y ) are deterministic .\n",
            "0.8274333761401832\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "using bert\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "using bert\n",
            "0.31951120520085247\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "embeddings\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "embeddings\n",
            "-0.11544860495069247\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our coreference model relies on input representations for each input token .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our coreference model relies on input representations for each input token .\n",
            "0.794325970513918\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "used the elmo context - dependent embeddings for this purpose .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "used the elmo context - dependent embeddings for this purpose .\n",
            "0.821992899919901\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we propose to use the more recent bert embeddings instead , which have achieved state of the art performance on many natural language processing tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we propose to use the more recent bert embeddings instead , which have achieved state of the art performance on many natural language processing tasks .\n",
            "0.8908084712327534\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert is a bidirectional contextual language model based on the transformer architecture .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert is a bidirectional contextual language model based on the transformer architecture .\n",
            "0.7689694016104394\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "using bert for coreference resolution is not trivial : bert can only run on sequences of fixed length which is determined in the pretraining process .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "using bert for coreference resolution is not trivial : bert can only run on sequences of fixed length which is determined in the pretraining process .\n",
            "0.841853610455774\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the pre-trained model published by , this limitation is 512 tokens , which is shorter than many of the documents in the conll - 2012 coreference resolution task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the pre-trained model published by , this limitation is 512 tokens , which is shorter than many of the documents in the conll - 2012 coreference resolution task .\n",
            "0.8902727948541451\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "even without considering the pre-training limitation , because the attention mechanism grows as the square of the sequence length , and because of the large number of parameters of the bert model , running it on very large sequences is not feasible on most machines due to memory constraints .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "even without considering the pre-training limitation , because the attention mechanism grows as the square of the sequence length , and because of the large number of parameters of the bert model , running it on very large sequences is not feasible on most machines due to memory constraints .\n",
            "0.8761694997957168\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in order to obtain bert embeddings for sequences of unlimited length , we propose to use bert in a convolutional mode as follows .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in order to obtain bert embeddings for sequences of unlimited length , we propose to use bert in a convolutional mode as follows .\n",
            "0.8356280408928053\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let d be a fixed window length .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let d be a fixed window length .\n",
            "0.7583359282071066\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we obtain a representation for token i by applying bert to the sequence of tokens from d to the left of i to d to the right of i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we obtain a representation for token i by applying bert to the sequence of tokens from d to the left of i to d to the right of i .\n",
            "0.8051113313390488\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we then take the four last layers of the bert model for token i and apply a learnable weighted averaging to those , similar to the process used in elmo .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we then take the four last layers of the bert model for token i and apply a learnable weighted averaging to those , similar to the process used in elmo .\n",
            "0.8447662914891105\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the output of the network is taken as the representation of token i , and replaced the elmo representation in the model of section 3.1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the output of the network is taken as the representation of token i , and replaced the elmo representation in the model of section 3.1 .\n",
            "0.841473343191321\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use d = 64 , since using the maximum size of d = 256 is computationally intensive , and good results are already obtained with 64 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use d = 64 , since using the maximum size of d = 256 is computationally intensive , and good results are already obtained with 64 .\n",
            "0.8081397246636152\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "2\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "2\n",
            "0.20276126991161597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "several works have addressed the issue of entitylevel representation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "several works have addressed the issue of entitylevel representation .\n",
            "0.83081454214759\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in an rnn is used to model each entity .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in an rnn is used to model each entity .\n",
            "0.8093930753247607\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while this allows complex entity representations , the assignment of a mention to an rnn is a hard decision , and as such can not be optimized in an end - to - end manner .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while this allows complex entity representations , the assignment of a mention to an rnn is a hard decision , and as such can not be optimized in an end - to - end manner .\n",
            "0.9147515015265067\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "use whole - entity representations as obtained from agglomerative clustering .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "use whole - entity representations as obtained from agglomerative clustering .\n",
            "0.8600731710216568\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "but again the clustering operation in non-differentiable , requiring the use of imitation learning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "but again the clustering operation in non-differentiable , requiring the use of imitation learning .\n",
            "0.8624144337382338\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in , entity refinement is more restricted , as it is only obtained from the attention vector at each step .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in , entity refinement is more restricted , as it is only obtained from the attention vector at each step .\n",
            "0.8822546164768459\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "thus , we believe that our model is the first to use entity - level representations that correspond directly to the inferred clusters , and are end - to - end differentiable .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "thus , we believe that our model is the first to use entity - level representations that correspond directly to the inferred clusters , and are end - to - end differentiable .\n",
            "0.8999045655857892\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "mention - entity mappings have been used in the context of optimizing coreference performance measures .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "mention - entity mappings have been used in the context of optimizing coreference performance measures .\n",
            "0.8839404917462044\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we show that these mappings can also be used for the resolution model itself .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we show that these mappings can also be used for the resolution model itself .\n",
            "0.8509584845909938\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we note that we did not try to optimize for coreference measures as in , and this is likely to further improve results .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we note that we did not try to optimize for coreference measures as in , and this is likely to further improve results .\n",
            "0.861966101400627\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments\n",
            "0.3699332036909505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "data for all our experiments is taken from the english portion of the conll - 2012 coreference resolution tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "data for all our experiments is taken from the english portion of the conll - 2012 coreference resolution tasks .\n",
            "0.8736293216525285\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our experimental setup is very similar to , and our code is built on theirs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our experimental setup is very similar to , and our code is built on theirs .\n",
            "0.8528459871952269\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we did not change the optimizer or any of the training hyperparameters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we did not change the optimizer or any of the training hyperparameters .\n",
            "0.7897953836632633\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the following changes were made to the model :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the following changes were made to the model :\n",
            "0.8101643508537966\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we used bert word embeddings instead of elmo as input to the lstm ( see section 4 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we used bert word embeddings instead of elmo as input to the lstm ( see section 4 ) .\n",
            "0.8213083367458046\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we replaced the span representation refinement mechanism with our entity equalization approach ( see section 3 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we replaced the span representation refinement mechanism with our entity equalization approach ( see section 3 ) .\n",
            "0.8497377298861847\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results\n",
            "0.45462375196773575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "following pradhan et al. , we report precision , recall and f1 of the muc , b 3 and ceaf ? 4 metrics , and average the f 1 score of all three metrics to get the main evaluation metric used in the conll - 2012 coreference resolution task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "following pradhan et al. , we report precision , recall and f1 of the muc , b 3 and ceaf ? 4 metrics , and average the f 1 score of all three metrics to get the main evaluation metric used in the conll - 2012 coreference resolution task .\n",
            "0.8491403430732105\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we calculated the metrics using the official evaluation scripts of conll - 2012 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we calculated the metrics using the official evaluation scripts of conll - 2012 .\n",
            "0.8355293647019459\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results on the test set are shown in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results on the test set are shown in .\n",
            "0.8091695386215773\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our baseline is the span - ranking model from with elmo input features and second - order span representations , which achieves 73.0 % avg.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our baseline is the span - ranking model from with elmo input features and second - order span representations , which achieves 73.0 % avg.\n",
            "0.8354151879441067\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "f1 . replacing the elmo features with bert features achieves 76. 25 % average f1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "f1 . replacing the elmo features with bert features achieves 76. 25 % average f1 .\n",
            "0.6347800308215925\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "removing the second - order span - representations while using bert features achieves 76.37 % f1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "removing the second - order span - representations while using bert features achieves 76.37 % f1 , achieving higher recall and lower precision on all evaluation metrics , while somewhat surprisingly being better over all .\n",
            "0.8678659671427119\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "replacing secondorder span representations with entity equalization achieves 76. 64 % average f1 , while also consistently achieving the highest f 1 score on all three evaluation metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "replacing secondorder span representations with entity equalization achieves 76. 64 % average f1 , while also consistently achieving the highest f 1 score on all three evaluation metrics .\n",
            "0.7992938041681652\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average f1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our results set a new state of the art for coreference resolution , improving the previous state of the art by 3.6 % average f1 .\n",
            "0.8114437194773407\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "conclusion\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "conclusion\n",
            "0.5120527534823697\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in this work we presented a new state - of - the - art coreference resolution system .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in this work we presented a new state - of - the - art coreference resolution system .\n",
            "0.8603989602476113\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "key to our approach is the idea that each mention should contain information about all its coreferring mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "key to our approach is the idea that each mention should contain information about all its coreferring mentions .\n",
            "0.874884141234185\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here we implemented this idea by summing all mention representations within a cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here we implemented this idea by summing all mention representations within a cluster .\n",
            "0.8480990519540818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the future we plan to further enrich these representations by considering information from across the document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the future we plan to further enrich these representations by considering information from across the document .\n",
            "0.8661411943866255\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "furthermore , we can consider more structured representations of entities that reflect entity attributes and inter-entity relations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "furthermore , we can consider more structured representations of entities that reflect entity attributes and inter-entity relations .\n",
            "0.8662101367512055\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "end - to - end neural coreference resolution\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "end - to - end neural coreference resolution\n",
            "0.760614941552855\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we introduce the first end - to - end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector .\n",
            "0.8834935618264956\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each .\n",
            "0.8927121483907029\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model computes span embeddings that combine context - dependent boundary representations with a headfinding attention mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model computes span embeddings that combine context - dependent boundary representations with a headfinding attention mechanism .\n",
            "0.8268442479372122\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions .\n",
            "0.8293592274325914\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments demonstrate state - of - the - art performance , with again of 1.5 f1 on the ontonotes benchmark and by 3.1 f1 using a 5 - model ensemble , despite the fact that this is the first approach to be successfully trained with no external resources .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments demonstrate state - of - the - art performance , with again of 1.5 f1 on the ontonotes benchmark and by 3.1 f1 using a 5 - model ensemble , despite the fact that this is the first approach to be successfully trained with no external resources .\n",
            "0.8848660749927915\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we present the first state - of - the - art neural coreference resolution model that is learned end - toend given only gold mention clusters .\n",
            "0.8738031889082712\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all recent coreference models , including neural approaches that achieved impressive performance gains , rely on syntactic parsers , both for headword features and as the input to carefully handengineered mention proposal algorithms .\n",
            "0.8679344135259862\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we demonstrate for the first time that these resources are not required , and in fact performance can be improved significantly without them , by training an end - to - end neural model that jointly learns which spans are entity mentions and how to best cluster them .\n",
            "0.89662314250613\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters .\n",
            "0.8172613089629963\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it includes a span - ranking model that decides , for each span , which of the previous spans ( if any ) is a good antecedent .\n",
            "0.8780761058512124\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "at the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "at the core of our model are vector embeddings representing spans of text in the document , which combine context - dependent boundary representations with a head - finding attention mechanism over the span .\n",
            "0.8960914122525421\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the attention component is inspired by parser - derived head - word matching features from previous systems , but is less susceptible to cascading errors .\n",
            "0.9076475742449455\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in our analyses , we show empirically that these learned attention weights correlate strongly with traditional headedness definitions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in our analyses , we show empirically that these learned attention weights correlate strongly with traditional headedness definitions .\n",
            "0.8260217394405815\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "scoring all span pairs in our end - to - end model is impractical , since the complexity would be quartic in the document length .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "scoring all span pairs in our end - to - end model is impractical , since the complexity would be quartic in the document length .\n",
            "0.8783455026205039\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "therefore we factor the model over unary mention scores and pairwise antecedent scores , both of which are simple functions of the learned span embedding .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "therefore we factor the model over unary mention scores and pairwise antecedent scores , both of which are simple functions of the learned span embedding .\n",
            "0.8319632680749597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the unary mention scores are used to prune the space of spans and antecedents , to aggressively reduce the number of pairwise computations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the unary mention scores are used to prune the space of spans and antecedents , to aggressively reduce the number of pairwise computations .\n",
            "0.8260058595469653\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our final approach outperforms existing models by 1.5 f1 on the ontonotes benchmark and by 3.1 f1 using a 5 - model ensemble .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our final approach outperforms existing models by 1.5 f1 on the ontonotes benchmark and by 3.1 f1 using a 5 - model ensemble .\n",
            "0.753205687309802\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it is not only accurate , but also relatively interpretable .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it is not only accurate , but also relatively interpretable .\n",
            "0.8298997143101361\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model factors , for example , directly indicate whether an absent coreference link is due to low mention scores ( for either span ) or a low score from the mention ranking component .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model factors , for example , directly indicate whether an absent coreference link is due to low mention scores ( for either span ) or a low score from the mention ranking component .\n",
            "0.862639193508513\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the head - finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions .\n",
            "0.8818721850464806\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we leverage this over all interpretability to do detailed quantitative and qualitative analyses , providing insights into the strengths and weak - nesses of the approach .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we leverage this over all interpretability to do detailed quantitative and qualitative analyses , providing insights into the strengths and weak - nesses of the approach .\n",
            "0.8502379275877077\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "machine learning methods have along history in coreference resolution ( see for a detailed survey ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "machine learning methods have along history in coreference resolution ( see for a detailed survey ) .\n",
            "0.8826242604150116\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , the learning problem is challenging and , until very recently , handengineered systems built on top of automatically produced parse trees outperformed all learning approaches .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , the learning problem is challenging and , until very recently , handengineered systems built on top of automatically produced parse trees outperformed all learning approaches .\n",
            "0.8788430185884789\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "showed that highly lexical learning approaches reverse this trend , and more recent neural models have achieved significant performance gains .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "showed that highly lexical learning approaches reverse this trend , and more recent neural models have achieved significant performance gains .\n",
            "0.8290552030478127\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , all of these models use parsers for head features and include highly engineered mention proposal algorithms .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , all of these models use parsers for head features and include highly engineered mention proposal algorithms .\n",
            "0.8374988168338622\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 such pipelined systems suffer from two major drawbacks : ( 1 ) parsing mistakes can introduce cascading errors and ( 2 ) many of the handengineered rules do not generalize to new languages .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 such pipelined systems suffer from two major drawbacks : ( 1 ) parsing mistakes can introduce cascading errors and ( 2 ) many of the handengineered rules do not generalize to new languages .\n",
            "0.8417463453202274\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a non-pipelined system that jointly models mention detection and coreference resolution was first proposed by .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a non-pipelined system that jointly models mention detection and coreference resolution was first proposed by .\n",
            "0.836421912293529\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "they introduce a search - based system that predicts the coreference structure in a left - to - right transition system that can incorporate global features .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "they introduce a search - based system that predicts the coreference structure in a left - to - right transition system that can incorporate global features .\n",
            "0.9062946471340945\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in contrast , our approach performs well while making much stronger independence assumptions , enabling straightforward inference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in contrast , our approach performs well while making much stronger independence assumptions , enabling straightforward inference .\n",
            "0.8544070495284091\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "more generally , a wide variety of approaches for learning coreference models have been proposed .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "more generally , a wide variety of approaches for learning coreference models have been proposed .\n",
            "0.8608720283564544\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "they can typically be categorized as ( 1 ) mention - pair classifiers , ( 2 ) entity - level models , ( 3 ) latent - tree models , or ( 4 ) mention - ranking models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "they can typically be categorized as ( 1 ) mention - pair classifiers , ( 2 ) entity - level models , ( 3 ) latent - tree models , or ( 4 ) mention - ranking models .\n",
            "0.75828839565037\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our span - ranking approach is most similar to mention ranking , but we reason over a larger space by jointly detecting mentions and predicting coreference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our span - ranking approach is most similar to mention ranking , but we reason over a larger space by jointly detecting mentions and predicting coreference .\n",
            "0.8745425926959781\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "task\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "task\n",
            "0.6284976342669503\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we formulate the task of end - to - end coreference resolution as a set of decisions for every possible span in the document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we formulate the task of end - to - end coreference resolution as a set of decisions for every possible span in the document .\n",
            "0.8928783167186318\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the input is a document d containing t words along with metadata such as speaker and genre information .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the input is a document d containing t words along with metadata such as speaker and genre information .\n",
            "0.8960502561163194\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let n = t ( t + 1 ) 2 be the number of possible text spans in d .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let n = t ( t + 1 ) 2 be the number of possible text spans in d .\n",
            "0.7442351870756387\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "denote the start and end indices of a span i in d respectively by and end ( i ) , for 1 ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "denote the start and end indices of a span i in d respectively by and end ( i ) , for 1 ?\n",
            "0.8059876491407825\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "i ?\n",
            "0.4829123928440474\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "n .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "n .\n",
            "0.481201785405922\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we assume an ordering of the spans based on start ( i ) ; spans with the same start index are ordered by end ( i ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we assume an ordering of the spans based on start ( i ) ; spans with the same start index are ordered by end ( i ) .\n",
            "0.8392421517042122\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the task is to assign to each span i an antecedent y i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the task is to assign to each span i an antecedent y i .\n",
            "0.8096912194506353\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the set of possible assignments for each y i is y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the set of possible assignments for each y i is y ( i ) = {? , 1 , . . . , i ? 1 } , a dummy antecedent ?\n",
            "0.7579758390350506\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "and all preceding spans .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "and all preceding spans .\n",
            "0.7489516167371395\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "true antecedents of span i , i.e. span j such that 1 ? j ? i ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "true antecedents of span i , i.e. span j such that 1 ? j ? i ?\n",
            "0.7310144067882245\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 , represent coreference links between i and j .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 , represent coreference links between i and j .\n",
            "0.7546419332743319\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the dummy antecedent ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the dummy antecedent ?\n",
            "0.5992757561869155\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "represents two possible scenarios : ( 1 ) the span is not an entity mention or ( 2 ) the span is an entity mention but it is not coreferent with any previous span .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "represents two possible scenarios : ( 1 ) the span is not an entity mention or ( 2 ) the span is an entity mention but it is not coreferent with any previous span .\n",
            "0.8522910397548281\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these decisions implicitly define a final clustering , which can be recovered by grouping all spans thatare connected by a set of antecedent predictions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these decisions implicitly define a final clustering , which can be recovered by grouping all spans thatare connected by a set of antecedent predictions .\n",
            "0.871532017060285\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "model\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "model\n",
            "0.3921188713883906\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we aim to learn a conditional probability distribution p ( y 1 , . . . , y n | d ) whose most likely configuration produces the correct clustering .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we aim to learn a conditional probability distribution p ( y 1 , . . . , y n | d ) whose most likely configuration produces the correct clustering .\n",
            "0.8188668628662172\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use a product of multinomials for each span :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use a product of multinomials for each span :\n",
            "0.8208229303490835\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where s ( i , j ) is a pairwise score for a coreference link between span i and span j in document d .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where s ( i , j ) is a pairwise score for a coreference link between span i and span j in document d .\n",
            "0.7964324966198274\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we omit the document d from the notation when the context is unambiguous .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we omit the document d from the notation when the context is unambiguous .\n",
            "0.8602470044706513\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "there are three factors for this pairwise coreference score : ( 1 ) whether span i is a mention , ( 2 ) whether span j is a mention , and ( 3 ) whether j is an antecedent of i:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "there are three factors for this pairwise coreference score : ( 1 ) whether span i is a mention , ( 2 ) whether span j is a mention , and ( 3 ) whether j is an antecedent of i:\n",
            "0.8081346335050661\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "here s m ( i ) is a unary score for span i being a mention , and s a ( i , j ) is pairwise score for span j being an antecedent of span i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "here s m ( i ) is a unary score for span i being a mention , and s a ( i , j ) is pairwise score for span j being an antecedent of span i .\n",
            "0.7885273368122873\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "by fixing the score of the dummy antecedent ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "by fixing the score of the dummy antecedent ?\n",
            "0.7270933514312874\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to 0 , the model predicts the best scoring antecedent if any non-dummy scores are positive , and it abstains if they are all negative .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to 0 , the model predicts the best scoring antecedent if any non-dummy scores are positive , and it abstains if they are all negative .\n",
            "0.8399181885628714\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a challenging aspect of this model is that it s size is o ( t 4 ) in the document length .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a challenging aspect of this model is that it s size is o ( t 4 ) in the document length .\n",
            "0.8546596958475559\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as we will see in section 5 , the above factoring enables aggressive pruning of spans thatare unlikely to belong to a coreference cluster according the mention score s m ( i ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as we will see in section 5 , the above factoring enables aggressive pruning of spans thatare unlikely to belong to a coreference cluster according the mention score s m ( i ) .\n",
            "0.8747861707176006\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "scoring architecture\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "scoring architecture\n",
            "0.46243587808649067\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we propose an end - toend neural architecture that computes the above scores given the document and its metadata .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we propose an end - toend neural architecture that computes the above scores given the document and its metadata .\n",
            "0.8821617180151061\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "at the core of the model are vector representations g i for each possible span i , which we describe in detail in the following section .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "at the core of the model are vector representations g i for each possible span i , which we describe in detail in the following section .\n",
            "0.8576491743109897\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "given these span representations , the scoring functions above are computed via standard feed - forward neural networks :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "given these span representations , the scoring functions above are computed via standard feed - forward neural networks :\n",
            "0.838127908356216\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where denotes the dot product , denotes element - wise multiplication , and ffnn denotes a feed - forward neural network that computes a nonlinear mapping from input to output vectors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where denotes the dot product , denotes element - wise multiplication , and ffnn denotes a feed - forward neural network that computes a nonlinear mapping from input to output vectors .\n",
            "0.8261850257734646\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the antecedent scoring function s a ( i , j ) includes explicit element - wise similarity of each span g i g j and a feature vector ?( i , j ) encoding speaker and genre information from the metadata and the distance between the two spans .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the antecedent scoring function s a ( i , j ) includes explicit element - wise similarity of each span g i g j and a feature vector ?( i , j ) encoding speaker and genre information from the metadata and the distance between the two spans .\n",
            "0.8469934951144239\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "span representations\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "span representations\n",
            "0.5140561109172217\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "two types of information are crucial to accurately predicting coreference links : the context surrounding the mention span and the internal structure within the span .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "two types of information are crucial to accurately predicting coreference links : the context surrounding the mention span and the internal structure within the span .\n",
            "0.8701981882781298\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use a bidirectional lstm to encode the lexical information of both the inside and outside of each span .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use a bidirectional lstm to encode the lexical information of both the inside and outside of each span .\n",
            "0.8567561108397993\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also include an attention mechanism over words in each span to model head words .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also include an attention mechanism over words in each span to model head words .\n",
            "0.8795113487960792\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we assume vector representations of each word {x 1 , . . . , x t } , which are composed of fixed pretrained word embeddings and 1 - dimensional convolution neural networks ( cnn ) over characters ( see section 7.1 for details )\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we assume vector representations of each word {x 1 , . . . , x t } , which are composed of fixed pretrained word embeddings and 1 - dimensional convolution neural networks ( cnn ) over characters ( see section 7.1 for details )\n",
            "0.8168423074478669\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to compute vector representations of each span , we first use bidirectional lstms to encode every word in its context :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to compute vector representations of each span , we first use bidirectional lstms to encode every word in its context :\n",
            "0.8570414208259702\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where ? ? {? 1 , 1 } indicates the directionality of each lstm , and x * t is the concatenated output of the bidirectional lstm .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where ? ? {? 1 , 1 } indicates the directionality of each lstm , and x * t is the concatenated output of the bidirectional lstm .\n",
            "0.7899880464453456\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use independent lstms for every sentence , since cross - sentence context was not helpful in our experiments .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use independent lstms for every sentence , since cross - sentence context was not helpful in our experiments .\n",
            "0.8889402807496888\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "syntactic heads are typically included as features in previous systems .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "syntactic heads are typically included as features in previous systems .\n",
            "0.8291771345193771\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "instead of relying on syntactic parses , our model learns a taskspecific notion of headedness using an attention mechanism over words in each span :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "instead of relying on syntactic parses , our model learns a taskspecific notion of headedness using an attention mechanism over words in each span :\n",
            "0.865253139901508\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "wherex i is a weighted sum of word vectors in span i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "wherex i is a weighted sum of word vectors in span i .\n",
            "0.7636134851072229\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the weights a i ,t are automatically learned and correlate strongly with traditional definitions of head words as we will see in section 9.2 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the weights a i ,t are automatically learned and correlate strongly with traditional definitions of head words as we will see in section 9.2 .\n",
            "0.8861279412641506\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the above span information is concatenated to produce the final representation g i of span i:\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the above span information is concatenated to produce the final representation g i of span i:\n",
            "0.8218642180423213\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this generalizes the recurrent span representations recently proposed for questionanswering , which only include the boundary representations x * start ( i ) and x * end ( i ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this generalizes the recurrent span representations recently proposed for questionanswering , which only include the boundary representations x * start ( i ) and x * end ( i ) .\n",
            "0.8246242676082807\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we introduce the soft headword vector xi and a feature vector ?( i ) encoding the size of span i.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we introduce the soft headword vector xi and a feature vector ?( i ) encoding the size of span i.\n",
            "0.7884303038783786\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "inference\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "inference\n",
            "0.3027989136459489\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the size of the full model described above is o ( t 4 ) in the document length t .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the size of the full model described above is o ( t 4 ) in the document length t .\n",
            "0.8182228640967266\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to maintain computation efficiency , we prune the candidate spans greedily during both training and evaluation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to maintain computation efficiency , we prune the candidate spans greedily during both training and evaluation .\n",
            "0.8286409725830377\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we only consider spans with up to l words and compute their unary mention scores s m ( i ) ( as defined in section 4 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we only consider spans with up to l words and compute their unary mention scores s m ( i ) ( as defined in section 4 ) .\n",
            "0.8347033601786724\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to further reduce the number of spans to consider , we only keep up to ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to further reduce the number of spans to consider , we only keep up to ?\n",
            "0.8417927085323433\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "t spans with the highest mention scores and consider only up to k antecedents for each .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "t spans with the highest mention scores and consider only up to k antecedents for each .\n",
            "0.8283527329211648\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also enforce non-crossing bracketing structures with a simple suppression scheme .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also enforce non-crossing bracketing structures with a simple suppression scheme .\n",
            "0.8303694560728554\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we accept spans in decreasing order of the mention scores , unless , when considering span i , there exists a previously accepted span j such that\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we accept spans in decreasing order of the mention scores , unless , when considering span i , there exists a previously accepted span j such that\n",
            "0.8438178046106516\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "despite these aggressive pruning strategies , we maintain a high recall of gold mentions in our experiments ( over 92 % when ? = 0.4 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "despite these aggressive pruning strategies , we maintain a high recall of gold mentions in our experiments ( over 92 % when ? = 0.4 ) .\n",
            "0.8407972490325559\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for the remaining mentions , the joint distribution of antecedents for each document is computed in a forward pass over a single computation graph .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for the remaining mentions , the joint distribution of antecedents for each document is computed in a forward pass over a single computation graph .\n",
            "0.8537101956937359\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the final prediction is the clustering produced by the most likely configuration .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the final prediction is the clustering produced by the most likely configuration .\n",
            "0.8180442699923494\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "learning\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "learning\n",
            "0.5473753264817361\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the training data , only clustering information is observed .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the training data , only clustering information is observed .\n",
            "0.8688404479824253\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "since the antecedents are latent , we optimize the marginal log -likelihood of all correct antecedents implied by the gold clustering :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "since the antecedents are latent , we optimize the marginal log -likelihood of all correct antecedents implied by the gold clustering :\n",
            "0.8209047003486283\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where gold ( i ) is the set of spans in the gold cluster containing span i .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where gold ( i ) is the set of spans in the gold cluster containing span i .\n",
            "0.7888259532392331\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "if span i does not belong to a gold cluster or all gold antecedents have been pruned , gold ( i ) = {?}.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "if span i does not belong to a gold cluster or all gold antecedents have been pruned , gold ( i ) = {?}.\n",
            "0.775380600689108\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "by optimizing this objective , the model naturally learns to prune spans accurately .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "by optimizing this objective , the model naturally learns to prune spans accurately .\n",
            "0.8393230389079507\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while the initial pruning is completely random , only gold mentions receive positive updates .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while the initial pruning is completely random , only gold mentions receive positive updates .\n",
            "0.8317359678844365\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model can quickly leverage this learning signal for appropriate credit assignment to the different factors , such as the mention scores s m used for pruning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model can quickly leverage this learning signal for appropriate credit assignment to the different factors , such as the mention scores s m used for pruning .\n",
            "0.883854127247202\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over all model with respect to mention detection .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over all model with respect to mention detection .\n",
            "0.8288413679010702\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it also prevents the span pruning from introducing noise .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it also prevents the span pruning from introducing noise .\n",
            "0.7775506140590053\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , consider the case where span i has a single gold antecedent that was pruned , so gold ( i ) = {?}.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , consider the case where span i has a single gold antecedent that was pruned , so gold ( i ) = {?}.\n",
            "0.8136742664531744\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the learning objective will only correctly push the scores of non-gold antecedents lower , and it can not incorrectly push the score of the dummy antecedent higher .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the learning objective will only correctly push the scores of non-gold antecedents lower , and it can not incorrectly push the score of the dummy antecedent higher .\n",
            "0.8571083427317883\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this learning objective can be considered a span - level , cost - insensitive analog of the learning objective proposed by .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this learning objective can be considered a span - level , cost - insensitive analog of the learning objective proposed by .\n",
            "0.8918427921554312\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we experimented with these cost-sensitive alternatives ,\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we experimented with these cost-sensitive alternatives ,\n",
            "0.7982919215690802\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "including margin - based variants , but a simple maximum - likelihood objective proved to be most effective .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "including margin - based variants , but a simple maximum - likelihood objective proved to be most effective .\n",
            "0.8805307894935638\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments\n",
            "0.3699332036909505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use the english coreference resolution data from the conll - 2012 shared task in our experiments .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use the english coreference resolution data from the conll - 2012 shared task in our experiments .\n",
            "0.88320787784144\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this dataset contains 2802 training documents , 343 development documents , and 348 test documents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this dataset contains 2802 training documents , 343 development documents , and 348 test documents .\n",
            "0.7875245811728655\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the training documents contain on average 454 words and a maximum of 4009 words .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the training documents contain on average 454 words and a maximum of 4009 words .\n",
            "0.8371178320054599\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hyperparameters\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hyperparameters\n",
            "-0.1876930455932129\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "word representations\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "word representations\n",
            "0.6269291025853095\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the word embeddings area fixed concatenation of 300 - dimensional glove embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the word embeddings area fixed concatenation of 300 - dimensional glove embeddings and 50 - dimensional embeddings from , both normalized to be unit vectors .\n",
            "0.7530931954844388\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "outof - vocabulary words are represented by a vector of zeros .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "outof - vocabulary words are represented by a vector of zeros .\n",
            "0.8040537263688632\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the character cnn , characters are represented as learned 8 - dimensional embeddings .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the character cnn , characters are represented as learned 8 - dimensional embeddings .\n",
            "0.8214673056264474\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the convolutions have window sizes of 3 , 4 , and 5 characters , each consisting of 50 filters .\n",
            "0.7553897033917893\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "hidden dimensions\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "hidden dimensions\n",
            "0.5119799290606433\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the hidden states in the lstms have 200 dimensions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the hidden states in the lstms have 200 dimensions .\n",
            "0.8055393176872367\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each feedforward neural network consists of two hidden layers with 150 dimensions and rectified linear units .\n",
            "0.7135012141691001\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "feature encoding\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "feature encoding\n",
            "0.4948777506356639\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker .\n",
            "0.8716804774734765\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "following , the distance features are binned into the following buckets .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "following , the distance features are binned into the following buckets .\n",
            "0.8136028189501178\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all features ( speaker , genre , span distance , mention width ) are represented as learned 20 - dimensional embeddings .\n",
            "0.8225977358723431\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "pruning\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "pruning\n",
            "0.17451867543188496\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we prune the spans such that the maximum span width l = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents k = 250 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we prune the spans such that the maximum span width l = 10 , the number of spans per word ? = 0.4 , and the maximum number of antecedents k = 250 .\n",
            "0.7760716336244438\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "during training , documents are randomly truncated to up to 50 sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "during training , documents are randomly truncated to up to 50 sentences .\n",
            "0.8558679295571978\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "learning\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "learning\n",
            "0.5473753264817361\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we use adam for learning with a minibatch size of 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we use adam for learning with a minibatch size of 1 .\n",
            "0.8142369841010454\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the lstm weights are initialized with random orthonormal matrices as described in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the lstm weights are initialized with random orthonormal matrices as described in .\n",
            "0.7213401331392515\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we apply 0.5 dropout to the word embeddings and character cnn outputs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we apply 0.5 dropout to the word embeddings and character cnn outputs .\n",
            "0.7828646014000692\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we apply 0.2 dropout to all hidden layers and feature embeddings .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we apply 0.2 dropout to all hidden layers and feature embeddings .\n",
            "0.7615628020592764\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "dropout masks are shared across timesteps to preserve long - distance information as described in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "dropout masks are shared across timesteps to preserve long - distance information as described in .\n",
            "0.8618353856241218\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the learning rate is decayed by 0.1 % every 100 steps .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the learning rate is decayed by 0.1 % every 100 steps .\n",
            "0.7304355935442187\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model is trained for up to 150 epochs , with early stopping based on the development set .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model is trained for up to 150 epochs , with early stopping based on the development set .\n",
            "0.8496412989329689\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all code is implemented in tensor - flow and is publicly available .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all code is implemented in tensor - flow and is publicly available .\n",
            "0.8555099016431517\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "3\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "3\n",
            "0.229986804259578\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ensembling\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ensembling\n",
            "0.0\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also report ensemble experiments using five models trained with different random initializations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also report ensemble experiments using five models trained with different random initializations .\n",
            "0.7843603904438747\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ensembling is performed for both the span pruning and antecedent decisions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ensembling is performed for both the span pruning and antecedent decisions .\n",
            "0.8061647608988286\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "at test time , we first average the mention scores s m ( i ) over each model before pruning the spans .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "at test time , we first average the mention scores s m ( i ) over each model before pruning the spans .\n",
            "0.8164038359433575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "given the same pruned spans , each model then computes the antecedent scores s a ( i , j ) separately , and they are averaged to produce the final scores .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "given the same pruned spans , each model then computes the antecedent scores s a ( i , j ) separately , and they are averaged to produce the final scores .\n",
            "0.8392122471367826\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "results\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "results\n",
            "0.45462375196773575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we report the precision , recall , and f1 for the standard muc , b 3 , and ceaf ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we report the precision , recall , and f1 for the standard muc , b 3 , and ceaf ?\n",
            "0.7889318940820936\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "4 metrics using the official conll - 2012 evaluation scripts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "4 metrics using the official conll - 2012 evaluation scripts .\n",
            "0.7857334558862975\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the main evaluation is the average f1 of the three metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the main evaluation is the average f1 of the three metrics .\n",
            "0.788283526101365\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "coreference results\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "coreference results\n",
            "0.45462375196773575\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "table 1 compares our model to several previous systems that have driven substantial improvements over the past several years on the ontonotes benchmark .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "table 1 compares our model to several previous systems that have driven substantial improvements over the past several years on the ontonotes benchmark .\n",
            "0.8361871922090386\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we outperform previous systems in all metrics .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we outperform previous systems in all metrics .\n",
            "0.7732883359732484\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in particular , our single model improves the state - of - the - art average f1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in particular , our single model improves the state - of - the - art average f1 by 1.5 , and our 5 - model ensemble improves it by 3.1 .\n",
            "0.8336003858561767\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the most significant gains come from improvements in recall , which is likely due to our end - toend setup .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the most significant gains come from improvements in recall , which is likely due to our end - toend setup .\n",
            "0.8573151212789503\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "during training , pipelined systems typically discard any mentions that the mention detector misses , which for consists of more than 9 % of the labeled mentions in the training data .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "during training , pipelined systems typically discard any mentions that the mention detector misses , which for consists of more than 9 % of the labeled mentions in the training data .\n",
            "0.8675400019843825\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in contrast , we only discard mentions that exceed our maximum mention width of 10 , which accounts for less than 2 % of the training mentions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in contrast , we only discard mentions that exceed our maximum mention width of 10 , which accounts for less than 2 % of the training mentions .\n",
            "0.8339566473802243\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the contribution of joint mention scoring is further discussed in section 8.3\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the contribution of joint mention scoring is further discussed in section 8.3\n",
            "0.771964768124114\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ablations\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ablations\n",
            "-0.18641398968500775\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to show the importance of each component in our proposed model , we ablate various parts of the architecture and report the average f1 on the development set of the data ( see ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to show the importance of each component in our proposed model , we ablate various parts of the architecture and report the average f1 on the development set of the data ( see ) .\n",
            "0.858991403000636\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "features\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "features\n",
            "0.4537983150515719\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the distance between spans and the width of spans are crucial signals for coreference resolution , consistent with previous findings from other coreference models .\n",
            "0.8278546887407932\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "they contribute 3.8 f1 to the final result .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "they contribute 3.8 f1 to the final result .\n",
            "0.7552463586737432\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "word representations\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "word representations\n",
            "0.6269291025853095\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "since our word embeddings are fixed , having access to a variety of word embeddings allows for a more expressive model without overfitting .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "since our word embeddings are fixed , having access to a variety of word embeddings allows for a more expressive model without overfitting .\n",
            "0.8558997825583107\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we hypothesis that the different learning objectives of the glove and turian embeddings provide orthogonal information ( the former is word - order insensitive while the latter is word - order sensitive ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we hypothesis that the different learning objectives of the glove and turian embeddings provide orthogonal information ( the former is word - order insensitive while the latter is word - order sensitive ) .\n",
            "0.8953502552772329\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "both embeddings contribute to some improvement in development f1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "both embeddings contribute to some improvement in development f1 .\n",
            "0.7441871793098459\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the character cnn provides morphological information and away to backoff for out - ofvocabulary words .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the character cnn provides morphological information and away to backoff for out - ofvocabulary words .\n",
            "0.8606096235215464\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "since coreference decisions often involve rare named entities , we see a contribution of 0.9 f1 from character - level modeling .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "since coreference decisions often involve rare named entities , we see a contribution of 0.9 f1 from character - level modeling .\n",
            "0.8776540133670004\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "metadata speaker and genre indicators many not be available in downstream applications .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "metadata speaker and genre indicators many not be available in downstream applications .\n",
            "0.8567188051518336\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we show that performance degrades by 1.4 f1 without them , but is still on par with previous state - of - theart systems that assume access to this metadata .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we show that performance degrades by 1.4 f1 without them , but is still on par with previous state - of - theart systems that assume access to this metadata .\n",
            "0.8796798167523942\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "head - finding attention\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "head - finding attention\n",
            "0.7307872142899522\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ablations also show a 1.3 f1 degradation in performance without the attention mechanism for finding task - specific heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ablations also show a 1.3 f1 degradation in performance without the attention mechanism for finding task - specific heads .\n",
            "0.840921413213589\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as we will see in section 9.4 , the attention mechanism should not be viewed as simply an approximation of syntactic heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as we will see in section 9.4 , the attention mechanism should not be viewed as simply an approximation of syntactic heads .\n",
            "0.88389764531577\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in many cases , it is beneficial to pay attention to multiple words thatare useful specifically for coreference but are not traditionally considered to be syntactic heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in many cases , it is beneficial to pay attention to multiple words thatare useful specifically for coreference but are not traditionally considered to be syntactic heads .\n",
            "0.8845642527046517\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "avg. f1 ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "avg. f1 ?\n",
            "0.33663539531096776\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our model ( joint mention scoring ) 67.7 w/ rule - based mentions 66.7 - 1.0 w/ oracle mentions 85.2 + 17.5\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our model ( joint mention scoring ) 67.7 w/ rule - based mentions 66.7 - 1.0 w/ oracle mentions 85.2 + 17.5\n",
            "0.6084338987616931\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "comparing span pruning strategies\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "comparing span pruning strategies\n",
            "0.5602022572882033\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to tease apart the contributions of improved mention scoring and improved coreference decisions , we compare the results of our model with alternate span pruning strategies .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to tease apart the contributions of improved mention scoring and improved coreference decisions , we compare the results of our model with alternate span pruning strategies .\n",
            "0.8388605238732576\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in these experiments , we use the alternate spans for both training and evaluation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in these experiments , we use the alternate spans for both training and evaluation .\n",
            "0.8550556419983174\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( raghunathan et al. , 2010 ) degrades performance by 1 f1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as shown in , keeping mention candidates detected by the rule - based system over predicted parse trees ( raghunathan et al. , 2010 ) degrades performance by 1 f1 .\n",
            "0.8467273641019453\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also provide oracle experiment results , where we keep exactly the mentions thatare present in gold coreference clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also provide oracle experiment results , where we keep exactly the mentions thatare present in gold coreference clusters .\n",
            "0.8529483961283745\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "with oracle mentions , we see an improvement of 17.5 f1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "with oracle mentions , we see an improvement of 17.5 f1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .\n",
            "0.8480864978033089\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "analysis\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "analysis\n",
            "0.49990667593127536\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "to highlight the strengths and weaknesses of our model , we provide both quantitative and qualitative analyses .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "to highlight the strengths and weaknesses of our model , we provide both quantitative and qualitative analyses .\n",
            "0.82168381947191\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in the following discussion , we use predictions from the single model rather than the ensembled model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in the following discussion , we use predictions from the single model rather than the ensembled model .\n",
            "0.8567950777413286\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "mention recall\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "mention recall\n",
            "0.5482582430152649\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the training data only provides a weak signal for spans that correspond to entity mentions , since singleton clusters are not explicitly labeled .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the training data only provides a weak signal for spans that correspond to entity mentions , since singleton clusters are not explicitly labeled .\n",
            "0.8857595243946214\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as a byproduct of optimizing marginal likelihood , our model automatically learns a useful ranking of spans via the unary mention scores from section 4 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as a byproduct of optimizing marginal likelihood , our model automatically learns a useful ranking of spans via the unary mention scores from section 4 .\n",
            "0.858452842564304\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the top spans , according to the mention scores , cover a large portion of the mentions in gold clusters , as shown in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the top spans , according to the mention scores , cover a large portion of the mentions in gold clusters , as shown in .\n",
            "0.8472091134483246\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "given a similar number of spans kept , our recall is comparable to the rulebased mention detector ( raghunathan et al. , 2010 ) that produces 0.26 spans per word with a recall of 89.2 % .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "given a similar number of spans kept , our recall is comparable to the rulebased mention detector ( raghunathan et al. , 2010 ) that produces 0.26 spans per word with a recall of 89.2 % .\n",
            "0.8562753731696681\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as we increase the number of spans per word (? in section 5 ) , we observe higher recall but with diminishing returns .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as we increase the number of spans per word (? in section 5 ) , we observe higher recall but with diminishing returns .\n",
            "0.8593968648206202\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in our experiments , keeping 0.4 spans per word results in 92.7 % recall in the development data .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in our experiments , keeping 0.4 spans per word results in 92.7 % recall in the development data .\n",
            "0.8194010477016084\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "mention precision\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "mention precision\n",
            "0.5396639209044465\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while the training data does not offer a direct measure of mention precision , we can use the gold syntactic structures provided in the data as a proxy .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while the training data does not offer a direct measure of mention precision , we can use the gold syntactic structures provided in the data as a proxy .\n",
            "0.9038677857923226\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "spans with high mention scores should correspond to syntactic constituents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "spans with high mention scores should correspond to syntactic constituents .\n",
            "0.8102431547274821\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in , we show the precision of topscoring spans when keeping 0.4 spans per word .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in , we show the precision of topscoring spans when keeping 0.4 spans per word .\n",
            "0.8256745069493089\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for spans with 2 - 5 words , 75 - 90 % of the predictions are constituents , indicating that the vast majority of the mentions are syntactically plausible .\n",
            "0.8522418338264011\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "longer spans , which are all relatively rare , prove more difficult for the model , and precision drops to 46 % for spans with 10 words .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "longer spans , which are all relatively rare , prove more difficult for the model , and precision drops to 46 % for spans with 10 words .\n",
            "0.8602392257027102\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "head agreement\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "head agreement\n",
            "0.5543808587592415\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also investigate how well the learned head preferences correlate with syntactic heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also investigate how well the learned head preferences correlate with syntactic heads .\n",
            "0.8332218943029628\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each of the top - scoring spans in the development data that correspond to gold constituents , we compute the word with the highest attention weight .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each of the top - scoring spans in the development data that correspond to gold constituents , we compute the word with the highest attention weight .\n",
            "0.8765150290722216\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we plot in the proportion of these words that match syntactic heads .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we plot in the proportion of these words that match syntactic heads .\n",
            "0.8367726862463778\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "agreement ranges between 68 - 93 % , which is surprisingly 1 ( a fire in a bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "agreement ranges between 68 - 93 % , which is surprisingly 1 ( a fire in a bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .\n",
            "0.7821090071759474\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "most of the deceased were killed in the crush as workers tried to flee ( the blaze ) in the four - story building .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "most of the deceased were killed in the crush as workers tried to flee ( the blaze ) in the four - story building .\n",
            "0.7966652194318442\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a fire in ( a bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a fire in ( a bangladeshi garment factory ) has left at least 37 people dead and 100 hospitalized .\n",
            "0.7458802524101301\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "most of the deceased were killed in the crush as workers tried to flee the blaze in ( the four - story building ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "most of the deceased were killed in the crush as workers tried to flee the blaze in ( the four - story building ) .\n",
            "0.7966652141526569\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "2\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "2\n",
            "0.20276126991161597\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we are looking for ( a region of central italy bordering the adriatic sea ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we are looking for ( a region of central italy bordering the adriatic sea ) .\n",
            "0.7486399161236144\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( the area ) is mostly mountainous and includes mt. corno , the highest peak of the apennines .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( the area ) is mostly mountainous and includes mt. corno , the highest peak of the apennines .\n",
            "0.7739603575080186\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( it ) also includes a lot of sheep , good clean - living , healthy sheep , and an italian entrepreneur has an idea about how to make a little money of them .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( it ) also includes a lot of sheep , good clean - living , healthy sheep , and an italian entrepreneur has an idea about how to make a little money of them .\n",
            "0.8467209123415232\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "3\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "3\n",
            "0.229986804259578\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( the flight attendants ) have until 6:00 today to ratify labor concessions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( the flight attendants ) have until 6:00 today to ratify labor concessions .\n",
            "0.6927401523760682\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "( the pilots ' ) union and ground crew did so yesterday .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "( the pilots ' ) union and ground crew did so yesterday .\n",
            "0.7525067624209536\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "also such location devices , ( some ships ) have smoke floats ( they ) can toss out so the man overboard will be able to use smoke signals as away of trying to , let the rescuer locate ( them ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "also such location devices , ( some ships ) have smoke floats ( they ) can toss out so the man overboard will be able to use smoke signals as away of trying to , let the rescuer locate ( them ) .\n",
            "0.8308792124141434\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "high , since no explicit supervision of syntactic heads is provided .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "high , since no explicit supervision of syntactic heads is provided .\n",
            "0.8493157750771776\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model simply learns from the clustering data that these head words are useful for making coreference decisions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model simply learns from the clustering data that these head words are useful for making coreference decisions .\n",
            "0.8903611926233818\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "qualitative analysis\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "qualitative analysis\n",
            "0.4833952063873106\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our qualitative analysis in highlights the strengths and weaknesses of our model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our qualitative analysis in highlights the strengths and weaknesses of our model .\n",
            "0.7748823838808211\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each row is a visualization of a single coreference cluster predicted by the model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each row is a visualization of a single coreference cluster predicted by the model .\n",
            "0.8151380626951589\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bolded spans in parentheses belong to the predicted cluster , and the redness of a word indicates its weight from the headfinding attention mechanism ( a i ,t in section 4 ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bolded spans in parentheses belong to the predicted cluster , and the redness of a word indicates its weight from the headfinding attention mechanism ( a i ,t in section 4 ) .\n",
            "0.8591718957489175\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "strengths\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "strengths\n",
            "0.3627149218545955\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the effectiveness of the attention mechanism for making coreference decisions can be seen in example 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the effectiveness of the attention mechanism for making coreference decisions can be seen in example 1 .\n",
            "0.8531538177123023\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model pays attention to fire in the span a fire in a bangladeshi garment factory , allowing it to successfully predict the coreference link with the blaze .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model pays attention to fire in the span a fire in a bangladeshi garment factory , allowing it to successfully predict the coreference link with the blaze .\n",
            "0.8262477385196021\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for a subspan of that mention , a bangladeshi garment factory , the model pays most attention instead to factory , allowing it successfully predict the coreference link with the four - story building .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for a subspan of that mention , a bangladeshi garment factory , the model pays most attention instead to factory , allowing it successfully predict the coreference link with the four - story building .\n",
            "0.8688979014652526\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the task - specific nature of the attention mechanism is also illustrated in example 4 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the task - specific nature of the attention mechanism is also illustrated in example 4 .\n",
            "0.8865241367023341\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model generally pays attention to coordinators more than the content of the coordination , since coordinators , such as and , provide strong cues for plurality .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model generally pays attention to coordinators more than the content of the coordination , since coordinators , such as and , provide strong cues for plurality .\n",
            "0.8661090943267596\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the model is capable of detecting relatively long and complex noun phrases , such as a region of central italy bordering the adriatic sea in example 2 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the model is capable of detecting relatively long and complex noun phrases , such as a region of central italy bordering the adriatic sea in example 2 .\n",
            "0.8414014321735052\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it also appropriately pays atten - tion to region , showing that the attention mechanism provides more than content - word classification .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it also appropriately pays atten - tion to region , showing that the attention mechanism provides more than content - word classification .\n",
            "0.9040880585246085\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the context encoding provided by the bidirectional lstms is critical to making informative headword decisions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the context encoding provided by the bidirectional lstms is critical to making informative headword decisions .\n",
            "0.8707840171310136\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "weaknesses\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "weaknesses\n",
            "0.33172093854922063\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a benefit of using neural models for coreference resolution is their ability to use word embeddings to capture similarity between words , a property that many traditional featurebased models lack .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a benefit of using neural models for coreference resolution is their ability to use word embeddings to capture similarity between words , a property that many traditional featurebased models lack .\n",
            "0.8797005023654478\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while this can dramatically increase recall , as demonstrated in example 1 , it is also prone to predicting false positive links when the model conflates paraphrasing with relatedness or similarity .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while this can dramatically increase recall , as demonstrated in example 1 , it is also prone to predicting false positive links when the model conflates paraphrasing with relatedness or similarity .\n",
            "0.8740081012198518\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in example 3 , the model mistakenly predicts a link between\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in example 3 , the model mistakenly predicts a link between\n",
            "0.8172663499770446\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the flight attendants and the pilots '.\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the flight attendants and the pilots '.\n",
            "0.6805238657309574\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the predicted head words attendants and pilots likely have nearby word embeddings , which is a signal used - and often overused - by the model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the predicted head words attendants and pilots likely have nearby word embeddings , which is a signal used - and often overused - by the model .\n",
            "0.8764196699965687\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the same type of error is made in example 4 , where the model predicts a coreference link between prince charles and his new wife camilla and charles and diana , two noncoreferent mentions thatare similar in many ways .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the same type of error is made in example 4 , where the model predicts a coreference link between prince charles and his new wife camilla and charles and diana , two noncoreferent mentions thatare similar in many ways .\n",
            "0.8315011247994782\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these mistakes suggest substantial room for improvement with word or span representations that can cleanly distinguish between equivalence , entailment , and alternation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these mistakes suggest substantial room for improvement with word or span representations that can cleanly distinguish between equivalence , entailment , and alternation .\n",
            "0.8516047044037688\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "unsurprisingly , our model does little in the uphill battle of making coreference decisions requiring world knowledge .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "unsurprisingly , our model does little in the uphill battle of making coreference decisions requiring world knowledge .\n",
            "0.8458095685822558\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in example 5 , the model incorrectly decides that them ( in the context of let the rescuer locate them ) is coreferent with some ships , likely due to plurality cues .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in example 5 , the model incorrectly decides that them ( in the context of let the rescuer locate them ) is coreferent with some ships , likely due to plurality cues .\n",
            "0.8819345960845023\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , an ideal model that uses common - sense reasoning would instead correctly infer that a rescuer is more likely to look for the man overboard rather than the ship from which he fell .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , an ideal model that uses common - sense reasoning would instead correctly infer that a rescuer is more likely to look for the man overboard rather than the ship from which he fell .\n",
            "0.8712865045806508\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this type of reasoning would require either ( 1 ) models that integrate external sources of knowledge with more complex inference or ( 2 ) a vastly larger corpus of training data to overcome the sparsity of these patterns .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this type of reasoning would require either ( 1 ) models that integrate external sources of knowledge with more complex inference or ( 2 ) a vastly larger corpus of training data to overcome the sparsity of these patterns .\n",
            "0.8800970141227497\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "conclusion\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "conclusion\n",
            "0.5120527534823697\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we presented a state - of - the - art coreference resolution model that is trained end - to - end for the first time .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we presented a state - of - the - art coreference resolution model that is trained end - to - end for the first time .\n",
            "0.8626673490776446\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our final model ensemble improves performance on the ontonotes benchmark by over 3 f1 without external preprocessing tools used by previous systems .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our final model ensemble improves performance on the ontonotes benchmark by over 3 f1 without external preprocessing tools used by previous systems .\n",
            "0.8001857826877489\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we showed that our model implicitly learns to generate useful mention candidates from the space of all possible spans .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we showed that our model implicitly learns to generate useful mention candidates from the space of all possible spans .\n",
            "0.8697488650470373\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a novel head - finding attention mechanism also learns a taskspecific preference for head words , which we empirically showed correlate strongly with traditional head - word definitions .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a novel head - finding attention mechanism also learns a taskspecific preference for head words , which we empirically showed correlate strongly with traditional head - word definitions .\n",
            "0.9029724208492934\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while our model substantially pushes the stateof - the - art performance , the improvements are potentially complementary to a large body of work on various strategies to improve coreference resolution , including entity - level inference and incorporating world knowledge , which are important avenues for future work .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while our model substantially pushes the stateof - the - art performance , the improvements are potentially complementary to a large body of work on various strategies to improve coreference resolution , including entity - level inference and incorporating world knowledge , which are important avenues for future work .\n",
            "0.8987261935650942\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "title\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "title\n",
            "0.4332305433690145\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert for coreference resolution : baselines and analysis\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert for coreference resolution : baselines and analysis\n",
            "0.6276940975359384\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "abstract\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "abstract\n",
            "0.4180977772107475\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we apply bert to coreference resolution , achieving strong improvements on the ontonotes ( + 3.9 f1 ) and gap ( + 11.5 f1 ) benchmarks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we apply bert to coreference resolution , achieving strong improvements on the ontonotes ( + 3.9 f1 ) and gap ( + 11.5 f1 ) benchmarks .\n",
            "0.717526073636189\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a qualitative analysis of model predictions indicates that , compared to elmo and bert - base , bert - large is particularly better at distinguishing between related but distinct entities ( e.g. , president and ceo ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a qualitative analysis of model predictions indicates that , compared to elmo and bert - base , bert - large is particularly better at distinguishing between related but distinct entities ( e.g. , president and ceo ) .\n",
            "0.8823754324579359\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .\n",
            "0.8930752397996163\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "our code and models are publicly available 1 .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "our code and models are publicly available 1 .\n",
            "0.7943995536837032\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "introduction\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "introduction\n",
            "0.5200929285021679\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "recent bert - based models have reported dramatic gains on multiple semantic benchmarks including question - answering , natural language inference , and named entity recognition .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "recent bert - based models have reported dramatic gains on multiple semantic benchmarks including question - answering , natural language inference , and named entity recognition .\n",
            "0.894234515345144\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "apart from better bidirectional reasoning , one of bert 's major improvements over previous methods is passage - level training , 2 which allows it to better model longer sequences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "apart from better bidirectional reasoning , one of bert 's major improvements over previous methods is passage - level training , 2 which allows it to better model longer sequences .\n",
            "0.8906095848687077\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we fine - tune bert to coreference resolution , achieving strong improvements on the gap and benchmarks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we fine - tune bert to coreference resolution , achieving strong improvements on the gap and benchmarks .\n",
            "0.8189698103664133\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we present two ways of extending the c 2f - coref model in .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we present two ways of extending the c 2f - coref model in .\n",
            "0.8195030549847451\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the independent variant uses non-overlapping segments each of which acts as an independent instance for bert .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the independent variant uses non-overlapping segments each of which acts as an independent instance for bert .\n",
            "0.8485541888822893\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens .\n",
            "0.8663828242518791\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert - large improves over elmo - based c 2f - coref 3.9 % on ontonotes and 11.5 % on gap ( both absolute ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert - large improves over elmo - based c 2f - coref 3.9 % on ontonotes and 11.5 % on gap ( both absolute ) .\n",
            "0.6983935566910862\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "1 https://github.com/mandarjoshi90/coref\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "1 https://github.com/mandarjoshi90/coref\n",
            "0.22804272532698444\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "2 each bert training example consists of around 512 word pieces , while elmo is trained on single sentences .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "2 each bert training example consists of around 512 word pieces , while elmo is trained on single sentences .\n",
            "0.8382280229033684\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "a qualitative analysis of bert and elmobased models suggests that bert - large ( unlike bert - base ) is remarkably better at distinguishing between related yet distinct entities or concepts ( e.g. , repulse bay and victoria harbor ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "a qualitative analysis of bert and elmobased models suggests that bert - large ( unlike bert - base ) is remarkably better at distinguishing between related yet distinct entities or concepts ( e.g. , repulse bay and victoria harbor ) .\n",
            "0.8631487517444812\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , both models often struggle to resolve coreferences for cases that require world knowledge ( e.g. , the developing story and the scandal ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , both models often struggle to resolve coreferences for cases that require world knowledge ( e.g. , the developing story and the scandal ) .\n",
            "0.8884429026455577\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "likewise , modeling pronouns remains difficult , especially in conversations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "likewise , modeling pronouns remains difficult , especially in conversations .\n",
            "0.8277332377713349\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also find that bert - large benefits from using longer context windows ( 384 word pieces ) while bert - base performs better with shorter contexts ( 128 word pieces ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also find that bert - large benefits from using longer context windows ( 384 word pieces ) while bert - base performs better with shorter contexts ( 128 word pieces ) .\n",
            "0.870124576965309\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "yet , both variants perform much worse with longer context windows ( 512 tokens ) in spite of being trained on 512 - size contexts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "yet , both variants perform much worse with longer context windows ( 512 tokens ) in spite of being trained on 512 - size contexts .\n",
            "0.8574657755524097\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "moreover , the overlap variant , which artificially extends the context window beyond 512 tokens provides no improvement .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "moreover , the overlap variant , which artificially extends the context window beyond 512 tokens provides no improvement .\n",
            "0.8401112984261844\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this indicates that using larger context windows for pretraining might not translate into effective long - range features for a downstream task .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this indicates that using larger context windows for pretraining might not translate into effective long - range features for a downstream task .\n",
            "0.8996208611607329\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "larger models also exacerbate the memory - intensive nature of span representations , which have driven recent improvements in coreference resolution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "larger models also exacerbate the memory - intensive nature of span representations , which have driven recent improvements in coreference resolution .\n",
            "0.8632772916186761\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "together , these observations suggest that there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "together , these observations suggest that there is still room for improvement in modeling document - level context , conversations , and mention paraphrasing .\n",
            "0.8972170296223504\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "method\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "method\n",
            "0.5025961146098722\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for our experiments , we use the higher - order coreference model in which is the current state of the art for the english ontonotes dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for our experiments , we use the higher - order coreference model in which is the current state of the art for the english ontonotes dataset .\n",
            "0.870188474515303\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we refer to this as c 2 f - coref in the paper .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we refer to this as c 2 f - coref in the paper .\n",
            "0.8353283116434236\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "overview of c2f- coref\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "overview of c2f- coref\n",
            "0.6331155643380734\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for each mention span x , the model learns a distribution p ( ) over possible antecedent spans y :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for each mention span x , the model learns a distribution p ( ) over possible antecedent spans y :\n",
            "0.8024292882370586\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the scoring function s ( x , y ) between spans x and y uses fixed - length span representations , g x and g y to represent its inputs .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the scoring function s ( x , y ) between spans x and y uses fixed - length span representations , g x and g y to represent its inputs .\n",
            "0.7232667323570608\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these consist of a concatenation of three vectors : the two lstm states of the span endpoints and an attention vector computed over the span tokens .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these consist of a concatenation of three vectors : the two lstm states of the span endpoints and an attention vector computed over the span tokens .\n",
            "0.8013003719553654\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it computes the score s ( x , y ) by the mention score of x ( i.e. how likely is the span x to be a mention ) , the mention score of y , and the joint compatibility score of x and y ( i.e. assuming they are both mentions , how likely are x and y to refer to the same entity ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it computes the score s ( x , y ) by the mention score of x ( i.e. how likely is the span x to be a mention ) , the mention score of y , and the joint compatibility score of x and y ( i.e. assuming they are both mentions , how likely are x and y to refer to the same entity ) .\n",
            "0.8218635460594083\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the components are computed as follows :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the components are computed as follows :\n",
            "0.7853813969929182\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where ffnn ( ) represents a feedforward neural network and ?( x , y) represents speaker and metadata features .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where ffnn ( ) represents a feedforward neural network and ?( x , y) represents speaker and metadata features .\n",
            "0.7730228187183943\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these span representations are later refined using antecedent distribution from a spanranking architecture as an attention mechanism .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these span representations are later refined using antecedent distribution from a spanranking architecture as an attention mechanism .\n",
            "0.8649315100617048\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "applying bert\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "applying bert\n",
            "0.2644317145243052\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we replace the entire lstm - based encoder ( with elmo and glove embeddings as input ) in c2fcoref with the bert transformer .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we replace the entire lstm - based encoder ( with elmo and glove embeddings as input ) in c2fcoref with the bert transformer .\n",
            "0.8139293012229916\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we treat the first and last word - pieces ( concatenated with the attended version of all word pieces in the span ) as span representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we treat the first and last word - pieces ( concatenated with the attended version of all word pieces in the span ) as span representations .\n",
            "0.8759283360204405\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "documents are split into segments of max segment len , which we treat as a hyperparameter .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "documents are split into segments of max segment len , which we treat as a hyperparameter .\n",
            "0.8701801018594729\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we experiment with two variants of splitting :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we experiment with two variants of splitting :\n",
            "0.7569095345295856\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "independent\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "independent\n",
            "0.45344012154241925\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the independent variant uses nonoverlapping segments each of which acts as an independent instance for bert .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the independent variant uses nonoverlapping segments each of which acts as an independent instance for bert .\n",
            "0.8323982680857316\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the representation for each token is limited to the set of words that lie in its segment .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the representation for each token is limited to the set of words that lie in its segment .\n",
            "0.867583993870132\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as bert is trained on sequences of at most 512 word pieces , this variant has limited encoding capacity especially for tokens that lie at the start or end of their segments .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as bert is trained on sequences of at most 512 word pieces , this variant has limited encoding capacity especially for tokens that lie at the start or end of their segments .\n",
            "0.8740662428710743\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "overlap\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "overlap\n",
            "0.35726636893991853\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the overlap variant splits the document into overlapping segments by creating a tsized segment after every t / 2 tokens .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the overlap variant splits the document into overlapping segments by creating a tsized segment after every t / 2 tokens .\n",
            "0.8327771229220207\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these segments are then passed onto the bert encoder independently , and the final token representation is derived by element - wise interpolation of representations from both overlapping segments .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these segments are then passed onto the bert encoder independently , and the final token representation is derived by element - wise interpolation of representations from both overlapping segments .\n",
            "0.8539317306959984\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "let r 1 ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "let r 1 ?\n",
            "0.5339694427010137\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd and r 2 ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd and r 2 ?\n",
            "0.45549909814303474\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd be the token representations from the overlapping bert segments .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd be the token representations from the overlapping bert segments .\n",
            "0.7639304515026963\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the final representation r ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the final representation r ?\n",
            "0.7176761274834026\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "rd is given by :\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "rd is given by :\n",
            "0.6327793809016533\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "where w ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "where w ?\n",
            "0.5406789048797561\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "r 2dd is a trained parameter and [ ; ] represents concatenation .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "r 2dd is a trained parameter and [ ; ] represents concatenation .\n",
            "0.7715490666957991\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this variant allows the model to artificially increase the context window beyond the max segment len hyperparameter .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this variant allows the model to artificially increase the context window beyond the max segment len hyperparameter .\n",
            "0.8165559933318471\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "all layers in both model variants are then finetuned following .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "all layers in both model variants are then finetuned following .\n",
            "0.7829963269500456\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "experiments\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "experiments\n",
            "0.3699332036909505\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we evaluate our bert - based models on two benchmarks : the paragraph - level gap dataset , and the documentlevel english ontonotes 5.0 dataset .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we evaluate our bert - based models on two benchmarks : the paragraph - level gap dataset , and the documentlevel english ontonotes 5.0 dataset .\n",
            "0.8593409267608768\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ontonotes examples are considerably longer and typically require multiple segments to read the entire document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ontonotes examples are considerably longer and typically require multiple segments to read the entire document .\n",
            "0.8755286760365028\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "implementation and hyperparameters\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "implementation and hyperparameters\n",
            "0.3310333019900042\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we extend the original tensorflow implementations of c 2f - coref 3 and bert .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we extend the original tensorflow implementations of c 2f - coref 3 and bert .\n",
            "0.7504155732717832\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we fine tune all models on the ontonotes english data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the bert parameters and the task parameters respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we fine tune all models on the ontonotes english data for 20 epochs using a dropout of 0.3 , and learning rates of 1 10 ?5 and 2 10 ? 4 with linear decay for the bert parameters and the task parameters respectively .\n",
            "0.8264833476602715\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we found that this made a sizable impact of 2 - 3 % over using the same learning rate for all parameters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we found that this made a sizable impact of 2 - 3 % over using the same learning rate for all parameters .\n",
            "0.8617329361742949\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for bert - base and bert - large respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we trained separate models with max segment len of 128 , 256 , 384 , and 512 ; the models trained on 128 and 384 word pieces performed the best for bert - base and bert - large respectively .\n",
            "0.799881239649744\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "as span representations are memory intensive , we truncate documents randomly to eleven segments for bert - base and\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "as span representations are memory intensive , we truncate documents randomly to eleven segments for bert - base and\n",
            "0.869403911368567\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "martschat and strube 2017 ) , which does not use contextualized representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "martschat and strube 2017 ) , which does not use contextualized representations .\n",
            "0.8266729379541154\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in addition to being more computationally efficient than e2e -coref , c2 f - coref iteratively refines span representations using attention for higher - order reasoning .\n",
            "0.8432209976328975\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "paragraph level : gap\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "paragraph level : gap\n",
            "0.6501883254812231\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "gap ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from wikipedia snippets .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "gap ) is a human - labeled corpus of ambiguous pronoun - name pairs derived from wikipedia snippets .\n",
            "0.8507599653783664\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "examples in the gap dataset fit within a single bert segment , thus eliminating the need for cross - segment inference .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "examples in the gap dataset fit within a single bert segment , thus eliminating the need for cross - segment inference .\n",
            "0.8755278232553757\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "following , we trained our bert - based c 2f - coref model on ontonotes .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "following , we trained our bert - based c 2f - coref model on ontonotes .\n",
            "0.7737806155145831\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "5\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "5\n",
            "0.24023858029353806\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the predicted clusters were scored against gap examples according to the official evaluation script .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the predicted clusters were scored against gap examples according to the official evaluation script .\n",
            "0.8233942389582897\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "table 2 shows that bert improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "table 2 shows that bert improves c 2 f - coref by 9 % and 11.5 % for the base and large models respectively .\n",
            "0.7370327017537758\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these results are inline with large gains reported for a variety of semantic tasks by bertbased models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these results are inline with large gains reported for a variety of semantic tasks by bertbased models .\n",
            "0.8485449418490105\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "document level : ontonotes\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "document level : ontonotes\n",
            "0.6865245179235964\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "ontonotes ( english ) is a document - level dataset from the conll - 2012 shared task on coreference resolution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "ontonotes ( english ) is a document - level dataset from the conll - 2012 shared task on coreference resolution .\n",
            "0.8662425791743772\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "it consists of about one million words of newswire , magazine articles , broadcast news , broadcast conversations , web data and conversational speech data , and the new testament .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "it consists of about one million words of newswire , magazine articles , broadcast news , broadcast conversations , web data and conversational speech data , and the new testament .\n",
            "0.8641578962203731\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the main evaluation is the average f1 of three metrics - muc , b 3 , and ceaf ?\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the main evaluation is the average f1 of three metrics - muc , b 3 , and ceaf ?\n",
            "0.816763534900028\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "4 on the test set according to the official conll - 2012 evaluation scripts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "4 on the test set according to the official conll - 2012 evaluation scripts .\n",
            "0.8207296467085947\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "shows that bert - base offers an improvement of 0.9 % over the elmo - based c2 fcoref model .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "shows that bert - base offers an improvement of 0.9 % over the elmo - based c2 fcoref model .\n",
            "0.7864687688799251\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "given how gains on coreference resolution have been hard to come by as evidenced by the table , this is still a considerable improvement .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "given how gains on coreference resolution have been hard to come by as evidenced by the table , this is still a considerable improvement .\n",
            "0.8664096530421174\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , the magnitude of gains is relatively modest considering bert 's arguably better architecture and many more trainable parameters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , the magnitude of gains is relatively modest considering bert 's arguably better architecture and many more trainable parameters .\n",
            "0.8209031195592458\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "this is in sharp contrast to how even the base variant of bert has very substantially improved the state of the art in other tasks .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "this is in sharp contrast to how even the base variant of bert has very substantially improved the state of the art in other tasks .\n",
            "0.8642323059196919\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert - large , however , improves c 2 f - coref by the much larger margin of 3.9 % .\n",
            "0.7599063284776568\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we also observe that the overlap variant offers no improvement over independent .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we also observe that the overlap variant offers no improvement over independent .\n",
            "0.8398775975652427\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "concurrent with our work , , who use higher - order entity - level representations over \" frozen \" bert features , also report large gains over c 2 f - coref .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "concurrent with our work , , who use higher - order entity - level representations over \" frozen \" bert features , also report large gains over c 2 f - coref .\n",
            "0.8887913096294615\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "while their feature - based approach is more memory efficient , the fine - tuned model seems to yield better results .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "while their feature - based approach is more memory efficient , the fine - tuned model seems to yield better results .\n",
            "0.8788943350485279\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "also concurrent , span bert , another self - supervised method , pretrains span representations achieving state of the art results ( avg. f1 79.6 ) with the independent variant .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "also concurrent , span bert , another self - supervised method , pretrains span representations achieving state of the art results ( avg. f1 79.6 ) with the independent variant .\n",
            "0.8478091989951101\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "analysis\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "analysis\n",
            "0.49990667593127536\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we performed a qualitative comparison of elmo and bert models ) on the ontonotes english development set by manually assigning error categories ( e.g. , pronouns , mention paraphrasing ) to incorrect predicted clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we performed a qualitative comparison of elmo and bert models ) on the ontonotes english development set by manually assigning error categories ( e.g. , pronouns , mention paraphrasing ) to incorrect predicted clusters .\n",
            "0.8760860075946207\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "overall , we found 93 errors for bert - base and 74 for bert - large from the same 15 documents .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "overall , we found 93 errors for bert - base and 74 for bert - large from the same 15 documents .\n",
            "0.8348764961964105\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "each incorrect cluster can belong to multiple categories .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "each incorrect cluster can belong to multiple categories .\n",
            "0.8145837586341116\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "strengths\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "strengths\n",
            "0.3627149218545955\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we did not find salient qualitative differences between elmo and bert - base models , which is consistent with the quantitative results .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we did not find salient qualitative differences between elmo and bert - base models , which is consistent with the quantitative results .\n",
            "0.8611468722446376\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert - large improves over bert - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert - large improves over bert - base in a variety of ways including pronoun resolution and lexical matching ( e.g. , racetrack and track ) .\n",
            "0.8647032838803546\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "in particular , the bert - large variant is better at distinguishing related , but distinct , entities .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "in particular , the bert - large variant is better at distinguishing related , but distinct , entities .\n",
            "0.8718950416985084\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "shows several examples where the bert - base variant merges distinct entities ( like ocean theater and marine life center ) into a single cluster .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "shows several examples where the bert - base variant merges distinct entities ( like ocean theater and marine life center ) into a single cluster .\n",
            "0.8567995945759043\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "bert - large seems to be able to avoid such merging on a more regular basis .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "bert - large seems to be able to avoid such merging on a more regular basis .\n",
            "0.8761639883574752\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "weaknesses\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "weaknesses\n",
            "0.33172093854922063\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "an analysis of errors on the ontonotes english development set suggests that better modeling of document - level context , conversations , and entity paraphrasing might further improve the state of the art .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "an analysis of errors on the ontonotes english development set suggests that better modeling of document - level context , conversations , and entity paraphrasing might further improve the state of the art .\n",
            "0.9110243476940744\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "longer documents in ontonotes generally contain larger and more spread - out clusters .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "longer documents in ontonotes generally contain larger and more spread - out clusters .\n",
            "0.8540739062191229\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "we focus on three observations -( a ) shows how models perform distinctly worse on longer documents , ( b ) both models are unable to use larger segments more effectively ) and perform worse when the max segment len of 450 and 512 are used , and , ( c ) using overlapping segments to provide additional context does not improve results .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "we focus on three observations -( a ) shows how models perform distinctly worse on longer documents , ( b ) both models are unable to use larger segments more effectively ) and perform worse when the max segment len of 450 and 512 are used , and , ( c ) using overlapping segments to provide additional context does not improve results .\n",
            "0.8784026570245951\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "recent work suggests that bert 's inability to use longer sequences effectively is likely a by - product pretraining on short sequences for a vast majority of updates .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "recent work suggests that bert 's inability to use longer sequences effectively is likely a by - product pretraining on short sequences for a vast majority of updates .\n",
            "0.887193504900469\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "comparing preferred segment lengths for base and large variants of bert indicates that larger models might better encode longer contexts .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "comparing preferred segment lengths for base and large variants of bert indicates that larger models might better encode longer contexts .\n",
            "0.8214818364985513\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "however , larger models also exacerbate the memoryintensive nature of span representations , 7 which have driven recent improvements in coreference resolution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "however , larger models also exacerbate the memoryintensive nature of span representations , 7 which have driven recent improvements in coreference resolution .\n",
            "0.8510236500428614\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "these observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "these observations suggest that future research in pretraining methods should look at more effectively encoding document - level context using sparse representations .\n",
            "0.8862037215643948\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "modeling pronouns especially in the context of conversations , continues to be difficult for all models , perhaps partly because c 2 f - coref does very little to model dialog structure of the document .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "modeling pronouns especially in the context of conversations , continues to be difficult for all models , perhaps partly because c 2 f - coref does very little to model dialog structure of the document .\n",
            "0.9113433445066325\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "lastly , a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "lastly , a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing .\n",
            "0.8616861098813158\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "for example , bridging the royals with prince charles and his wife camilla likely requires pretraining models to encode relations between entities , especially considering that such learning signal is rather sparse in the training set .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "for example , bridging the royals with prince charles and his wife camilla likely requires pretraining models to encode relations between entities , especially considering that such learning signal is rather sparse in the training set .\n",
            "0.8669667175063288\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "related work\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "related work\n",
            "0.6830207291410828\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution .\n",
            "0.8388385810368458\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "the base coreference model used in this paper from belongs to this family of models .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "the base coreference model used in this paper from belongs to this family of models .\n",
            "0.8209969052384479\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "more recently , advances in coreference resolution and other nlp tasks have been driven by unsupervised contextualized representations .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "more recently , advances in coreference resolution and other nlp tasks have been driven by unsupervised contextualized representations .\n",
            "0.8458721569127426\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "of these , bert notably uses pretraining on passage - level sequences ( in conjunction with a bidirectional masked language modeling objective ) to more effectively model long - range dependencies .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "of these , bert notably uses pretraining on passage - level sequences ( in conjunction with a bidirectional masked language modeling objective ) to more effectively model long - range dependencies .\n",
            "0.8991634070494877\n",
            "A Hierarchical Model for Data - to - Text Generation\n",
            "-----------------\n",
            "spanbert focuses on pretraining span representations achieving current state of the art results on ontonotes with the independent variant .\n",
            "Transcribing structured data into natural language descriptions has emerged as a challenging task , referred to as \" data - to - text \" .\n",
            "-----------------\n",
            "spanbert focuses on pretraining span representations achieving current state of the art results on ontonotes with the independent variant .\n",
            "0.8083253568824964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(articles, research_problem_sentences):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    for i, article in enumerate(articles):\n",
        "        for sentence in article.split('\\n')[0:-1]:\n",
        "            sentences.append(sentence)\n",
        "            for i in range(len(train_research_problem_sentences)):\n",
        "\n",
        "              labels.append(int(check_similarity(sentence, train_research_problem_sentences)))\n",
        "    return sentences, labels"
      ],
      "metadata": {
        "id": "Y4dWWK1CfIE-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/test-data'\n",
        "\n",
        "train_articles, train_research_problem_sentences = Read_Data(train_data_dir)\n",
        "\n",
        "train_sentences, train_labels = create_dataset(train_articles, train_research_problem_sentences)\n",
        "\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=.2)\n"
      ],
      "metadata": {
        "id": "4Jp9g27IfLez"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (set(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktsg8TeXGo8F",
        "outputId": "12052b6c-c94e-4526-d253-7cbc9dc0c25e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check Imbalence data\n",
        "we can see bellow that the dataset is imbalanced and our minority class is 1.\n",
        "so, in the nex steps we try to figure out it."
      ],
      "metadata": {
        "id": "sbGTHDHyLhX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(train_labels).keys() \n",
        "Counter(train_labels).values() \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRivdj4PGKjp",
        "outputId": "974118d0-f729-45f1-87ff-6b1c31439b93"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([26103, 937])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization\n",
        "In this task i have used \"SciBERT\" pretrain model wich is a BERT model for scientific text. for both tokenization and training."
      ],
      "metadata": {
        "id": "2RnDJ-loNbKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "config = BertConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "config.num_labels = 2\n",
        "\n",
        "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "print('Train Data Size:({0})'.format(len(train_labels)))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "print('Validation Data Size:({0})'.format(len(val_labels)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC6kjDWOfPBG",
        "outputId": "98514d90-a79a-4212-b88b-412f180177a0"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Size:(27040)\n",
            "Validation Data Size:(6760)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "1Mp3Px0622dE"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overcome Imbalance Dataset\n",
        "becuase the dataset is imbalance, we should use class weight in our training process. \n",
        "we used class_weight from sklearn to compute the class weight."
      ],
      "metadata": {
        "id": "JKnV1nkg_0xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes=np.unique(train_labels),y=train_labels)"
      ],
      "metadata": {
        "id": "XYlu0H_HpEdz"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm9Ip_iFqx2z",
        "outputId": "604cab53-9a93-4002-ee0c-6a6dd9596b4d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.51794813, 14.42902882])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dict = {i:w for i,w in enumerate(class_weights)}\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 8\n",
        "lr=1e-5"
      ],
      "metadata": {
        "id": "GrI_L0DlPpC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Becuase lack of enough time and time consuming model, I can just train in few epochs and also I can only impelement one approach to find research problem, in the next steps, I will try with detect the span of a candidate research problem sentence and generating research topic for a given research papaer using text generation models like seq2seq models.\n",
        "this impelementation was just a try to do the task, It will be better with more complex models, using BI-LSTMs and will be optimize."
      ],
      "metadata": {
        "id": "pw_em446Oc8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', from_pt=True, config=config)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= lr, epsilon=1e-8),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "history=model.fit(train_dataset.shuffle(len(train_labels)).batch(BATCH_SIZE),\n",
        "          validation_data=val_dataset.batch(BATCH_SIZE),\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          class_weight=weights_dict)\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/ncg/model-SC-BERT/')\n",
        "print('*****************model saved!******************')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRUjPallfRfx",
        "outputId": "0d3ac3ca-3e36-4e6f-d7c5-8cfcce14270a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "3380/3380 [==============================] - 2478s 728ms/step - loss: 0.2800 - accuracy: 0.8561 - f1_m: 0.0524 - precision_m: 0.0312 - recall_m: 0.1871 - val_loss: 0.4579 - val_accuracy: 0.7664 - val_f1_m: 0.0847 - val_precision_m: 0.0533 - val_recall_m: 0.2308\n",
            "Epoch 2/8\n",
            "3380/3380 [==============================] - 2456s 727ms/step - loss: 0.1572 - accuracy: 0.9186 - f1_m: 0.0592 - precision_m: 0.0345 - recall_m: 0.2277 - val_loss: 0.3414 - val_accuracy: 0.8967 - val_f1_m: 0.0633 - val_precision_m: 0.0371 - val_recall_m: 0.2396\n",
            "Epoch 3/8\n",
            "2797/3380 [=======================>......] - ETA: 6:36 - loss: 0.1201 - accuracy: 0.9398 - f1_m: 0.0596 - precision_m: 0.0347 - recall_m: 0.2314"
          ]
        }
      ]
    }
  ]
}